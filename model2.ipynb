{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from balance_dataset import balance_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "H-Z9MPIWI72q"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Final/dataset_2.csv')\n",
        "# add the target column to the features\n",
        "\n",
        "df['gender'] = 0\n",
        "df['inpatient'] = 0\n",
        "df['age'] = 0\n",
        "df['mean'] = 0\n",
        "df['std'] = 0\n",
        "df['max'] = 0\n",
        "df['min'] = 0\n",
        "df['target'] = 0\n",
        "\n",
        "denoised_data = pd.read_csv('Data/denoised_action.csv')\n",
        "handcraft_data = pd.read_csv(\"Data/handcraft_data.csv\")\n",
        "tabular = pd.read_csv(\"Data/scores_for classification_2.csv\")\n",
        "dataset = pd.read_csv(\"Final/dataset_2.csv\")\n",
        "\n",
        "# assign the first 359 patients (17232/48)\n",
        "for i in range(0, 359):\n",
        "    real_patient = denoised_data.loc[i][\"patient\"]\n",
        "    real_patient_pos = real_patient -1\n",
        "    t = tabular.loc[real_patient_pos][\"afftype\"]\n",
        "    df.loc[i, 'gender'] = tabular.loc[real_patient_pos]['gender']\n",
        "    df.loc[i, 'inpatient'] = tabular.loc[real_patient_pos]['inpatient']\n",
        "    df.loc[i, 'age'] = tabular.loc[real_patient_pos]['age']\n",
        "    df.loc[i, 'mean'] = handcraft_data.loc[real_patient_pos]['means']\n",
        "    df.loc[i, 'std'] = handcraft_data.loc[real_patient_pos]['stds']\n",
        "    df.loc[i, 'max'] = handcraft_data.loc[real_patient_pos]['max']\n",
        "    df.loc[i, 'min'] = handcraft_data.loc[real_patient_pos]['min']\n",
        "    if int(t) == 1 or int(t) == 3:\n",
        "        df.loc[i, 'target'] = 0\n",
        "    else:\n",
        "        df.loc[i, 'target'] = 1"
      ],
      "metadata": {
        "id": "A2ylcfqjLsJm"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "PwW3rRSDpRXL",
        "outputId": "4d3dff2e-5387-4876-c98b-364c7ec1f0e3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           f1        f2        f3        f4        f5        f6        f7  \\\n",
              "0    0.843199 -0.019875  0.348144 -3.734176  0.793592 -2.311102  0.872043   \n",
              "1    2.384476  0.075164 -1.233456 -1.727925 -0.328627 -1.431436  1.717354   \n",
              "2    0.332048 -1.513692  1.440681 -2.308658 -0.041846 -1.517303 -0.487194   \n",
              "3   -1.402971 -0.658496  0.481852 -3.419377  0.539492 -1.853214  1.294299   \n",
              "4    0.852535  0.862926  2.134740 -2.638090 -0.442957 -0.718977  0.090587   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "354 -1.600638  0.283824  2.018573 -1.191005 -0.657852 -2.721894 -0.059204   \n",
              "355 -0.725218  0.192610  0.117516 -1.099034 -1.249787 -0.897582  0.777956   \n",
              "356 -0.403742 -0.144096  0.103674 -0.749376 -1.266680 -1.128065  0.482980   \n",
              "357  0.790339 -0.491060 -0.851775 -1.025607 -1.013319 -0.679107  0.924477   \n",
              "358  0.576877 -0.342436  0.629780 -1.158061 -0.661487 -0.729685 -0.281760   \n",
              "\n",
              "           f8        f9       f10  ...       f29       f30  gender  inpatient  \\\n",
              "0   -0.625631  0.684587 -0.424186  ...  1.505166 -2.901132       2          2   \n",
              "1   -1.287621  2.351772 -1.477809  ...  0.746082 -2.343760       2          2   \n",
              "2   -0.846304  0.269623 -2.833039  ...  2.174545 -2.293868       2          2   \n",
              "3   -1.714455  1.201168 -1.017584  ...  2.777213 -2.310588       2          2   \n",
              "4    0.103345  0.841696 -1.218732  ...  1.594197 -1.063941       2          2   \n",
              "..        ...       ...       ...  ...       ...       ...     ...        ...   \n",
              "354 -0.194050 -0.025236 -1.738701  ... -0.503800  0.401390       1          1   \n",
              "355 -0.244736  0.278399 -1.449569  ... -0.256283 -0.925431       1          1   \n",
              "356 -0.848260 -0.348291 -0.108733  ... -0.539237 -0.754682       1          1   \n",
              "357 -0.230366  0.313328 -0.793535  ...  0.183362 -0.272740       1          1   \n",
              "358 -1.093940  0.933950 -0.320456  ... -0.025737 -1.187449       1          1   \n",
              "\n",
              "     age        mean         std          max  min  target  \n",
              "0      4  156.247222  158.294357   478.600000  0.0       1  \n",
              "1      4  156.247222  158.294357   478.600000  0.0       1  \n",
              "2      4  156.247222  158.294357   478.600000  0.0       1  \n",
              "3      4  156.247222  158.294357   478.600000  0.0       1  \n",
              "4      4  156.247222  158.294357   478.600000  0.0       1  \n",
              "..   ...         ...         ...          ...  ...     ...  \n",
              "354    3  338.096528  504.973433  2186.633333  4.0       1  \n",
              "355    3  338.096528  504.973433  2186.633333  4.0       1  \n",
              "356    3  338.096528  504.973433  2186.633333  4.0       1  \n",
              "357    3  338.096528  504.973433  2186.633333  4.0       1  \n",
              "358    3  338.096528  504.973433  2186.633333  4.0       1  \n",
              "\n",
              "[359 rows x 38 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5c39f589-fd98-4fef-a577-a3a70a652012\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>f8</th>\n",
              "      <th>f9</th>\n",
              "      <th>f10</th>\n",
              "      <th>...</th>\n",
              "      <th>f29</th>\n",
              "      <th>f30</th>\n",
              "      <th>gender</th>\n",
              "      <th>inpatient</th>\n",
              "      <th>age</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>max</th>\n",
              "      <th>min</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.843199</td>\n",
              "      <td>-0.019875</td>\n",
              "      <td>0.348144</td>\n",
              "      <td>-3.734176</td>\n",
              "      <td>0.793592</td>\n",
              "      <td>-2.311102</td>\n",
              "      <td>0.872043</td>\n",
              "      <td>-0.625631</td>\n",
              "      <td>0.684587</td>\n",
              "      <td>-0.424186</td>\n",
              "      <td>...</td>\n",
              "      <td>1.505166</td>\n",
              "      <td>-2.901132</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>156.247222</td>\n",
              "      <td>158.294357</td>\n",
              "      <td>478.600000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.384476</td>\n",
              "      <td>0.075164</td>\n",
              "      <td>-1.233456</td>\n",
              "      <td>-1.727925</td>\n",
              "      <td>-0.328627</td>\n",
              "      <td>-1.431436</td>\n",
              "      <td>1.717354</td>\n",
              "      <td>-1.287621</td>\n",
              "      <td>2.351772</td>\n",
              "      <td>-1.477809</td>\n",
              "      <td>...</td>\n",
              "      <td>0.746082</td>\n",
              "      <td>-2.343760</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>156.247222</td>\n",
              "      <td>158.294357</td>\n",
              "      <td>478.600000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.332048</td>\n",
              "      <td>-1.513692</td>\n",
              "      <td>1.440681</td>\n",
              "      <td>-2.308658</td>\n",
              "      <td>-0.041846</td>\n",
              "      <td>-1.517303</td>\n",
              "      <td>-0.487194</td>\n",
              "      <td>-0.846304</td>\n",
              "      <td>0.269623</td>\n",
              "      <td>-2.833039</td>\n",
              "      <td>...</td>\n",
              "      <td>2.174545</td>\n",
              "      <td>-2.293868</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>156.247222</td>\n",
              "      <td>158.294357</td>\n",
              "      <td>478.600000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.402971</td>\n",
              "      <td>-0.658496</td>\n",
              "      <td>0.481852</td>\n",
              "      <td>-3.419377</td>\n",
              "      <td>0.539492</td>\n",
              "      <td>-1.853214</td>\n",
              "      <td>1.294299</td>\n",
              "      <td>-1.714455</td>\n",
              "      <td>1.201168</td>\n",
              "      <td>-1.017584</td>\n",
              "      <td>...</td>\n",
              "      <td>2.777213</td>\n",
              "      <td>-2.310588</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>156.247222</td>\n",
              "      <td>158.294357</td>\n",
              "      <td>478.600000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.852535</td>\n",
              "      <td>0.862926</td>\n",
              "      <td>2.134740</td>\n",
              "      <td>-2.638090</td>\n",
              "      <td>-0.442957</td>\n",
              "      <td>-0.718977</td>\n",
              "      <td>0.090587</td>\n",
              "      <td>0.103345</td>\n",
              "      <td>0.841696</td>\n",
              "      <td>-1.218732</td>\n",
              "      <td>...</td>\n",
              "      <td>1.594197</td>\n",
              "      <td>-1.063941</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>156.247222</td>\n",
              "      <td>158.294357</td>\n",
              "      <td>478.600000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>354</th>\n",
              "      <td>-1.600638</td>\n",
              "      <td>0.283824</td>\n",
              "      <td>2.018573</td>\n",
              "      <td>-1.191005</td>\n",
              "      <td>-0.657852</td>\n",
              "      <td>-2.721894</td>\n",
              "      <td>-0.059204</td>\n",
              "      <td>-0.194050</td>\n",
              "      <td>-0.025236</td>\n",
              "      <td>-1.738701</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.503800</td>\n",
              "      <td>0.401390</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>338.096528</td>\n",
              "      <td>504.973433</td>\n",
              "      <td>2186.633333</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>355</th>\n",
              "      <td>-0.725218</td>\n",
              "      <td>0.192610</td>\n",
              "      <td>0.117516</td>\n",
              "      <td>-1.099034</td>\n",
              "      <td>-1.249787</td>\n",
              "      <td>-0.897582</td>\n",
              "      <td>0.777956</td>\n",
              "      <td>-0.244736</td>\n",
              "      <td>0.278399</td>\n",
              "      <td>-1.449569</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.256283</td>\n",
              "      <td>-0.925431</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>338.096528</td>\n",
              "      <td>504.973433</td>\n",
              "      <td>2186.633333</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>356</th>\n",
              "      <td>-0.403742</td>\n",
              "      <td>-0.144096</td>\n",
              "      <td>0.103674</td>\n",
              "      <td>-0.749376</td>\n",
              "      <td>-1.266680</td>\n",
              "      <td>-1.128065</td>\n",
              "      <td>0.482980</td>\n",
              "      <td>-0.848260</td>\n",
              "      <td>-0.348291</td>\n",
              "      <td>-0.108733</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.539237</td>\n",
              "      <td>-0.754682</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>338.096528</td>\n",
              "      <td>504.973433</td>\n",
              "      <td>2186.633333</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>357</th>\n",
              "      <td>0.790339</td>\n",
              "      <td>-0.491060</td>\n",
              "      <td>-0.851775</td>\n",
              "      <td>-1.025607</td>\n",
              "      <td>-1.013319</td>\n",
              "      <td>-0.679107</td>\n",
              "      <td>0.924477</td>\n",
              "      <td>-0.230366</td>\n",
              "      <td>0.313328</td>\n",
              "      <td>-0.793535</td>\n",
              "      <td>...</td>\n",
              "      <td>0.183362</td>\n",
              "      <td>-0.272740</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>338.096528</td>\n",
              "      <td>504.973433</td>\n",
              "      <td>2186.633333</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>358</th>\n",
              "      <td>0.576877</td>\n",
              "      <td>-0.342436</td>\n",
              "      <td>0.629780</td>\n",
              "      <td>-1.158061</td>\n",
              "      <td>-0.661487</td>\n",
              "      <td>-0.729685</td>\n",
              "      <td>-0.281760</td>\n",
              "      <td>-1.093940</td>\n",
              "      <td>0.933950</td>\n",
              "      <td>-0.320456</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.025737</td>\n",
              "      <td>-1.187449</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>338.096528</td>\n",
              "      <td>504.973433</td>\n",
              "      <td>2186.633333</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>359 rows × 38 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5c39f589-fd98-4fef-a577-a3a70a652012')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5c39f589-fd98-4fef-a577-a3a70a652012 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5c39f589-fd98-4fef-a577-a3a70a652012');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zg2s5eintvSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts = df['target'].value_counts()\n",
        "print(f'Before balancing :{counts}')\n",
        "# balance the dataset\n",
        "dataset = balance_dataset(df)\n",
        "counts = dataset['target'].value_counts()\n",
        "print(f'After: {counts}')\n",
        "\n",
        "# scale the data from all the features except the target\n",
        "scaler = StandardScaler()\n",
        "dataset.iloc[:, 1:-1] = scaler.fit_transform(dataset.iloc[:, 1:-1])\n",
        "\n",
        "# split to features and targets\n",
        "y = dataset['target']\n",
        "X = dataset.loc[:, ~dataset.columns.isin(['patient', 'target'])]"
      ],
      "metadata": {
        "id": "t6wrAFEioldZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e44ad62-9374-4ff2-bbea-898fa3fc27bf"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before balancing :1    233\n",
            "0    126\n",
            "Name: target, dtype: int64\n",
            "After: 0    126\n",
            "1    126\n",
            "Name: target, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data to train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "y_train = y_train.reset_index(drop=True)\n",
        "y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "print()\n",
        "print(f'We have {len(X_train)} patients in the trainig set')\n",
        "print(f'and {len(X_test)} patients in the test set')\n",
        "print()\n",
        "print('-----------------------------')\n",
        "\n",
        "# convert the data to tensors\n",
        "X_train_ = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "X_test_ = torch.tensor(X_test.values, dtype=torch.float32)\n",
        "y_train_ = torch.tensor(y_train.values, dtype=torch.float32)\n",
        "y_test_ = torch.tensor(y_test.values, dtype=torch.float32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpgPRwHmM5w-",
        "outputId": "bd22738b-b64e-4163-ac37-32c724b90f70"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "We have 201 patients in the trainig set\n",
            "and 51 patients in the test set\n",
            "\n",
            "-----------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(37, 15)\n",
        "        self.fc2 = nn.Linear(15, 7)\n",
        "        self.fc3 = nn.Linear(7, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        # we have classification problem so we will use sigmoid function\n",
        "        x = torch.sigmoid(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "# set the seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "hs4myxB3iY5x"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the loss function and the optimizer. We will use MSE loss function\n",
        "criterion = nn.BCELoss() # Binary Cross Entropy loss for binary classification\n",
        "#optimizer = torch.optim.Adam(net.parameters(), lr=0.1, weight_decay=0)\n",
        "# another optimizer that we can use is SGD\n",
        "\n",
        "# run model_1.py for different hyperparameters\n",
        "# and save the results in a csv file\n",
        "# lr = learning rate\n",
        "# wd = weight decay\n",
        "# mm = momentum\n",
        "# ld = learning rate decay\n",
        "\n",
        "for lr in [0.01, 0.001]:\n",
        "    for wd in [ 0.001]:\n",
        "        for mm in [0.5, 0.725]:\n",
        "            for ld in [1, 0.9]:\n",
        "                hyperparameters = {'lr': lr, 'weight_decay': wd, 'momentum': mm}\n",
        "                lr_decay = ld\n",
        "                print(f'lr = {lr}, wd = {wd}, mm = {mm}, ld = {ld}')\n",
        "                net = Net()\n",
        "\n",
        "\n",
        "                hyperparameters = {'lr': lr, 'weight_decay': wd, 'momentum': mm}\n",
        "                lr_decay = ld\n",
        "\n",
        "                optimizer = torch.optim.SGD(net.parameters(), **hyperparameters)\n",
        "\n",
        "\n",
        "                # train the model\n",
        "                epochs = 5000\n",
        "                loss_values = []\n",
        "\n",
        "                from sklearn.model_selection import KFold\n",
        "\n",
        "                # Define the number of folds for cross-validation\n",
        "                num_folds = 5\n",
        "\n",
        "                # Create a KFold object\n",
        "                kf = KFold(n_splits=num_folds, shuffle=True)\n",
        "                for fold, (train_ids, val_ids) in enumerate(kf.split(X_train_)):\n",
        "                    X_train_fold = X_train_[train_ids]\n",
        "                    y_train_fold = y_train_[train_ids]\n",
        "                    X_val_fold = X_train_[val_ids]\n",
        "                    y_val_fold = y_train_[val_ids]\n",
        "\n",
        "\n",
        "                    for i in range(epochs):\n",
        "                        i += 1\n",
        "                        y_pred = net.forward(X_train_fold)\n",
        "\n",
        "                        loss = criterion(y_pred, y_train_fold.unsqueeze(1).float())\n",
        "                        optimizer.zero_grad()\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        # print every 10 epochs\n",
        "                        if i%1000 == 0:\n",
        "                            print(f'epoch: {i:2}  loss: {loss.item():10.8f}')\n",
        "                            loss_values.append([i, loss.item()])\n",
        "                        # decrease the learning rate every 300 epochs\n",
        "                        if i % 1000 == 0:\n",
        "                            for g in optimizer.param_groups:\n",
        "                                g['lr'] = g['lr'] * lr_decay\n",
        "\n",
        "\n",
        "\n",
        "                # test the model\n",
        "                with torch.no_grad():\n",
        "                    y_val = net.forward(X_test_)\n",
        "                    loss = criterion(y_val, y_test_.unsqueeze(1).float())\n",
        "\n",
        "                # calculate the accuracy\n",
        "                correct = 0\n",
        "                total = 0\n",
        "                correct_predictions = []\n",
        "                prediction = []\n",
        "                with torch.no_grad():\n",
        "                    predictions = net.forward(X_test_)\n",
        "                    for i in range(len(y_test_)):\n",
        "                        if predictions[i] >= 0.5:\n",
        "                            y_pred = 1\n",
        "                            prediction.append(1)\n",
        "                        else:\n",
        "                            y_pred = 0\n",
        "                            prediction.append(0)\n",
        "                        if y_pred == y_test_[i]:\n",
        "                            correct += 1\n",
        "                            print(f'correct prediction: {i}')\n",
        "                            correct_predictions.append(i)\n",
        "                        total += 1\n",
        "                print()\n",
        "                accuracy = correct/total\n",
        "                print(f'loss on the test set = {loss:.3f}')\n",
        "                print(f'Accuracy on the test set: {round(accuracy, 4)*100:3.4f}%')\n",
        "\n",
        "\n",
        "\n",
        "                # csv with colums: epochs, lr, weight_decay, momentum, accuracy\n",
        "                # in order to find the best hyperparameters\n",
        "                df = pd.read_csv('NN_hyperparameters.csv')\n",
        "\n",
        "\n",
        "\n",
        "                df = pd.concat([df, pd.DataFrame([[epochs, hyperparameters['lr'], hyperparameters['weight_decay'], \\\n",
        "                                                    hyperparameters['momentum'], lr_decay, accuracy]], columns=['epochs', 'lr', \\\n",
        "                                                    'weight_decay', 'momentum', 'lr_decay',  'accuracy'])], axis=0, ignore_index=True)\n",
        "\n",
        "                # change the order of the columns\n",
        "                df = df[['epochs', 'lr', 'weight_decay', 'momentum', 'lr_decay',  'accuracy']]\n",
        "\n",
        "                df.to_csv('NN_hyperparameters.csv', index=False, header=True)\n",
        "\n",
        "\n",
        "                # save the model's weights in order to plot the features with their weights\n",
        "                torch.save(net.state_dict(), 'NN_weights.pt')\n",
        "\n",
        "\n",
        "                # merge X_test with y_test and save it to csv\n",
        "                X_test_df = pd.DataFrame(X_test_)\n",
        "                y_test_df = pd.DataFrame(y_test_)\n",
        "                predictions_df = pd.DataFrame(prediction)\n",
        "                # merge X_test_df with y_test_df\n",
        "                test_df = pd.concat([X_test_df, y_test_df, predictions_df], axis=1, ignore_index=True)\n",
        "\n",
        "                # header\n",
        "                header = ['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', \\\n",
        "                        'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', \\\n",
        "                            'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', \\\n",
        "                                'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'gender', \\\n",
        "                                 \t'inpatient', \t'age',\t'mean' ,\t'std', \t'max', \t'min', 'target', 'prediction']\n",
        "\n",
        "\n",
        "                print(test_df)\n",
        "                # save the test_df to csv\n",
        "                test_df.to_csv('test_df.csv', index=False, header=header)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ke42e0hhdwT",
        "outputId": "c4ad218e-2564-45a9-842a-87bb4ca2dcb5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr = 0.01, wd = 0.001, mm = 0.5, ld = 1\n",
            "epoch: 1000  loss: 0.50879633\n",
            "epoch: 2000  loss: 0.19837317\n",
            "epoch: 3000  loss: 0.09366910\n",
            "epoch: 4000  loss: 0.06743531\n",
            "epoch: 5000  loss: 0.05880526\n",
            "epoch: 1000  loss: 0.09826312\n",
            "epoch: 2000  loss: 0.09095439\n",
            "epoch: 3000  loss: 0.08873633\n",
            "epoch: 4000  loss: 0.08710450\n",
            "epoch: 5000  loss: 0.08610415\n",
            "epoch: 1000  loss: 0.10934492\n",
            "epoch: 2000  loss: 0.10608510\n",
            "epoch: 3000  loss: 0.10457227\n",
            "epoch: 4000  loss: 0.10367596\n",
            "epoch: 5000  loss: 0.10299361\n",
            "epoch: 1000  loss: 0.09488130\n",
            "epoch: 2000  loss: 0.09419014\n",
            "epoch: 3000  loss: 0.09317821\n",
            "epoch: 4000  loss: 0.09269316\n",
            "epoch: 5000  loss: 0.09230428\n",
            "epoch: 1000  loss: 0.10568133\n",
            "epoch: 2000  loss: 0.10446747\n",
            "epoch: 3000  loss: 0.10292778\n",
            "epoch: 4000  loss: 0.10149106\n",
            "epoch: 5000  loss: 0.09995227\n",
            "correct prediction: 0\n",
            "correct prediction: 2\n",
            "correct prediction: 3\n",
            "correct prediction: 4\n",
            "correct prediction: 5\n",
            "correct prediction: 6\n",
            "correct prediction: 7\n",
            "correct prediction: 8\n",
            "correct prediction: 11\n",
            "correct prediction: 12\n",
            "correct prediction: 13\n",
            "correct prediction: 15\n",
            "correct prediction: 16\n",
            "correct prediction: 18\n",
            "correct prediction: 19\n",
            "correct prediction: 20\n",
            "correct prediction: 21\n",
            "correct prediction: 22\n",
            "correct prediction: 26\n",
            "correct prediction: 29\n",
            "correct prediction: 30\n",
            "correct prediction: 31\n",
            "correct prediction: 32\n",
            "correct prediction: 33\n",
            "correct prediction: 34\n",
            "correct prediction: 35\n",
            "correct prediction: 36\n",
            "correct prediction: 37\n",
            "correct prediction: 38\n",
            "correct prediction: 39\n",
            "correct prediction: 40\n",
            "correct prediction: 42\n",
            "correct prediction: 43\n",
            "correct prediction: 45\n",
            "correct prediction: 46\n",
            "correct prediction: 47\n",
            "correct prediction: 48\n",
            "correct prediction: 50\n",
            "\n",
            "loss on the test set = 1.258\n",
            "Accuracy on the test set: 74.5100%\n",
            "          0         1         2         3         4         5         6   \\\n",
            "0   0.235795 -0.596528  0.764939  0.155125 -0.709392 -1.921546 -0.110274   \n",
            "1  -2.949119 -2.031854  0.231524  0.141733  2.474848  0.246714  0.185667   \n",
            "2  -0.228339 -0.359309 -0.337852  0.821626 -0.872955  0.984931  0.392991   \n",
            "3  -1.213337  2.146571 -0.274030  0.495122 -1.323094  1.240324 -0.271201   \n",
            "4   1.122109  0.807842  0.980374 -0.072595 -0.186554  0.357749 -0.860289   \n",
            "5   0.633887 -0.313952  0.828973 -0.531478  0.995154  0.668412 -0.913099   \n",
            "6  -1.196963  0.714636  0.874417 -2.132932 -0.346888 -0.139611  0.275690   \n",
            "7  -0.208678  0.924384  0.068978 -0.392970 -0.002467  0.156285 -0.486114   \n",
            "8  -0.459825 -0.898318 -1.508754  1.435905  0.692827  3.039905 -0.190101   \n",
            "9  -0.822481 -0.223209 -0.019506  1.332998 -1.304332 -1.430331  0.158815   \n",
            "10  4.518938  1.767920 -0.655172 -0.114569 -1.247974  0.208459 -0.956752   \n",
            "11  0.807073  0.455789  1.731345 -0.566816  1.194391  0.115341  1.135136   \n",
            "12  0.127495  0.425414 -0.007127 -1.479204 -1.642907 -0.709907 -0.342184   \n",
            "13 -1.044644 -0.399502 -0.448795  0.109540  1.014075  0.344043 -0.407666   \n",
            "14  0.703290 -0.205306 -1.139115  1.155272 -1.439506  0.086987 -1.145898   \n",
            "15 -0.987320 -0.488019  0.505248  0.574651 -0.600331  0.469882  1.185208   \n",
            "16 -1.590583 -0.297732 -0.862299 -0.494346  0.679952 -1.054007  0.043731   \n",
            "17  0.961275  1.291009  0.965461 -0.210189 -0.250317 -1.335828 -0.919517   \n",
            "18 -0.675030 -0.008716 -1.001394  1.176765 -0.357519  1.582644  1.515647   \n",
            "19  0.670038  0.478898  0.133934  0.317207 -0.383914  2.476607 -2.575679   \n",
            "20  1.780669  1.033887 -0.043665  0.135341  0.036268 -0.694678 -1.321278   \n",
            "21 -2.905534  0.963953  1.057505 -0.600924  0.621815 -0.175828  1.888403   \n",
            "22 -0.484269 -0.717361 -1.216896 -1.341853  1.359862 -0.333006  0.239972   \n",
            "23  0.770300  0.286824 -0.522803  0.170265 -0.770391 -0.034662 -0.657823   \n",
            "24  0.171093  0.955744 -1.376500  0.614331  0.794085 -0.985971 -1.033522   \n",
            "25 -1.382818 -0.457078 -0.456693  2.177205  0.242991 -0.474857  0.064215   \n",
            "26 -0.986676  0.818727  0.758720 -0.899834 -0.734657 -1.053275 -0.060929   \n",
            "27  1.099488 -0.697891 -0.403934 -0.176542  0.164813  0.451029 -1.484485   \n",
            "28  2.423820  1.457278  0.317998 -2.058740 -0.420090 -0.691256  0.167741   \n",
            "29  0.503652 -0.642579 -0.903764  0.374938 -0.213290  0.795397 -0.387991   \n",
            "30  1.014337 -1.608052  0.375635 -2.259168 -0.696333 -1.079305 -0.400122   \n",
            "31  3.116689  1.723113  0.681957 -1.420649 -1.193706 -0.665680 -0.091697   \n",
            "32 -1.517675 -1.205269 -0.024106  0.664088  3.003847  0.674357 -0.142209   \n",
            "33 -1.125131  0.378030 -0.374538  0.361702  1.661269  1.541844  1.134609   \n",
            "34  0.007997  0.717900 -1.084321  0.565784  0.038188 -1.910262 -0.709885   \n",
            "35  1.317641  0.414077  0.614004 -0.419078 -1.573296  0.660523  1.796185   \n",
            "36  0.786040  0.074047 -0.166161 -0.262730  0.763262  0.706814  2.259574   \n",
            "37  0.011365 -0.370733  0.261825  0.013103  1.260039 -0.554880  0.191121   \n",
            "38 -0.560580  0.164782  0.557392 -1.035448  0.752872 -1.385119 -0.630531   \n",
            "39 -0.725218  0.331052 -0.523784  0.194932 -0.856695  0.352498 -0.053287   \n",
            "40  0.803086 -0.835295 -0.639992 -0.869191 -1.658046  0.863716  0.382231   \n",
            "41  0.272421  1.009930  1.523204 -1.375841 -1.603089 -0.350015  0.297634   \n",
            "42 -2.727341  0.145088  0.484466  1.042311  1.162478 -1.482644 -0.157254   \n",
            "43 -0.789957  0.549711  0.977592 -0.019222 -0.048327 -0.215502  0.429913   \n",
            "44 -3.751709 -0.048892  1.479589  1.088468  1.030387  0.744044  1.597948   \n",
            "45  0.261220  1.287996  0.349535 -0.324269 -1.129768 -1.046599  0.082300   \n",
            "46  1.947578  1.323064 -0.352006 -1.286524 -1.308874  0.670241 -0.144401   \n",
            "47 -1.549491 -1.522130 -0.313656  0.316227 -0.249302  1.104591 -0.581757   \n",
            "48  1.092981 -1.028857 -0.250087 -1.441544 -1.115122  1.402114 -1.131119   \n",
            "49  0.572089  0.007242 -0.636844  0.853076  1.122678  1.509380 -1.020430   \n",
            "50 -2.689340 -0.277448  1.042525  1.452017  0.123011  1.133931  0.748255   \n",
            "\n",
            "          7         8         9   ...        29        30        31        32  \\\n",
            "0   0.382224 -1.064994 -2.581160  ...  0.219561 -1.164105 -2.302173 -1.035291   \n",
            "1  -0.375862 -0.384313 -0.203708  ... -0.154425  0.859029  0.434372 -0.076124   \n",
            "2  -1.113034 -1.138260 -0.083353  ...  0.030184 -1.164105  0.434372  0.883043   \n",
            "3  -0.098764  0.337893  2.173916  ...  2.094682 -1.164105  0.434372  0.403459   \n",
            "4   1.165677  1.248191  0.310892  ...  0.263554 -1.164105  0.434372  0.883043   \n",
            "5   1.512495 -0.868699 -1.661515  ...  0.149895  0.859029 -2.302173  0.883043   \n",
            "6  -0.951835  1.527703 -0.403727  ... -0.395063  0.859029 -2.302173 -0.555708   \n",
            "7  -0.173060  0.068664  0.033924  ... -0.087071  0.859029  0.434372 -0.555708   \n",
            "8   0.584333 -1.482787  1.048752  ...  1.611295  0.859029  0.434372 -0.076124   \n",
            "9   0.488403  0.530823 -1.020090  ... -0.079679  0.859029  0.434372 -0.076124   \n",
            "10  0.252102  0.355264  1.974272  ...  0.574427  0.859029  0.434372 -0.076124   \n",
            "11  1.379504 -0.101004 -0.011852  ... -2.185978 -1.164105 -2.302173  2.321793   \n",
            "12 -1.226289  1.084064  0.252926  ... -0.787906 -1.164105  0.434372  0.403459   \n",
            "13  0.077320 -0.676263  0.871215  ...  1.041880  0.859029  0.434372 -0.555708   \n",
            "14  0.471782  0.232531  2.293824  ...  2.185123  0.859029  0.434372 -0.076124   \n",
            "15 -0.273246 -0.531004 -0.567577  ... -0.876400 -1.164105 -2.302173  2.321793   \n",
            "16 -0.891745 -0.625318 -1.002406  ...  1.048573 -1.164105 -2.302173 -1.035291   \n",
            "17  1.668175 -0.032122 -0.608809  ... -0.085208  0.859029  0.434372 -0.076124   \n",
            "18 -1.201217 -1.086889  0.143054  ... -0.610362  0.859029  0.434372 -0.555708   \n",
            "19  3.880914 -2.190124  0.326164  ...  2.908866  0.859029  0.434372 -0.076124   \n",
            "20  1.928448 -0.231571  1.163341  ...  0.765533 -1.164105  0.434372  0.883043   \n",
            "21  0.477749 -0.935990 -0.204657  ...  0.077658  0.859029  0.434372 -0.076124   \n",
            "22  0.308476  1.260645  0.242491  ... -0.227157  0.859029  0.434372  0.403459   \n",
            "23  0.957006 -0.012627 -0.059101  ... -0.139646  0.859029  0.434372 -0.076124   \n",
            "24 -0.577370 -1.338864 -0.805743  ...  1.489135  0.859029  0.434372  0.403459   \n",
            "25  0.012977 -0.374317  0.811695  ...  0.989533 -1.164105  0.434372  0.403459   \n",
            "26 -0.306770  0.288864  0.338384  ... -0.651107  0.859029  0.434372 -0.555708   \n",
            "27  0.342000 -0.040139 -0.198566  ...  1.079230 -1.164105  0.434372  0.403459   \n",
            "28  1.034245  0.167103 -0.535725  ... -0.677696  0.859029  0.434372 -0.555708   \n",
            "29  0.128518 -0.699788  0.538024  ...  0.641478 -1.164105  0.434372  0.403459   \n",
            "30 -0.226448  1.329023 -0.446898  ... -1.456581 -1.164105  0.434372 -0.555708   \n",
            "31  1.514548 -0.396321 -0.823777  ...  0.047431 -1.164105  0.434372 -1.994458   \n",
            "32 -0.628094 -1.419638  1.878376  ...  0.980789 -1.164105  0.434372  1.842209   \n",
            "33 -2.042031  0.616344  1.442150  ... -0.415767 -1.164105  0.434372  1.842209   \n",
            "34  0.442049  0.151550  0.522742  ...  0.479677 -1.164105  0.434372  0.403459   \n",
            "35  0.938744  1.178404  0.439759  ... -0.505916  0.859029  0.434372 -0.076124   \n",
            "36 -1.782716  1.869428  0.239494  ... -0.741127  0.859029  0.434372  0.403459   \n",
            "37  0.257665 -0.422206 -1.140060  ... -1.971977 -1.164105  0.434372  0.883043   \n",
            "38 -1.204079  1.796819  0.624790  ... -0.385871  0.859029  0.434372  0.403459   \n",
            "39  0.339062 -0.523117 -0.516064  ...  0.357690 -1.164105 -2.302173 -1.035291   \n",
            "40  0.349595 -1.054598 -1.122964  ... -1.632031  0.859029  0.434372  0.403459   \n",
            "41  0.143264  1.653829  0.386328  ... -1.314442  0.859029 -2.302173  0.883043   \n",
            "42 -0.797886 -0.517182 -0.206603  ...  0.727034 -1.164105  0.434372 -0.555708   \n",
            "43  0.702972  0.181321  0.734456  ...  0.258117  0.859029  0.434372  0.883043   \n",
            "44  0.149512 -0.831327  1.386731  ... -0.265317  0.859029  0.434372 -0.076124   \n",
            "45  0.076983  0.999414 -0.797919  ...  0.080311 -1.164105  0.434372  0.403459   \n",
            "46  0.690211  2.110784  0.259129  ... -0.905546 -1.164105  0.434372 -1.994458   \n",
            "47 -0.235874 -1.889797  1.388273  ...  0.720933 -1.164105  0.434372  1.842209   \n",
            "48 -0.576694  1.359136  0.334442  ... -1.248674  0.859029 -2.302173 -0.555708   \n",
            "49  1.017679 -1.414678  0.752618  ...  0.679185  0.859029  0.434372 -1.514875   \n",
            "50  0.395918 -0.451649 -0.826654  ... -0.224069  0.859029  0.434372 -0.076124   \n",
            "\n",
            "          33        34        35        36   37  38  \n",
            "0   2.585000  2.817139  2.571306  1.736718  1.0   1  \n",
            "1  -0.797790 -0.718681 -0.718598 -0.705769  0.0   1  \n",
            "2   1.098832 -0.026861 -0.116593  1.126096  0.0   0  \n",
            "3   1.344660  0.292413  0.468332  1.126096  1.0   1  \n",
            "4   1.098832 -0.026861 -0.116593  1.126096  0.0   0  \n",
            "5  -0.919079 -0.644039 -0.593604  1.736718  1.0   1  \n",
            "6   1.049271  1.118267  1.003893  1.431407  1.0   1  \n",
            "7  -0.290092 -0.493438 -0.743830 -0.705769  1.0   1  \n",
            "8  -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "9  -0.102652  0.180798  0.405252 -0.705769  0.0   1  \n",
            "10 -0.303103 -0.272472 -0.300268  1.655301  0.0   1  \n",
            "11  0.820384  0.663055  0.713856  1.187158  1.0   1  \n",
            "12 -0.626621 -0.792831 -0.764792 -0.705769  1.0   1  \n",
            "13 -0.290092 -0.493438 -0.743830 -0.705769  1.0   1  \n",
            "14 -0.797790 -0.718681 -0.718598 -0.705769  0.0   1  \n",
            "15  0.820384  0.663055  0.713856  1.187158  1.0   1  \n",
            "16  0.789663  0.562746  0.645536  1.349991  1.0   1  \n",
            "17 -0.102652  0.180798  0.405252 -0.705769  0.0   1  \n",
            "18 -0.997746 -0.768170 -0.834729 -0.705769  0.0   0  \n",
            "19 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "20  1.098832 -0.026861 -0.116593  1.126096  0.0   0  \n",
            "21 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "22  0.251586 -0.296086 -0.246117 -0.705769  0.0   0  \n",
            "23 -0.797790 -0.718681 -0.718598 -0.705769  0.0   1  \n",
            "24  0.251586 -0.296086 -0.246117 -0.705769  0.0   1  \n",
            "25  1.344660  0.292413  0.468332  1.126096  1.0   0  \n",
            "26 -0.290092 -0.493438 -0.743830 -0.705769  1.0   1  \n",
            "27  1.344660  0.292413  0.468332  1.126096  1.0   0  \n",
            "28 -0.997746 -0.768170 -0.834729 -0.705769  0.0   1  \n",
            "29  1.344660  0.292413  0.468332  1.126096  1.0   1  \n",
            "30 -1.045375 -0.932500 -0.896321 -0.705769  1.0   1  \n",
            "31 -0.298986 -0.514065 -0.582735 -0.705769  0.0   0  \n",
            "32  0.395284  2.473013  2.730719 -0.705769  0.0   0  \n",
            "33  0.395284  2.473013  2.730719 -0.705769  0.0   0  \n",
            "34 -1.215280 -0.947767 -0.917477 -0.705769  1.0   1  \n",
            "35 -0.102652  0.180798  0.405252 -0.705769  0.0   0  \n",
            "36  0.251586 -0.296086 -0.246117 -0.705769  0.0   0  \n",
            "37  1.098832 -0.026861 -0.116593  1.126096  0.0   0  \n",
            "38  0.050038  0.009018 -0.100743 -0.705769  1.0   1  \n",
            "39  2.585000  2.817139  2.571306  1.736718  1.0   1  \n",
            "40  0.251586 -0.296086 -0.246117 -0.705769  0.0   0  \n",
            "41 -0.919079 -0.644039 -0.593604  1.736718  1.0   0  \n",
            "42 -1.045375 -0.932500 -0.896321 -0.705769  1.0   1  \n",
            "43  2.249470  1.713688  0.624122 -0.705769  1.0   1  \n",
            "44 -0.797790 -0.718681 -0.718598 -0.705769  0.0   1  \n",
            "45 -0.626621 -0.792831 -0.764792 -0.705769  1.0   1  \n",
            "46 -0.298986 -0.514065 -0.582735 -0.705769  0.0   0  \n",
            "47  0.395284  2.473013  2.730719 -0.705769  0.0   0  \n",
            "48  1.049271  1.118267  1.003893  1.431407  1.0   1  \n",
            "49 -0.058790 -0.544583 -0.759293 -0.705769  1.0   0  \n",
            "50 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "\n",
            "[51 rows x 39 columns]\n",
            "lr = 0.01, wd = 0.001, mm = 0.5, ld = 0.9\n",
            "epoch: 1000  loss: 0.45366350\n",
            "epoch: 2000  loss: 0.12778947\n",
            "epoch: 3000  loss: 0.03964520\n",
            "epoch: 4000  loss: 0.01890809\n",
            "epoch: 5000  loss: 0.01207951\n",
            "epoch: 1000  loss: 0.07334796\n",
            "epoch: 2000  loss: 0.03813857\n",
            "epoch: 3000  loss: 0.02368576\n",
            "epoch: 4000  loss: 0.01747133\n",
            "epoch: 5000  loss: 0.01406434\n",
            "epoch: 1000  loss: 0.01483188\n",
            "epoch: 2000  loss: 0.01072061\n",
            "epoch: 3000  loss: 0.00876496\n",
            "epoch: 4000  loss: 0.00764767\n",
            "epoch: 5000  loss: 0.00695470\n",
            "epoch: 1000  loss: 0.01247162\n",
            "epoch: 2000  loss: 0.01011260\n",
            "epoch: 3000  loss: 0.00888192\n",
            "epoch: 4000  loss: 0.00807969\n",
            "epoch: 5000  loss: 0.00750836\n",
            "epoch: 1000  loss: 0.00964207\n",
            "epoch: 2000  loss: 0.00834601\n",
            "epoch: 3000  loss: 0.00765684\n",
            "epoch: 4000  loss: 0.00720031\n",
            "epoch: 5000  loss: 0.00686306\n",
            "correct prediction: 0\n",
            "correct prediction: 4\n",
            "correct prediction: 5\n",
            "correct prediction: 6\n",
            "correct prediction: 7\n",
            "correct prediction: 8\n",
            "correct prediction: 9\n",
            "correct prediction: 10\n",
            "correct prediction: 11\n",
            "correct prediction: 12\n",
            "correct prediction: 15\n",
            "correct prediction: 16\n",
            "correct prediction: 17\n",
            "correct prediction: 19\n",
            "correct prediction: 21\n",
            "correct prediction: 22\n",
            "correct prediction: 24\n",
            "correct prediction: 26\n",
            "correct prediction: 27\n",
            "correct prediction: 29\n",
            "correct prediction: 31\n",
            "correct prediction: 32\n",
            "correct prediction: 33\n",
            "correct prediction: 34\n",
            "correct prediction: 35\n",
            "correct prediction: 36\n",
            "correct prediction: 39\n",
            "correct prediction: 43\n",
            "correct prediction: 44\n",
            "correct prediction: 45\n",
            "correct prediction: 46\n",
            "correct prediction: 47\n",
            "correct prediction: 48\n",
            "correct prediction: 50\n",
            "\n",
            "loss on the test set = 3.558\n",
            "Accuracy on the test set: 66.6700%\n",
            "          0         1         2         3         4         5         6   \\\n",
            "0   0.235795 -0.596528  0.764939  0.155125 -0.709392 -1.921546 -0.110274   \n",
            "1  -2.949119 -2.031854  0.231524  0.141733  2.474848  0.246714  0.185667   \n",
            "2  -0.228339 -0.359309 -0.337852  0.821626 -0.872955  0.984931  0.392991   \n",
            "3  -1.213337  2.146571 -0.274030  0.495122 -1.323094  1.240324 -0.271201   \n",
            "4   1.122109  0.807842  0.980374 -0.072595 -0.186554  0.357749 -0.860289   \n",
            "5   0.633887 -0.313952  0.828973 -0.531478  0.995154  0.668412 -0.913099   \n",
            "6  -1.196963  0.714636  0.874417 -2.132932 -0.346888 -0.139611  0.275690   \n",
            "7  -0.208678  0.924384  0.068978 -0.392970 -0.002467  0.156285 -0.486114   \n",
            "8  -0.459825 -0.898318 -1.508754  1.435905  0.692827  3.039905 -0.190101   \n",
            "9  -0.822481 -0.223209 -0.019506  1.332998 -1.304332 -1.430331  0.158815   \n",
            "10  4.518938  1.767920 -0.655172 -0.114569 -1.247974  0.208459 -0.956752   \n",
            "11  0.807073  0.455789  1.731345 -0.566816  1.194391  0.115341  1.135136   \n",
            "12  0.127495  0.425414 -0.007127 -1.479204 -1.642907 -0.709907 -0.342184   \n",
            "13 -1.044644 -0.399502 -0.448795  0.109540  1.014075  0.344043 -0.407666   \n",
            "14  0.703290 -0.205306 -1.139115  1.155272 -1.439506  0.086987 -1.145898   \n",
            "15 -0.987320 -0.488019  0.505248  0.574651 -0.600331  0.469882  1.185208   \n",
            "16 -1.590583 -0.297732 -0.862299 -0.494346  0.679952 -1.054007  0.043731   \n",
            "17  0.961275  1.291009  0.965461 -0.210189 -0.250317 -1.335828 -0.919517   \n",
            "18 -0.675030 -0.008716 -1.001394  1.176765 -0.357519  1.582644  1.515647   \n",
            "19  0.670038  0.478898  0.133934  0.317207 -0.383914  2.476607 -2.575679   \n",
            "20  1.780669  1.033887 -0.043665  0.135341  0.036268 -0.694678 -1.321278   \n",
            "21 -2.905534  0.963953  1.057505 -0.600924  0.621815 -0.175828  1.888403   \n",
            "22 -0.484269 -0.717361 -1.216896 -1.341853  1.359862 -0.333006  0.239972   \n",
            "23  0.770300  0.286824 -0.522803  0.170265 -0.770391 -0.034662 -0.657823   \n",
            "24  0.171093  0.955744 -1.376500  0.614331  0.794085 -0.985971 -1.033522   \n",
            "25 -1.382818 -0.457078 -0.456693  2.177205  0.242991 -0.474857  0.064215   \n",
            "26 -0.986676  0.818727  0.758720 -0.899834 -0.734657 -1.053275 -0.060929   \n",
            "27  1.099488 -0.697891 -0.403934 -0.176542  0.164813  0.451029 -1.484485   \n",
            "28  2.423820  1.457278  0.317998 -2.058740 -0.420090 -0.691256  0.167741   \n",
            "29  0.503652 -0.642579 -0.903764  0.374938 -0.213290  0.795397 -0.387991   \n",
            "30  1.014337 -1.608052  0.375635 -2.259168 -0.696333 -1.079305 -0.400122   \n",
            "31  3.116689  1.723113  0.681957 -1.420649 -1.193706 -0.665680 -0.091697   \n",
            "32 -1.517675 -1.205269 -0.024106  0.664088  3.003847  0.674357 -0.142209   \n",
            "33 -1.125131  0.378030 -0.374538  0.361702  1.661269  1.541844  1.134609   \n",
            "34  0.007997  0.717900 -1.084321  0.565784  0.038188 -1.910262 -0.709885   \n",
            "35  1.317641  0.414077  0.614004 -0.419078 -1.573296  0.660523  1.796185   \n",
            "36  0.786040  0.074047 -0.166161 -0.262730  0.763262  0.706814  2.259574   \n",
            "37  0.011365 -0.370733  0.261825  0.013103  1.260039 -0.554880  0.191121   \n",
            "38 -0.560580  0.164782  0.557392 -1.035448  0.752872 -1.385119 -0.630531   \n",
            "39 -0.725218  0.331052 -0.523784  0.194932 -0.856695  0.352498 -0.053287   \n",
            "40  0.803086 -0.835295 -0.639992 -0.869191 -1.658046  0.863716  0.382231   \n",
            "41  0.272421  1.009930  1.523204 -1.375841 -1.603089 -0.350015  0.297634   \n",
            "42 -2.727341  0.145088  0.484466  1.042311  1.162478 -1.482644 -0.157254   \n",
            "43 -0.789957  0.549711  0.977592 -0.019222 -0.048327 -0.215502  0.429913   \n",
            "44 -3.751709 -0.048892  1.479589  1.088468  1.030387  0.744044  1.597948   \n",
            "45  0.261220  1.287996  0.349535 -0.324269 -1.129768 -1.046599  0.082300   \n",
            "46  1.947578  1.323064 -0.352006 -1.286524 -1.308874  0.670241 -0.144401   \n",
            "47 -1.549491 -1.522130 -0.313656  0.316227 -0.249302  1.104591 -0.581757   \n",
            "48  1.092981 -1.028857 -0.250087 -1.441544 -1.115122  1.402114 -1.131119   \n",
            "49  0.572089  0.007242 -0.636844  0.853076  1.122678  1.509380 -1.020430   \n",
            "50 -2.689340 -0.277448  1.042525  1.452017  0.123011  1.133931  0.748255   \n",
            "\n",
            "          7         8         9   ...        29        30        31        32  \\\n",
            "0   0.382224 -1.064994 -2.581160  ...  0.219561 -1.164105 -2.302173 -1.035291   \n",
            "1  -0.375862 -0.384313 -0.203708  ... -0.154425  0.859029  0.434372 -0.076124   \n",
            "2  -1.113034 -1.138260 -0.083353  ...  0.030184 -1.164105  0.434372  0.883043   \n",
            "3  -0.098764  0.337893  2.173916  ...  2.094682 -1.164105  0.434372  0.403459   \n",
            "4   1.165677  1.248191  0.310892  ...  0.263554 -1.164105  0.434372  0.883043   \n",
            "5   1.512495 -0.868699 -1.661515  ...  0.149895  0.859029 -2.302173  0.883043   \n",
            "6  -0.951835  1.527703 -0.403727  ... -0.395063  0.859029 -2.302173 -0.555708   \n",
            "7  -0.173060  0.068664  0.033924  ... -0.087071  0.859029  0.434372 -0.555708   \n",
            "8   0.584333 -1.482787  1.048752  ...  1.611295  0.859029  0.434372 -0.076124   \n",
            "9   0.488403  0.530823 -1.020090  ... -0.079679  0.859029  0.434372 -0.076124   \n",
            "10  0.252102  0.355264  1.974272  ...  0.574427  0.859029  0.434372 -0.076124   \n",
            "11  1.379504 -0.101004 -0.011852  ... -2.185978 -1.164105 -2.302173  2.321793   \n",
            "12 -1.226289  1.084064  0.252926  ... -0.787906 -1.164105  0.434372  0.403459   \n",
            "13  0.077320 -0.676263  0.871215  ...  1.041880  0.859029  0.434372 -0.555708   \n",
            "14  0.471782  0.232531  2.293824  ...  2.185123  0.859029  0.434372 -0.076124   \n",
            "15 -0.273246 -0.531004 -0.567577  ... -0.876400 -1.164105 -2.302173  2.321793   \n",
            "16 -0.891745 -0.625318 -1.002406  ...  1.048573 -1.164105 -2.302173 -1.035291   \n",
            "17  1.668175 -0.032122 -0.608809  ... -0.085208  0.859029  0.434372 -0.076124   \n",
            "18 -1.201217 -1.086889  0.143054  ... -0.610362  0.859029  0.434372 -0.555708   \n",
            "19  3.880914 -2.190124  0.326164  ...  2.908866  0.859029  0.434372 -0.076124   \n",
            "20  1.928448 -0.231571  1.163341  ...  0.765533 -1.164105  0.434372  0.883043   \n",
            "21  0.477749 -0.935990 -0.204657  ...  0.077658  0.859029  0.434372 -0.076124   \n",
            "22  0.308476  1.260645  0.242491  ... -0.227157  0.859029  0.434372  0.403459   \n",
            "23  0.957006 -0.012627 -0.059101  ... -0.139646  0.859029  0.434372 -0.076124   \n",
            "24 -0.577370 -1.338864 -0.805743  ...  1.489135  0.859029  0.434372  0.403459   \n",
            "25  0.012977 -0.374317  0.811695  ...  0.989533 -1.164105  0.434372  0.403459   \n",
            "26 -0.306770  0.288864  0.338384  ... -0.651107  0.859029  0.434372 -0.555708   \n",
            "27  0.342000 -0.040139 -0.198566  ...  1.079230 -1.164105  0.434372  0.403459   \n",
            "28  1.034245  0.167103 -0.535725  ... -0.677696  0.859029  0.434372 -0.555708   \n",
            "29  0.128518 -0.699788  0.538024  ...  0.641478 -1.164105  0.434372  0.403459   \n",
            "30 -0.226448  1.329023 -0.446898  ... -1.456581 -1.164105  0.434372 -0.555708   \n",
            "31  1.514548 -0.396321 -0.823777  ...  0.047431 -1.164105  0.434372 -1.994458   \n",
            "32 -0.628094 -1.419638  1.878376  ...  0.980789 -1.164105  0.434372  1.842209   \n",
            "33 -2.042031  0.616344  1.442150  ... -0.415767 -1.164105  0.434372  1.842209   \n",
            "34  0.442049  0.151550  0.522742  ...  0.479677 -1.164105  0.434372  0.403459   \n",
            "35  0.938744  1.178404  0.439759  ... -0.505916  0.859029  0.434372 -0.076124   \n",
            "36 -1.782716  1.869428  0.239494  ... -0.741127  0.859029  0.434372  0.403459   \n",
            "37  0.257665 -0.422206 -1.140060  ... -1.971977 -1.164105  0.434372  0.883043   \n",
            "38 -1.204079  1.796819  0.624790  ... -0.385871  0.859029  0.434372  0.403459   \n",
            "39  0.339062 -0.523117 -0.516064  ...  0.357690 -1.164105 -2.302173 -1.035291   \n",
            "40  0.349595 -1.054598 -1.122964  ... -1.632031  0.859029  0.434372  0.403459   \n",
            "41  0.143264  1.653829  0.386328  ... -1.314442  0.859029 -2.302173  0.883043   \n",
            "42 -0.797886 -0.517182 -0.206603  ...  0.727034 -1.164105  0.434372 -0.555708   \n",
            "43  0.702972  0.181321  0.734456  ...  0.258117  0.859029  0.434372  0.883043   \n",
            "44  0.149512 -0.831327  1.386731  ... -0.265317  0.859029  0.434372 -0.076124   \n",
            "45  0.076983  0.999414 -0.797919  ...  0.080311 -1.164105  0.434372  0.403459   \n",
            "46  0.690211  2.110784  0.259129  ... -0.905546 -1.164105  0.434372 -1.994458   \n",
            "47 -0.235874 -1.889797  1.388273  ...  0.720933 -1.164105  0.434372  1.842209   \n",
            "48 -0.576694  1.359136  0.334442  ... -1.248674  0.859029 -2.302173 -0.555708   \n",
            "49  1.017679 -1.414678  0.752618  ...  0.679185  0.859029  0.434372 -1.514875   \n",
            "50  0.395918 -0.451649 -0.826654  ... -0.224069  0.859029  0.434372 -0.076124   \n",
            "\n",
            "          33        34        35        36   37  38  \n",
            "0   2.585000  2.817139  2.571306  1.736718  1.0   1  \n",
            "1  -0.797790 -0.718681 -0.718598 -0.705769  0.0   1  \n",
            "2   1.098832 -0.026861 -0.116593  1.126096  0.0   1  \n",
            "3   1.344660  0.292413  0.468332  1.126096  1.0   0  \n",
            "4   1.098832 -0.026861 -0.116593  1.126096  0.0   0  \n",
            "5  -0.919079 -0.644039 -0.593604  1.736718  1.0   1  \n",
            "6   1.049271  1.118267  1.003893  1.431407  1.0   1  \n",
            "7  -0.290092 -0.493438 -0.743830 -0.705769  1.0   1  \n",
            "8  -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "9  -0.102652  0.180798  0.405252 -0.705769  0.0   0  \n",
            "10 -0.303103 -0.272472 -0.300268  1.655301  0.0   0  \n",
            "11  0.820384  0.663055  0.713856  1.187158  1.0   1  \n",
            "12 -0.626621 -0.792831 -0.764792 -0.705769  1.0   1  \n",
            "13 -0.290092 -0.493438 -0.743830 -0.705769  1.0   0  \n",
            "14 -0.797790 -0.718681 -0.718598 -0.705769  0.0   1  \n",
            "15  0.820384  0.663055  0.713856  1.187158  1.0   1  \n",
            "16  0.789663  0.562746  0.645536  1.349991  1.0   1  \n",
            "17 -0.102652  0.180798  0.405252 -0.705769  0.0   0  \n",
            "18 -0.997746 -0.768170 -0.834729 -0.705769  0.0   1  \n",
            "19 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "20  1.098832 -0.026861 -0.116593  1.126096  0.0   1  \n",
            "21 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "22  0.251586 -0.296086 -0.246117 -0.705769  0.0   0  \n",
            "23 -0.797790 -0.718681 -0.718598 -0.705769  0.0   1  \n",
            "24  0.251586 -0.296086 -0.246117 -0.705769  0.0   0  \n",
            "25  1.344660  0.292413  0.468332  1.126096  1.0   0  \n",
            "26 -0.290092 -0.493438 -0.743830 -0.705769  1.0   1  \n",
            "27  1.344660  0.292413  0.468332  1.126096  1.0   1  \n",
            "28 -0.997746 -0.768170 -0.834729 -0.705769  0.0   1  \n",
            "29  1.344660  0.292413  0.468332  1.126096  1.0   1  \n",
            "30 -1.045375 -0.932500 -0.896321 -0.705769  1.0   0  \n",
            "31 -0.298986 -0.514065 -0.582735 -0.705769  0.0   0  \n",
            "32  0.395284  2.473013  2.730719 -0.705769  0.0   0  \n",
            "33  0.395284  2.473013  2.730719 -0.705769  0.0   0  \n",
            "34 -1.215280 -0.947767 -0.917477 -0.705769  1.0   1  \n",
            "35 -0.102652  0.180798  0.405252 -0.705769  0.0   0  \n",
            "36  0.251586 -0.296086 -0.246117 -0.705769  0.0   0  \n",
            "37  1.098832 -0.026861 -0.116593  1.126096  0.0   1  \n",
            "38  0.050038  0.009018 -0.100743 -0.705769  1.0   0  \n",
            "39  2.585000  2.817139  2.571306  1.736718  1.0   1  \n",
            "40  0.251586 -0.296086 -0.246117 -0.705769  0.0   1  \n",
            "41 -0.919079 -0.644039 -0.593604  1.736718  1.0   0  \n",
            "42 -1.045375 -0.932500 -0.896321 -0.705769  1.0   0  \n",
            "43  2.249470  1.713688  0.624122 -0.705769  1.0   1  \n",
            "44 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "45 -0.626621 -0.792831 -0.764792 -0.705769  1.0   1  \n",
            "46 -0.298986 -0.514065 -0.582735 -0.705769  0.0   0  \n",
            "47  0.395284  2.473013  2.730719 -0.705769  0.0   0  \n",
            "48  1.049271  1.118267  1.003893  1.431407  1.0   1  \n",
            "49 -0.058790 -0.544583 -0.759293 -0.705769  1.0   0  \n",
            "50 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "\n",
            "[51 rows x 39 columns]\n",
            "lr = 0.01, wd = 0.001, mm = 0.725, ld = 1\n",
            "epoch: 1000  loss: 0.12809427\n",
            "epoch: 2000  loss: 0.03882213\n",
            "epoch: 3000  loss: 0.03301888\n",
            "epoch: 4000  loss: 0.03149040\n",
            "epoch: 5000  loss: 0.03092790\n",
            "epoch: 1000  loss: 0.05366263\n",
            "epoch: 2000  loss: 0.04167121\n",
            "epoch: 3000  loss: 0.04086034\n",
            "epoch: 4000  loss: 0.04056732\n",
            "epoch: 5000  loss: 0.04044754\n",
            "epoch: 1000  loss: 0.05617768\n",
            "epoch: 2000  loss: 0.05600057\n",
            "epoch: 3000  loss: 0.05598494\n",
            "epoch: 4000  loss: 0.05600355\n",
            "epoch: 5000  loss: 0.05603871\n",
            "epoch: 1000  loss: 0.03541252\n",
            "epoch: 2000  loss: 0.03515518\n",
            "epoch: 3000  loss: 0.03507440\n",
            "epoch: 4000  loss: 0.03506356\n",
            "epoch: 5000  loss: 0.03507717\n",
            "epoch: 1000  loss: 0.05289763\n",
            "epoch: 2000  loss: 0.05268065\n",
            "epoch: 3000  loss: 0.05263750\n",
            "epoch: 4000  loss: 0.05264735\n",
            "epoch: 5000  loss: 0.05266383\n",
            "correct prediction: 0\n",
            "correct prediction: 1\n",
            "correct prediction: 3\n",
            "correct prediction: 5\n",
            "correct prediction: 6\n",
            "correct prediction: 7\n",
            "correct prediction: 9\n",
            "correct prediction: 11\n",
            "correct prediction: 12\n",
            "correct prediction: 14\n",
            "correct prediction: 15\n",
            "correct prediction: 16\n",
            "correct prediction: 19\n",
            "correct prediction: 21\n",
            "correct prediction: 24\n",
            "correct prediction: 26\n",
            "correct prediction: 27\n",
            "correct prediction: 31\n",
            "correct prediction: 32\n",
            "correct prediction: 33\n",
            "correct prediction: 34\n",
            "correct prediction: 37\n",
            "correct prediction: 38\n",
            "correct prediction: 39\n",
            "correct prediction: 40\n",
            "correct prediction: 41\n",
            "correct prediction: 43\n",
            "correct prediction: 44\n",
            "correct prediction: 47\n",
            "correct prediction: 48\n",
            "correct prediction: 49\n",
            "correct prediction: 50\n",
            "\n",
            "loss on the test set = 1.229\n",
            "Accuracy on the test set: 62.7500%\n",
            "          0         1         2         3         4         5         6   \\\n",
            "0   0.235795 -0.596528  0.764939  0.155125 -0.709392 -1.921546 -0.110274   \n",
            "1  -2.949119 -2.031854  0.231524  0.141733  2.474848  0.246714  0.185667   \n",
            "2  -0.228339 -0.359309 -0.337852  0.821626 -0.872955  0.984931  0.392991   \n",
            "3  -1.213337  2.146571 -0.274030  0.495122 -1.323094  1.240324 -0.271201   \n",
            "4   1.122109  0.807842  0.980374 -0.072595 -0.186554  0.357749 -0.860289   \n",
            "5   0.633887 -0.313952  0.828973 -0.531478  0.995154  0.668412 -0.913099   \n",
            "6  -1.196963  0.714636  0.874417 -2.132932 -0.346888 -0.139611  0.275690   \n",
            "7  -0.208678  0.924384  0.068978 -0.392970 -0.002467  0.156285 -0.486114   \n",
            "8  -0.459825 -0.898318 -1.508754  1.435905  0.692827  3.039905 -0.190101   \n",
            "9  -0.822481 -0.223209 -0.019506  1.332998 -1.304332 -1.430331  0.158815   \n",
            "10  4.518938  1.767920 -0.655172 -0.114569 -1.247974  0.208459 -0.956752   \n",
            "11  0.807073  0.455789  1.731345 -0.566816  1.194391  0.115341  1.135136   \n",
            "12  0.127495  0.425414 -0.007127 -1.479204 -1.642907 -0.709907 -0.342184   \n",
            "13 -1.044644 -0.399502 -0.448795  0.109540  1.014075  0.344043 -0.407666   \n",
            "14  0.703290 -0.205306 -1.139115  1.155272 -1.439506  0.086987 -1.145898   \n",
            "15 -0.987320 -0.488019  0.505248  0.574651 -0.600331  0.469882  1.185208   \n",
            "16 -1.590583 -0.297732 -0.862299 -0.494346  0.679952 -1.054007  0.043731   \n",
            "17  0.961275  1.291009  0.965461 -0.210189 -0.250317 -1.335828 -0.919517   \n",
            "18 -0.675030 -0.008716 -1.001394  1.176765 -0.357519  1.582644  1.515647   \n",
            "19  0.670038  0.478898  0.133934  0.317207 -0.383914  2.476607 -2.575679   \n",
            "20  1.780669  1.033887 -0.043665  0.135341  0.036268 -0.694678 -1.321278   \n",
            "21 -2.905534  0.963953  1.057505 -0.600924  0.621815 -0.175828  1.888403   \n",
            "22 -0.484269 -0.717361 -1.216896 -1.341853  1.359862 -0.333006  0.239972   \n",
            "23  0.770300  0.286824 -0.522803  0.170265 -0.770391 -0.034662 -0.657823   \n",
            "24  0.171093  0.955744 -1.376500  0.614331  0.794085 -0.985971 -1.033522   \n",
            "25 -1.382818 -0.457078 -0.456693  2.177205  0.242991 -0.474857  0.064215   \n",
            "26 -0.986676  0.818727  0.758720 -0.899834 -0.734657 -1.053275 -0.060929   \n",
            "27  1.099488 -0.697891 -0.403934 -0.176542  0.164813  0.451029 -1.484485   \n",
            "28  2.423820  1.457278  0.317998 -2.058740 -0.420090 -0.691256  0.167741   \n",
            "29  0.503652 -0.642579 -0.903764  0.374938 -0.213290  0.795397 -0.387991   \n",
            "30  1.014337 -1.608052  0.375635 -2.259168 -0.696333 -1.079305 -0.400122   \n",
            "31  3.116689  1.723113  0.681957 -1.420649 -1.193706 -0.665680 -0.091697   \n",
            "32 -1.517675 -1.205269 -0.024106  0.664088  3.003847  0.674357 -0.142209   \n",
            "33 -1.125131  0.378030 -0.374538  0.361702  1.661269  1.541844  1.134609   \n",
            "34  0.007997  0.717900 -1.084321  0.565784  0.038188 -1.910262 -0.709885   \n",
            "35  1.317641  0.414077  0.614004 -0.419078 -1.573296  0.660523  1.796185   \n",
            "36  0.786040  0.074047 -0.166161 -0.262730  0.763262  0.706814  2.259574   \n",
            "37  0.011365 -0.370733  0.261825  0.013103  1.260039 -0.554880  0.191121   \n",
            "38 -0.560580  0.164782  0.557392 -1.035448  0.752872 -1.385119 -0.630531   \n",
            "39 -0.725218  0.331052 -0.523784  0.194932 -0.856695  0.352498 -0.053287   \n",
            "40  0.803086 -0.835295 -0.639992 -0.869191 -1.658046  0.863716  0.382231   \n",
            "41  0.272421  1.009930  1.523204 -1.375841 -1.603089 -0.350015  0.297634   \n",
            "42 -2.727341  0.145088  0.484466  1.042311  1.162478 -1.482644 -0.157254   \n",
            "43 -0.789957  0.549711  0.977592 -0.019222 -0.048327 -0.215502  0.429913   \n",
            "44 -3.751709 -0.048892  1.479589  1.088468  1.030387  0.744044  1.597948   \n",
            "45  0.261220  1.287996  0.349535 -0.324269 -1.129768 -1.046599  0.082300   \n",
            "46  1.947578  1.323064 -0.352006 -1.286524 -1.308874  0.670241 -0.144401   \n",
            "47 -1.549491 -1.522130 -0.313656  0.316227 -0.249302  1.104591 -0.581757   \n",
            "48  1.092981 -1.028857 -0.250087 -1.441544 -1.115122  1.402114 -1.131119   \n",
            "49  0.572089  0.007242 -0.636844  0.853076  1.122678  1.509380 -1.020430   \n",
            "50 -2.689340 -0.277448  1.042525  1.452017  0.123011  1.133931  0.748255   \n",
            "\n",
            "          7         8         9   ...        29        30        31        32  \\\n",
            "0   0.382224 -1.064994 -2.581160  ...  0.219561 -1.164105 -2.302173 -1.035291   \n",
            "1  -0.375862 -0.384313 -0.203708  ... -0.154425  0.859029  0.434372 -0.076124   \n",
            "2  -1.113034 -1.138260 -0.083353  ...  0.030184 -1.164105  0.434372  0.883043   \n",
            "3  -0.098764  0.337893  2.173916  ...  2.094682 -1.164105  0.434372  0.403459   \n",
            "4   1.165677  1.248191  0.310892  ...  0.263554 -1.164105  0.434372  0.883043   \n",
            "5   1.512495 -0.868699 -1.661515  ...  0.149895  0.859029 -2.302173  0.883043   \n",
            "6  -0.951835  1.527703 -0.403727  ... -0.395063  0.859029 -2.302173 -0.555708   \n",
            "7  -0.173060  0.068664  0.033924  ... -0.087071  0.859029  0.434372 -0.555708   \n",
            "8   0.584333 -1.482787  1.048752  ...  1.611295  0.859029  0.434372 -0.076124   \n",
            "9   0.488403  0.530823 -1.020090  ... -0.079679  0.859029  0.434372 -0.076124   \n",
            "10  0.252102  0.355264  1.974272  ...  0.574427  0.859029  0.434372 -0.076124   \n",
            "11  1.379504 -0.101004 -0.011852  ... -2.185978 -1.164105 -2.302173  2.321793   \n",
            "12 -1.226289  1.084064  0.252926  ... -0.787906 -1.164105  0.434372  0.403459   \n",
            "13  0.077320 -0.676263  0.871215  ...  1.041880  0.859029  0.434372 -0.555708   \n",
            "14  0.471782  0.232531  2.293824  ...  2.185123  0.859029  0.434372 -0.076124   \n",
            "15 -0.273246 -0.531004 -0.567577  ... -0.876400 -1.164105 -2.302173  2.321793   \n",
            "16 -0.891745 -0.625318 -1.002406  ...  1.048573 -1.164105 -2.302173 -1.035291   \n",
            "17  1.668175 -0.032122 -0.608809  ... -0.085208  0.859029  0.434372 -0.076124   \n",
            "18 -1.201217 -1.086889  0.143054  ... -0.610362  0.859029  0.434372 -0.555708   \n",
            "19  3.880914 -2.190124  0.326164  ...  2.908866  0.859029  0.434372 -0.076124   \n",
            "20  1.928448 -0.231571  1.163341  ...  0.765533 -1.164105  0.434372  0.883043   \n",
            "21  0.477749 -0.935990 -0.204657  ...  0.077658  0.859029  0.434372 -0.076124   \n",
            "22  0.308476  1.260645  0.242491  ... -0.227157  0.859029  0.434372  0.403459   \n",
            "23  0.957006 -0.012627 -0.059101  ... -0.139646  0.859029  0.434372 -0.076124   \n",
            "24 -0.577370 -1.338864 -0.805743  ...  1.489135  0.859029  0.434372  0.403459   \n",
            "25  0.012977 -0.374317  0.811695  ...  0.989533 -1.164105  0.434372  0.403459   \n",
            "26 -0.306770  0.288864  0.338384  ... -0.651107  0.859029  0.434372 -0.555708   \n",
            "27  0.342000 -0.040139 -0.198566  ...  1.079230 -1.164105  0.434372  0.403459   \n",
            "28  1.034245  0.167103 -0.535725  ... -0.677696  0.859029  0.434372 -0.555708   \n",
            "29  0.128518 -0.699788  0.538024  ...  0.641478 -1.164105  0.434372  0.403459   \n",
            "30 -0.226448  1.329023 -0.446898  ... -1.456581 -1.164105  0.434372 -0.555708   \n",
            "31  1.514548 -0.396321 -0.823777  ...  0.047431 -1.164105  0.434372 -1.994458   \n",
            "32 -0.628094 -1.419638  1.878376  ...  0.980789 -1.164105  0.434372  1.842209   \n",
            "33 -2.042031  0.616344  1.442150  ... -0.415767 -1.164105  0.434372  1.842209   \n",
            "34  0.442049  0.151550  0.522742  ...  0.479677 -1.164105  0.434372  0.403459   \n",
            "35  0.938744  1.178404  0.439759  ... -0.505916  0.859029  0.434372 -0.076124   \n",
            "36 -1.782716  1.869428  0.239494  ... -0.741127  0.859029  0.434372  0.403459   \n",
            "37  0.257665 -0.422206 -1.140060  ... -1.971977 -1.164105  0.434372  0.883043   \n",
            "38 -1.204079  1.796819  0.624790  ... -0.385871  0.859029  0.434372  0.403459   \n",
            "39  0.339062 -0.523117 -0.516064  ...  0.357690 -1.164105 -2.302173 -1.035291   \n",
            "40  0.349595 -1.054598 -1.122964  ... -1.632031  0.859029  0.434372  0.403459   \n",
            "41  0.143264  1.653829  0.386328  ... -1.314442  0.859029 -2.302173  0.883043   \n",
            "42 -0.797886 -0.517182 -0.206603  ...  0.727034 -1.164105  0.434372 -0.555708   \n",
            "43  0.702972  0.181321  0.734456  ...  0.258117  0.859029  0.434372  0.883043   \n",
            "44  0.149512 -0.831327  1.386731  ... -0.265317  0.859029  0.434372 -0.076124   \n",
            "45  0.076983  0.999414 -0.797919  ...  0.080311 -1.164105  0.434372  0.403459   \n",
            "46  0.690211  2.110784  0.259129  ... -0.905546 -1.164105  0.434372 -1.994458   \n",
            "47 -0.235874 -1.889797  1.388273  ...  0.720933 -1.164105  0.434372  1.842209   \n",
            "48 -0.576694  1.359136  0.334442  ... -1.248674  0.859029 -2.302173 -0.555708   \n",
            "49  1.017679 -1.414678  0.752618  ...  0.679185  0.859029  0.434372 -1.514875   \n",
            "50  0.395918 -0.451649 -0.826654  ... -0.224069  0.859029  0.434372 -0.076124   \n",
            "\n",
            "          33        34        35        36   37  38  \n",
            "0   2.585000  2.817139  2.571306  1.736718  1.0   1  \n",
            "1  -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "2   1.098832 -0.026861 -0.116593  1.126096  0.0   1  \n",
            "3   1.344660  0.292413  0.468332  1.126096  1.0   1  \n",
            "4   1.098832 -0.026861 -0.116593  1.126096  0.0   1  \n",
            "5  -0.919079 -0.644039 -0.593604  1.736718  1.0   1  \n",
            "6   1.049271  1.118267  1.003893  1.431407  1.0   1  \n",
            "7  -0.290092 -0.493438 -0.743830 -0.705769  1.0   1  \n",
            "8  -0.797790 -0.718681 -0.718598 -0.705769  0.0   1  \n",
            "9  -0.102652  0.180798  0.405252 -0.705769  0.0   0  \n",
            "10 -0.303103 -0.272472 -0.300268  1.655301  0.0   1  \n",
            "11  0.820384  0.663055  0.713856  1.187158  1.0   1  \n",
            "12 -0.626621 -0.792831 -0.764792 -0.705769  1.0   1  \n",
            "13 -0.290092 -0.493438 -0.743830 -0.705769  1.0   0  \n",
            "14 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "15  0.820384  0.663055  0.713856  1.187158  1.0   1  \n",
            "16  0.789663  0.562746  0.645536  1.349991  1.0   1  \n",
            "17 -0.102652  0.180798  0.405252 -0.705769  0.0   1  \n",
            "18 -0.997746 -0.768170 -0.834729 -0.705769  0.0   1  \n",
            "19 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "20  1.098832 -0.026861 -0.116593  1.126096  0.0   1  \n",
            "21 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "22  0.251586 -0.296086 -0.246117 -0.705769  0.0   1  \n",
            "23 -0.797790 -0.718681 -0.718598 -0.705769  0.0   1  \n",
            "24  0.251586 -0.296086 -0.246117 -0.705769  0.0   0  \n",
            "25  1.344660  0.292413  0.468332  1.126096  1.0   0  \n",
            "26 -0.290092 -0.493438 -0.743830 -0.705769  1.0   1  \n",
            "27  1.344660  0.292413  0.468332  1.126096  1.0   1  \n",
            "28 -0.997746 -0.768170 -0.834729 -0.705769  0.0   1  \n",
            "29  1.344660  0.292413  0.468332  1.126096  1.0   0  \n",
            "30 -1.045375 -0.932500 -0.896321 -0.705769  1.0   0  \n",
            "31 -0.298986 -0.514065 -0.582735 -0.705769  0.0   0  \n",
            "32  0.395284  2.473013  2.730719 -0.705769  0.0   0  \n",
            "33  0.395284  2.473013  2.730719 -0.705769  0.0   0  \n",
            "34 -1.215280 -0.947767 -0.917477 -0.705769  1.0   1  \n",
            "35 -0.102652  0.180798  0.405252 -0.705769  0.0   1  \n",
            "36  0.251586 -0.296086 -0.246117 -0.705769  0.0   1  \n",
            "37  1.098832 -0.026861 -0.116593  1.126096  0.0   0  \n",
            "38  0.050038  0.009018 -0.100743 -0.705769  1.0   1  \n",
            "39  2.585000  2.817139  2.571306  1.736718  1.0   1  \n",
            "40  0.251586 -0.296086 -0.246117 -0.705769  0.0   0  \n",
            "41 -0.919079 -0.644039 -0.593604  1.736718  1.0   1  \n",
            "42 -1.045375 -0.932500 -0.896321 -0.705769  1.0   0  \n",
            "43  2.249470  1.713688  0.624122 -0.705769  1.0   1  \n",
            "44 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "45 -0.626621 -0.792831 -0.764792 -0.705769  1.0   0  \n",
            "46 -0.298986 -0.514065 -0.582735 -0.705769  0.0   1  \n",
            "47  0.395284  2.473013  2.730719 -0.705769  0.0   0  \n",
            "48  1.049271  1.118267  1.003893  1.431407  1.0   1  \n",
            "49 -0.058790 -0.544583 -0.759293 -0.705769  1.0   1  \n",
            "50 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "\n",
            "[51 rows x 39 columns]\n",
            "lr = 0.01, wd = 0.001, mm = 0.725, ld = 0.9\n",
            "epoch: 1000  loss: 0.32889724\n",
            "epoch: 2000  loss: 0.17443684\n",
            "epoch: 3000  loss: 0.13561891\n",
            "epoch: 4000  loss: 0.08417924\n",
            "epoch: 5000  loss: 0.06993359\n",
            "epoch: 1000  loss: 0.11487971\n",
            "epoch: 2000  loss: 0.03137263\n",
            "epoch: 3000  loss: 0.01343516\n",
            "epoch: 4000  loss: 0.00920217\n",
            "epoch: 5000  loss: 0.00724289\n",
            "epoch: 1000  loss: 0.03128785\n",
            "epoch: 2000  loss: 0.00998987\n",
            "epoch: 3000  loss: 0.00735156\n",
            "epoch: 4000  loss: 0.00619270\n",
            "epoch: 5000  loss: 0.00550688\n",
            "epoch: 1000  loss: 0.00774615\n",
            "epoch: 2000  loss: 0.00639376\n",
            "epoch: 3000  loss: 0.00569487\n",
            "epoch: 4000  loss: 0.00525678\n",
            "epoch: 5000  loss: 0.00494701\n",
            "epoch: 1000  loss: 0.00676687\n",
            "epoch: 2000  loss: 0.00617795\n",
            "epoch: 3000  loss: 0.00583018\n",
            "epoch: 4000  loss: 0.00558389\n",
            "epoch: 5000  loss: 0.00539900\n",
            "correct prediction: 0\n",
            "correct prediction: 1\n",
            "correct prediction: 3\n",
            "correct prediction: 4\n",
            "correct prediction: 5\n",
            "correct prediction: 6\n",
            "correct prediction: 7\n",
            "correct prediction: 8\n",
            "correct prediction: 9\n",
            "correct prediction: 10\n",
            "correct prediction: 11\n",
            "correct prediction: 12\n",
            "correct prediction: 14\n",
            "correct prediction: 15\n",
            "correct prediction: 16\n",
            "correct prediction: 18\n",
            "correct prediction: 19\n",
            "correct prediction: 21\n",
            "correct prediction: 22\n",
            "correct prediction: 24\n",
            "correct prediction: 26\n",
            "correct prediction: 27\n",
            "correct prediction: 29\n",
            "correct prediction: 31\n",
            "correct prediction: 32\n",
            "correct prediction: 33\n",
            "correct prediction: 34\n",
            "correct prediction: 36\n",
            "correct prediction: 38\n",
            "correct prediction: 39\n",
            "correct prediction: 41\n",
            "correct prediction: 43\n",
            "correct prediction: 46\n",
            "correct prediction: 47\n",
            "correct prediction: 48\n",
            "correct prediction: 50\n",
            "\n",
            "loss on the test set = 1.077\n",
            "Accuracy on the test set: 70.5900%\n",
            "          0         1         2         3         4         5         6   \\\n",
            "0   0.235795 -0.596528  0.764939  0.155125 -0.709392 -1.921546 -0.110274   \n",
            "1  -2.949119 -2.031854  0.231524  0.141733  2.474848  0.246714  0.185667   \n",
            "2  -0.228339 -0.359309 -0.337852  0.821626 -0.872955  0.984931  0.392991   \n",
            "3  -1.213337  2.146571 -0.274030  0.495122 -1.323094  1.240324 -0.271201   \n",
            "4   1.122109  0.807842  0.980374 -0.072595 -0.186554  0.357749 -0.860289   \n",
            "5   0.633887 -0.313952  0.828973 -0.531478  0.995154  0.668412 -0.913099   \n",
            "6  -1.196963  0.714636  0.874417 -2.132932 -0.346888 -0.139611  0.275690   \n",
            "7  -0.208678  0.924384  0.068978 -0.392970 -0.002467  0.156285 -0.486114   \n",
            "8  -0.459825 -0.898318 -1.508754  1.435905  0.692827  3.039905 -0.190101   \n",
            "9  -0.822481 -0.223209 -0.019506  1.332998 -1.304332 -1.430331  0.158815   \n",
            "10  4.518938  1.767920 -0.655172 -0.114569 -1.247974  0.208459 -0.956752   \n",
            "11  0.807073  0.455789  1.731345 -0.566816  1.194391  0.115341  1.135136   \n",
            "12  0.127495  0.425414 -0.007127 -1.479204 -1.642907 -0.709907 -0.342184   \n",
            "13 -1.044644 -0.399502 -0.448795  0.109540  1.014075  0.344043 -0.407666   \n",
            "14  0.703290 -0.205306 -1.139115  1.155272 -1.439506  0.086987 -1.145898   \n",
            "15 -0.987320 -0.488019  0.505248  0.574651 -0.600331  0.469882  1.185208   \n",
            "16 -1.590583 -0.297732 -0.862299 -0.494346  0.679952 -1.054007  0.043731   \n",
            "17  0.961275  1.291009  0.965461 -0.210189 -0.250317 -1.335828 -0.919517   \n",
            "18 -0.675030 -0.008716 -1.001394  1.176765 -0.357519  1.582644  1.515647   \n",
            "19  0.670038  0.478898  0.133934  0.317207 -0.383914  2.476607 -2.575679   \n",
            "20  1.780669  1.033887 -0.043665  0.135341  0.036268 -0.694678 -1.321278   \n",
            "21 -2.905534  0.963953  1.057505 -0.600924  0.621815 -0.175828  1.888403   \n",
            "22 -0.484269 -0.717361 -1.216896 -1.341853  1.359862 -0.333006  0.239972   \n",
            "23  0.770300  0.286824 -0.522803  0.170265 -0.770391 -0.034662 -0.657823   \n",
            "24  0.171093  0.955744 -1.376500  0.614331  0.794085 -0.985971 -1.033522   \n",
            "25 -1.382818 -0.457078 -0.456693  2.177205  0.242991 -0.474857  0.064215   \n",
            "26 -0.986676  0.818727  0.758720 -0.899834 -0.734657 -1.053275 -0.060929   \n",
            "27  1.099488 -0.697891 -0.403934 -0.176542  0.164813  0.451029 -1.484485   \n",
            "28  2.423820  1.457278  0.317998 -2.058740 -0.420090 -0.691256  0.167741   \n",
            "29  0.503652 -0.642579 -0.903764  0.374938 -0.213290  0.795397 -0.387991   \n",
            "30  1.014337 -1.608052  0.375635 -2.259168 -0.696333 -1.079305 -0.400122   \n",
            "31  3.116689  1.723113  0.681957 -1.420649 -1.193706 -0.665680 -0.091697   \n",
            "32 -1.517675 -1.205269 -0.024106  0.664088  3.003847  0.674357 -0.142209   \n",
            "33 -1.125131  0.378030 -0.374538  0.361702  1.661269  1.541844  1.134609   \n",
            "34  0.007997  0.717900 -1.084321  0.565784  0.038188 -1.910262 -0.709885   \n",
            "35  1.317641  0.414077  0.614004 -0.419078 -1.573296  0.660523  1.796185   \n",
            "36  0.786040  0.074047 -0.166161 -0.262730  0.763262  0.706814  2.259574   \n",
            "37  0.011365 -0.370733  0.261825  0.013103  1.260039 -0.554880  0.191121   \n",
            "38 -0.560580  0.164782  0.557392 -1.035448  0.752872 -1.385119 -0.630531   \n",
            "39 -0.725218  0.331052 -0.523784  0.194932 -0.856695  0.352498 -0.053287   \n",
            "40  0.803086 -0.835295 -0.639992 -0.869191 -1.658046  0.863716  0.382231   \n",
            "41  0.272421  1.009930  1.523204 -1.375841 -1.603089 -0.350015  0.297634   \n",
            "42 -2.727341  0.145088  0.484466  1.042311  1.162478 -1.482644 -0.157254   \n",
            "43 -0.789957  0.549711  0.977592 -0.019222 -0.048327 -0.215502  0.429913   \n",
            "44 -3.751709 -0.048892  1.479589  1.088468  1.030387  0.744044  1.597948   \n",
            "45  0.261220  1.287996  0.349535 -0.324269 -1.129768 -1.046599  0.082300   \n",
            "46  1.947578  1.323064 -0.352006 -1.286524 -1.308874  0.670241 -0.144401   \n",
            "47 -1.549491 -1.522130 -0.313656  0.316227 -0.249302  1.104591 -0.581757   \n",
            "48  1.092981 -1.028857 -0.250087 -1.441544 -1.115122  1.402114 -1.131119   \n",
            "49  0.572089  0.007242 -0.636844  0.853076  1.122678  1.509380 -1.020430   \n",
            "50 -2.689340 -0.277448  1.042525  1.452017  0.123011  1.133931  0.748255   \n",
            "\n",
            "          7         8         9   ...        29        30        31        32  \\\n",
            "0   0.382224 -1.064994 -2.581160  ...  0.219561 -1.164105 -2.302173 -1.035291   \n",
            "1  -0.375862 -0.384313 -0.203708  ... -0.154425  0.859029  0.434372 -0.076124   \n",
            "2  -1.113034 -1.138260 -0.083353  ...  0.030184 -1.164105  0.434372  0.883043   \n",
            "3  -0.098764  0.337893  2.173916  ...  2.094682 -1.164105  0.434372  0.403459   \n",
            "4   1.165677  1.248191  0.310892  ...  0.263554 -1.164105  0.434372  0.883043   \n",
            "5   1.512495 -0.868699 -1.661515  ...  0.149895  0.859029 -2.302173  0.883043   \n",
            "6  -0.951835  1.527703 -0.403727  ... -0.395063  0.859029 -2.302173 -0.555708   \n",
            "7  -0.173060  0.068664  0.033924  ... -0.087071  0.859029  0.434372 -0.555708   \n",
            "8   0.584333 -1.482787  1.048752  ...  1.611295  0.859029  0.434372 -0.076124   \n",
            "9   0.488403  0.530823 -1.020090  ... -0.079679  0.859029  0.434372 -0.076124   \n",
            "10  0.252102  0.355264  1.974272  ...  0.574427  0.859029  0.434372 -0.076124   \n",
            "11  1.379504 -0.101004 -0.011852  ... -2.185978 -1.164105 -2.302173  2.321793   \n",
            "12 -1.226289  1.084064  0.252926  ... -0.787906 -1.164105  0.434372  0.403459   \n",
            "13  0.077320 -0.676263  0.871215  ...  1.041880  0.859029  0.434372 -0.555708   \n",
            "14  0.471782  0.232531  2.293824  ...  2.185123  0.859029  0.434372 -0.076124   \n",
            "15 -0.273246 -0.531004 -0.567577  ... -0.876400 -1.164105 -2.302173  2.321793   \n",
            "16 -0.891745 -0.625318 -1.002406  ...  1.048573 -1.164105 -2.302173 -1.035291   \n",
            "17  1.668175 -0.032122 -0.608809  ... -0.085208  0.859029  0.434372 -0.076124   \n",
            "18 -1.201217 -1.086889  0.143054  ... -0.610362  0.859029  0.434372 -0.555708   \n",
            "19  3.880914 -2.190124  0.326164  ...  2.908866  0.859029  0.434372 -0.076124   \n",
            "20  1.928448 -0.231571  1.163341  ...  0.765533 -1.164105  0.434372  0.883043   \n",
            "21  0.477749 -0.935990 -0.204657  ...  0.077658  0.859029  0.434372 -0.076124   \n",
            "22  0.308476  1.260645  0.242491  ... -0.227157  0.859029  0.434372  0.403459   \n",
            "23  0.957006 -0.012627 -0.059101  ... -0.139646  0.859029  0.434372 -0.076124   \n",
            "24 -0.577370 -1.338864 -0.805743  ...  1.489135  0.859029  0.434372  0.403459   \n",
            "25  0.012977 -0.374317  0.811695  ...  0.989533 -1.164105  0.434372  0.403459   \n",
            "26 -0.306770  0.288864  0.338384  ... -0.651107  0.859029  0.434372 -0.555708   \n",
            "27  0.342000 -0.040139 -0.198566  ...  1.079230 -1.164105  0.434372  0.403459   \n",
            "28  1.034245  0.167103 -0.535725  ... -0.677696  0.859029  0.434372 -0.555708   \n",
            "29  0.128518 -0.699788  0.538024  ...  0.641478 -1.164105  0.434372  0.403459   \n",
            "30 -0.226448  1.329023 -0.446898  ... -1.456581 -1.164105  0.434372 -0.555708   \n",
            "31  1.514548 -0.396321 -0.823777  ...  0.047431 -1.164105  0.434372 -1.994458   \n",
            "32 -0.628094 -1.419638  1.878376  ...  0.980789 -1.164105  0.434372  1.842209   \n",
            "33 -2.042031  0.616344  1.442150  ... -0.415767 -1.164105  0.434372  1.842209   \n",
            "34  0.442049  0.151550  0.522742  ...  0.479677 -1.164105  0.434372  0.403459   \n",
            "35  0.938744  1.178404  0.439759  ... -0.505916  0.859029  0.434372 -0.076124   \n",
            "36 -1.782716  1.869428  0.239494  ... -0.741127  0.859029  0.434372  0.403459   \n",
            "37  0.257665 -0.422206 -1.140060  ... -1.971977 -1.164105  0.434372  0.883043   \n",
            "38 -1.204079  1.796819  0.624790  ... -0.385871  0.859029  0.434372  0.403459   \n",
            "39  0.339062 -0.523117 -0.516064  ...  0.357690 -1.164105 -2.302173 -1.035291   \n",
            "40  0.349595 -1.054598 -1.122964  ... -1.632031  0.859029  0.434372  0.403459   \n",
            "41  0.143264  1.653829  0.386328  ... -1.314442  0.859029 -2.302173  0.883043   \n",
            "42 -0.797886 -0.517182 -0.206603  ...  0.727034 -1.164105  0.434372 -0.555708   \n",
            "43  0.702972  0.181321  0.734456  ...  0.258117  0.859029  0.434372  0.883043   \n",
            "44  0.149512 -0.831327  1.386731  ... -0.265317  0.859029  0.434372 -0.076124   \n",
            "45  0.076983  0.999414 -0.797919  ...  0.080311 -1.164105  0.434372  0.403459   \n",
            "46  0.690211  2.110784  0.259129  ... -0.905546 -1.164105  0.434372 -1.994458   \n",
            "47 -0.235874 -1.889797  1.388273  ...  0.720933 -1.164105  0.434372  1.842209   \n",
            "48 -0.576694  1.359136  0.334442  ... -1.248674  0.859029 -2.302173 -0.555708   \n",
            "49  1.017679 -1.414678  0.752618  ...  0.679185  0.859029  0.434372 -1.514875   \n",
            "50  0.395918 -0.451649 -0.826654  ... -0.224069  0.859029  0.434372 -0.076124   \n",
            "\n",
            "          33        34        35        36   37  38  \n",
            "0   2.585000  2.817139  2.571306  1.736718  1.0   1  \n",
            "1  -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "2   1.098832 -0.026861 -0.116593  1.126096  0.0   1  \n",
            "3   1.344660  0.292413  0.468332  1.126096  1.0   1  \n",
            "4   1.098832 -0.026861 -0.116593  1.126096  0.0   0  \n",
            "5  -0.919079 -0.644039 -0.593604  1.736718  1.0   1  \n",
            "6   1.049271  1.118267  1.003893  1.431407  1.0   1  \n",
            "7  -0.290092 -0.493438 -0.743830 -0.705769  1.0   1  \n",
            "8  -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "9  -0.102652  0.180798  0.405252 -0.705769  0.0   0  \n",
            "10 -0.303103 -0.272472 -0.300268  1.655301  0.0   0  \n",
            "11  0.820384  0.663055  0.713856  1.187158  1.0   1  \n",
            "12 -0.626621 -0.792831 -0.764792 -0.705769  1.0   1  \n",
            "13 -0.290092 -0.493438 -0.743830 -0.705769  1.0   0  \n",
            "14 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "15  0.820384  0.663055  0.713856  1.187158  1.0   1  \n",
            "16  0.789663  0.562746  0.645536  1.349991  1.0   1  \n",
            "17 -0.102652  0.180798  0.405252 -0.705769  0.0   1  \n",
            "18 -0.997746 -0.768170 -0.834729 -0.705769  0.0   0  \n",
            "19 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "20  1.098832 -0.026861 -0.116593  1.126096  0.0   1  \n",
            "21 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "22  0.251586 -0.296086 -0.246117 -0.705769  0.0   0  \n",
            "23 -0.797790 -0.718681 -0.718598 -0.705769  0.0   1  \n",
            "24  0.251586 -0.296086 -0.246117 -0.705769  0.0   0  \n",
            "25  1.344660  0.292413  0.468332  1.126096  1.0   0  \n",
            "26 -0.290092 -0.493438 -0.743830 -0.705769  1.0   1  \n",
            "27  1.344660  0.292413  0.468332  1.126096  1.0   1  \n",
            "28 -0.997746 -0.768170 -0.834729 -0.705769  0.0   1  \n",
            "29  1.344660  0.292413  0.468332  1.126096  1.0   1  \n",
            "30 -1.045375 -0.932500 -0.896321 -0.705769  1.0   0  \n",
            "31 -0.298986 -0.514065 -0.582735 -0.705769  0.0   0  \n",
            "32  0.395284  2.473013  2.730719 -0.705769  0.0   0  \n",
            "33  0.395284  2.473013  2.730719 -0.705769  0.0   0  \n",
            "34 -1.215280 -0.947767 -0.917477 -0.705769  1.0   1  \n",
            "35 -0.102652  0.180798  0.405252 -0.705769  0.0   1  \n",
            "36  0.251586 -0.296086 -0.246117 -0.705769  0.0   0  \n",
            "37  1.098832 -0.026861 -0.116593  1.126096  0.0   1  \n",
            "38  0.050038  0.009018 -0.100743 -0.705769  1.0   1  \n",
            "39  2.585000  2.817139  2.571306  1.736718  1.0   1  \n",
            "40  0.251586 -0.296086 -0.246117 -0.705769  0.0   1  \n",
            "41 -0.919079 -0.644039 -0.593604  1.736718  1.0   1  \n",
            "42 -1.045375 -0.932500 -0.896321 -0.705769  1.0   0  \n",
            "43  2.249470  1.713688  0.624122 -0.705769  1.0   1  \n",
            "44 -0.797790 -0.718681 -0.718598 -0.705769  0.0   1  \n",
            "45 -0.626621 -0.792831 -0.764792 -0.705769  1.0   0  \n",
            "46 -0.298986 -0.514065 -0.582735 -0.705769  0.0   0  \n",
            "47  0.395284  2.473013  2.730719 -0.705769  0.0   0  \n",
            "48  1.049271  1.118267  1.003893  1.431407  1.0   1  \n",
            "49 -0.058790 -0.544583 -0.759293 -0.705769  1.0   0  \n",
            "50 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "\n",
            "[51 rows x 39 columns]\n",
            "lr = 0.001, wd = 0.001, mm = 0.5, ld = 1\n",
            "epoch: 1000  loss: 0.66962326\n",
            "epoch: 2000  loss: 0.62403971\n",
            "epoch: 3000  loss: 0.56996077\n",
            "epoch: 4000  loss: 0.51524377\n",
            "epoch: 5000  loss: 0.47161412\n",
            "epoch: 1000  loss: 0.49329022\n",
            "epoch: 2000  loss: 0.46455136\n",
            "epoch: 3000  loss: 0.43961746\n",
            "epoch: 4000  loss: 0.41456273\n",
            "epoch: 5000  loss: 0.38893706\n",
            "epoch: 1000  loss: 0.34689113\n",
            "epoch: 2000  loss: 0.30395854\n",
            "epoch: 3000  loss: 0.25721753\n",
            "epoch: 4000  loss: 0.21745008\n",
            "epoch: 5000  loss: 0.18556836\n",
            "epoch: 1000  loss: 0.17303054\n",
            "epoch: 2000  loss: 0.14084531\n",
            "epoch: 3000  loss: 0.11774337\n",
            "epoch: 4000  loss: 0.09530509\n",
            "epoch: 5000  loss: 0.07763699\n",
            "epoch: 1000  loss: 0.11088981\n",
            "epoch: 2000  loss: 0.08141743\n",
            "epoch: 3000  loss: 0.06386841\n",
            "epoch: 4000  loss: 0.05183024\n",
            "epoch: 5000  loss: 0.04321338\n",
            "correct prediction: 0\n",
            "correct prediction: 3\n",
            "correct prediction: 4\n",
            "correct prediction: 5\n",
            "correct prediction: 6\n",
            "correct prediction: 8\n",
            "correct prediction: 9\n",
            "correct prediction: 10\n",
            "correct prediction: 11\n",
            "correct prediction: 12\n",
            "correct prediction: 15\n",
            "correct prediction: 16\n",
            "correct prediction: 17\n",
            "correct prediction: 19\n",
            "correct prediction: 20\n",
            "correct prediction: 21\n",
            "correct prediction: 22\n",
            "correct prediction: 26\n",
            "correct prediction: 29\n",
            "correct prediction: 31\n",
            "correct prediction: 33\n",
            "correct prediction: 34\n",
            "correct prediction: 36\n",
            "correct prediction: 38\n",
            "correct prediction: 39\n",
            "correct prediction: 41\n",
            "correct prediction: 43\n",
            "correct prediction: 44\n",
            "correct prediction: 45\n",
            "correct prediction: 46\n",
            "correct prediction: 47\n",
            "correct prediction: 48\n",
            "correct prediction: 50\n",
            "\n",
            "loss on the test set = 0.871\n",
            "Accuracy on the test set: 64.7100%\n",
            "          0         1         2         3         4         5         6   \\\n",
            "0   0.235795 -0.596528  0.764939  0.155125 -0.709392 -1.921546 -0.110274   \n",
            "1  -2.949119 -2.031854  0.231524  0.141733  2.474848  0.246714  0.185667   \n",
            "2  -0.228339 -0.359309 -0.337852  0.821626 -0.872955  0.984931  0.392991   \n",
            "3  -1.213337  2.146571 -0.274030  0.495122 -1.323094  1.240324 -0.271201   \n",
            "4   1.122109  0.807842  0.980374 -0.072595 -0.186554  0.357749 -0.860289   \n",
            "5   0.633887 -0.313952  0.828973 -0.531478  0.995154  0.668412 -0.913099   \n",
            "6  -1.196963  0.714636  0.874417 -2.132932 -0.346888 -0.139611  0.275690   \n",
            "7  -0.208678  0.924384  0.068978 -0.392970 -0.002467  0.156285 -0.486114   \n",
            "8  -0.459825 -0.898318 -1.508754  1.435905  0.692827  3.039905 -0.190101   \n",
            "9  -0.822481 -0.223209 -0.019506  1.332998 -1.304332 -1.430331  0.158815   \n",
            "10  4.518938  1.767920 -0.655172 -0.114569 -1.247974  0.208459 -0.956752   \n",
            "11  0.807073  0.455789  1.731345 -0.566816  1.194391  0.115341  1.135136   \n",
            "12  0.127495  0.425414 -0.007127 -1.479204 -1.642907 -0.709907 -0.342184   \n",
            "13 -1.044644 -0.399502 -0.448795  0.109540  1.014075  0.344043 -0.407666   \n",
            "14  0.703290 -0.205306 -1.139115  1.155272 -1.439506  0.086987 -1.145898   \n",
            "15 -0.987320 -0.488019  0.505248  0.574651 -0.600331  0.469882  1.185208   \n",
            "16 -1.590583 -0.297732 -0.862299 -0.494346  0.679952 -1.054007  0.043731   \n",
            "17  0.961275  1.291009  0.965461 -0.210189 -0.250317 -1.335828 -0.919517   \n",
            "18 -0.675030 -0.008716 -1.001394  1.176765 -0.357519  1.582644  1.515647   \n",
            "19  0.670038  0.478898  0.133934  0.317207 -0.383914  2.476607 -2.575679   \n",
            "20  1.780669  1.033887 -0.043665  0.135341  0.036268 -0.694678 -1.321278   \n",
            "21 -2.905534  0.963953  1.057505 -0.600924  0.621815 -0.175828  1.888403   \n",
            "22 -0.484269 -0.717361 -1.216896 -1.341853  1.359862 -0.333006  0.239972   \n",
            "23  0.770300  0.286824 -0.522803  0.170265 -0.770391 -0.034662 -0.657823   \n",
            "24  0.171093  0.955744 -1.376500  0.614331  0.794085 -0.985971 -1.033522   \n",
            "25 -1.382818 -0.457078 -0.456693  2.177205  0.242991 -0.474857  0.064215   \n",
            "26 -0.986676  0.818727  0.758720 -0.899834 -0.734657 -1.053275 -0.060929   \n",
            "27  1.099488 -0.697891 -0.403934 -0.176542  0.164813  0.451029 -1.484485   \n",
            "28  2.423820  1.457278  0.317998 -2.058740 -0.420090 -0.691256  0.167741   \n",
            "29  0.503652 -0.642579 -0.903764  0.374938 -0.213290  0.795397 -0.387991   \n",
            "30  1.014337 -1.608052  0.375635 -2.259168 -0.696333 -1.079305 -0.400122   \n",
            "31  3.116689  1.723113  0.681957 -1.420649 -1.193706 -0.665680 -0.091697   \n",
            "32 -1.517675 -1.205269 -0.024106  0.664088  3.003847  0.674357 -0.142209   \n",
            "33 -1.125131  0.378030 -0.374538  0.361702  1.661269  1.541844  1.134609   \n",
            "34  0.007997  0.717900 -1.084321  0.565784  0.038188 -1.910262 -0.709885   \n",
            "35  1.317641  0.414077  0.614004 -0.419078 -1.573296  0.660523  1.796185   \n",
            "36  0.786040  0.074047 -0.166161 -0.262730  0.763262  0.706814  2.259574   \n",
            "37  0.011365 -0.370733  0.261825  0.013103  1.260039 -0.554880  0.191121   \n",
            "38 -0.560580  0.164782  0.557392 -1.035448  0.752872 -1.385119 -0.630531   \n",
            "39 -0.725218  0.331052 -0.523784  0.194932 -0.856695  0.352498 -0.053287   \n",
            "40  0.803086 -0.835295 -0.639992 -0.869191 -1.658046  0.863716  0.382231   \n",
            "41  0.272421  1.009930  1.523204 -1.375841 -1.603089 -0.350015  0.297634   \n",
            "42 -2.727341  0.145088  0.484466  1.042311  1.162478 -1.482644 -0.157254   \n",
            "43 -0.789957  0.549711  0.977592 -0.019222 -0.048327 -0.215502  0.429913   \n",
            "44 -3.751709 -0.048892  1.479589  1.088468  1.030387  0.744044  1.597948   \n",
            "45  0.261220  1.287996  0.349535 -0.324269 -1.129768 -1.046599  0.082300   \n",
            "46  1.947578  1.323064 -0.352006 -1.286524 -1.308874  0.670241 -0.144401   \n",
            "47 -1.549491 -1.522130 -0.313656  0.316227 -0.249302  1.104591 -0.581757   \n",
            "48  1.092981 -1.028857 -0.250087 -1.441544 -1.115122  1.402114 -1.131119   \n",
            "49  0.572089  0.007242 -0.636844  0.853076  1.122678  1.509380 -1.020430   \n",
            "50 -2.689340 -0.277448  1.042525  1.452017  0.123011  1.133931  0.748255   \n",
            "\n",
            "          7         8         9   ...        29        30        31        32  \\\n",
            "0   0.382224 -1.064994 -2.581160  ...  0.219561 -1.164105 -2.302173 -1.035291   \n",
            "1  -0.375862 -0.384313 -0.203708  ... -0.154425  0.859029  0.434372 -0.076124   \n",
            "2  -1.113034 -1.138260 -0.083353  ...  0.030184 -1.164105  0.434372  0.883043   \n",
            "3  -0.098764  0.337893  2.173916  ...  2.094682 -1.164105  0.434372  0.403459   \n",
            "4   1.165677  1.248191  0.310892  ...  0.263554 -1.164105  0.434372  0.883043   \n",
            "5   1.512495 -0.868699 -1.661515  ...  0.149895  0.859029 -2.302173  0.883043   \n",
            "6  -0.951835  1.527703 -0.403727  ... -0.395063  0.859029 -2.302173 -0.555708   \n",
            "7  -0.173060  0.068664  0.033924  ... -0.087071  0.859029  0.434372 -0.555708   \n",
            "8   0.584333 -1.482787  1.048752  ...  1.611295  0.859029  0.434372 -0.076124   \n",
            "9   0.488403  0.530823 -1.020090  ... -0.079679  0.859029  0.434372 -0.076124   \n",
            "10  0.252102  0.355264  1.974272  ...  0.574427  0.859029  0.434372 -0.076124   \n",
            "11  1.379504 -0.101004 -0.011852  ... -2.185978 -1.164105 -2.302173  2.321793   \n",
            "12 -1.226289  1.084064  0.252926  ... -0.787906 -1.164105  0.434372  0.403459   \n",
            "13  0.077320 -0.676263  0.871215  ...  1.041880  0.859029  0.434372 -0.555708   \n",
            "14  0.471782  0.232531  2.293824  ...  2.185123  0.859029  0.434372 -0.076124   \n",
            "15 -0.273246 -0.531004 -0.567577  ... -0.876400 -1.164105 -2.302173  2.321793   \n",
            "16 -0.891745 -0.625318 -1.002406  ...  1.048573 -1.164105 -2.302173 -1.035291   \n",
            "17  1.668175 -0.032122 -0.608809  ... -0.085208  0.859029  0.434372 -0.076124   \n",
            "18 -1.201217 -1.086889  0.143054  ... -0.610362  0.859029  0.434372 -0.555708   \n",
            "19  3.880914 -2.190124  0.326164  ...  2.908866  0.859029  0.434372 -0.076124   \n",
            "20  1.928448 -0.231571  1.163341  ...  0.765533 -1.164105  0.434372  0.883043   \n",
            "21  0.477749 -0.935990 -0.204657  ...  0.077658  0.859029  0.434372 -0.076124   \n",
            "22  0.308476  1.260645  0.242491  ... -0.227157  0.859029  0.434372  0.403459   \n",
            "23  0.957006 -0.012627 -0.059101  ... -0.139646  0.859029  0.434372 -0.076124   \n",
            "24 -0.577370 -1.338864 -0.805743  ...  1.489135  0.859029  0.434372  0.403459   \n",
            "25  0.012977 -0.374317  0.811695  ...  0.989533 -1.164105  0.434372  0.403459   \n",
            "26 -0.306770  0.288864  0.338384  ... -0.651107  0.859029  0.434372 -0.555708   \n",
            "27  0.342000 -0.040139 -0.198566  ...  1.079230 -1.164105  0.434372  0.403459   \n",
            "28  1.034245  0.167103 -0.535725  ... -0.677696  0.859029  0.434372 -0.555708   \n",
            "29  0.128518 -0.699788  0.538024  ...  0.641478 -1.164105  0.434372  0.403459   \n",
            "30 -0.226448  1.329023 -0.446898  ... -1.456581 -1.164105  0.434372 -0.555708   \n",
            "31  1.514548 -0.396321 -0.823777  ...  0.047431 -1.164105  0.434372 -1.994458   \n",
            "32 -0.628094 -1.419638  1.878376  ...  0.980789 -1.164105  0.434372  1.842209   \n",
            "33 -2.042031  0.616344  1.442150  ... -0.415767 -1.164105  0.434372  1.842209   \n",
            "34  0.442049  0.151550  0.522742  ...  0.479677 -1.164105  0.434372  0.403459   \n",
            "35  0.938744  1.178404  0.439759  ... -0.505916  0.859029  0.434372 -0.076124   \n",
            "36 -1.782716  1.869428  0.239494  ... -0.741127  0.859029  0.434372  0.403459   \n",
            "37  0.257665 -0.422206 -1.140060  ... -1.971977 -1.164105  0.434372  0.883043   \n",
            "38 -1.204079  1.796819  0.624790  ... -0.385871  0.859029  0.434372  0.403459   \n",
            "39  0.339062 -0.523117 -0.516064  ...  0.357690 -1.164105 -2.302173 -1.035291   \n",
            "40  0.349595 -1.054598 -1.122964  ... -1.632031  0.859029  0.434372  0.403459   \n",
            "41  0.143264  1.653829  0.386328  ... -1.314442  0.859029 -2.302173  0.883043   \n",
            "42 -0.797886 -0.517182 -0.206603  ...  0.727034 -1.164105  0.434372 -0.555708   \n",
            "43  0.702972  0.181321  0.734456  ...  0.258117  0.859029  0.434372  0.883043   \n",
            "44  0.149512 -0.831327  1.386731  ... -0.265317  0.859029  0.434372 -0.076124   \n",
            "45  0.076983  0.999414 -0.797919  ...  0.080311 -1.164105  0.434372  0.403459   \n",
            "46  0.690211  2.110784  0.259129  ... -0.905546 -1.164105  0.434372 -1.994458   \n",
            "47 -0.235874 -1.889797  1.388273  ...  0.720933 -1.164105  0.434372  1.842209   \n",
            "48 -0.576694  1.359136  0.334442  ... -1.248674  0.859029 -2.302173 -0.555708   \n",
            "49  1.017679 -1.414678  0.752618  ...  0.679185  0.859029  0.434372 -1.514875   \n",
            "50  0.395918 -0.451649 -0.826654  ... -0.224069  0.859029  0.434372 -0.076124   \n",
            "\n",
            "          33        34        35        36   37  38  \n",
            "0   2.585000  2.817139  2.571306  1.736718  1.0   1  \n",
            "1  -0.797790 -0.718681 -0.718598 -0.705769  0.0   1  \n",
            "2   1.098832 -0.026861 -0.116593  1.126096  0.0   1  \n",
            "3   1.344660  0.292413  0.468332  1.126096  1.0   1  \n",
            "4   1.098832 -0.026861 -0.116593  1.126096  0.0   0  \n",
            "5  -0.919079 -0.644039 -0.593604  1.736718  1.0   1  \n",
            "6   1.049271  1.118267  1.003893  1.431407  1.0   1  \n",
            "7  -0.290092 -0.493438 -0.743830 -0.705769  1.0   0  \n",
            "8  -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "9  -0.102652  0.180798  0.405252 -0.705769  0.0   0  \n",
            "10 -0.303103 -0.272472 -0.300268  1.655301  0.0   0  \n",
            "11  0.820384  0.663055  0.713856  1.187158  1.0   1  \n",
            "12 -0.626621 -0.792831 -0.764792 -0.705769  1.0   1  \n",
            "13 -0.290092 -0.493438 -0.743830 -0.705769  1.0   0  \n",
            "14 -0.797790 -0.718681 -0.718598 -0.705769  0.0   1  \n",
            "15  0.820384  0.663055  0.713856  1.187158  1.0   1  \n",
            "16  0.789663  0.562746  0.645536  1.349991  1.0   1  \n",
            "17 -0.102652  0.180798  0.405252 -0.705769  0.0   0  \n",
            "18 -0.997746 -0.768170 -0.834729 -0.705769  0.0   1  \n",
            "19 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "20  1.098832 -0.026861 -0.116593  1.126096  0.0   0  \n",
            "21 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "22  0.251586 -0.296086 -0.246117 -0.705769  0.0   0  \n",
            "23 -0.797790 -0.718681 -0.718598 -0.705769  0.0   1  \n",
            "24  0.251586 -0.296086 -0.246117 -0.705769  0.0   1  \n",
            "25  1.344660  0.292413  0.468332  1.126096  1.0   0  \n",
            "26 -0.290092 -0.493438 -0.743830 -0.705769  1.0   1  \n",
            "27  1.344660  0.292413  0.468332  1.126096  1.0   0  \n",
            "28 -0.997746 -0.768170 -0.834729 -0.705769  0.0   1  \n",
            "29  1.344660  0.292413  0.468332  1.126096  1.0   1  \n",
            "30 -1.045375 -0.932500 -0.896321 -0.705769  1.0   0  \n",
            "31 -0.298986 -0.514065 -0.582735 -0.705769  0.0   0  \n",
            "32  0.395284  2.473013  2.730719 -0.705769  0.0   1  \n",
            "33  0.395284  2.473013  2.730719 -0.705769  0.0   0  \n",
            "34 -1.215280 -0.947767 -0.917477 -0.705769  1.0   1  \n",
            "35 -0.102652  0.180798  0.405252 -0.705769  0.0   1  \n",
            "36  0.251586 -0.296086 -0.246117 -0.705769  0.0   0  \n",
            "37  1.098832 -0.026861 -0.116593  1.126096  0.0   1  \n",
            "38  0.050038  0.009018 -0.100743 -0.705769  1.0   1  \n",
            "39  2.585000  2.817139  2.571306  1.736718  1.0   1  \n",
            "40  0.251586 -0.296086 -0.246117 -0.705769  0.0   1  \n",
            "41 -0.919079 -0.644039 -0.593604  1.736718  1.0   1  \n",
            "42 -1.045375 -0.932500 -0.896321 -0.705769  1.0   0  \n",
            "43  2.249470  1.713688  0.624122 -0.705769  1.0   1  \n",
            "44 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "45 -0.626621 -0.792831 -0.764792 -0.705769  1.0   1  \n",
            "46 -0.298986 -0.514065 -0.582735 -0.705769  0.0   0  \n",
            "47  0.395284  2.473013  2.730719 -0.705769  0.0   0  \n",
            "48  1.049271  1.118267  1.003893  1.431407  1.0   1  \n",
            "49 -0.058790 -0.544583 -0.759293 -0.705769  1.0   0  \n",
            "50 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "\n",
            "[51 rows x 39 columns]\n",
            "lr = 0.001, wd = 0.001, mm = 0.5, ld = 0.9\n",
            "epoch: 1000  loss: 0.68632793\n",
            "epoch: 2000  loss: 0.67785305\n",
            "epoch: 3000  loss: 0.66668093\n",
            "epoch: 4000  loss: 0.65200949\n",
            "epoch: 5000  loss: 0.63417429\n",
            "epoch: 1000  loss: 0.62865180\n",
            "epoch: 2000  loss: 0.61363131\n",
            "epoch: 3000  loss: 0.59973687\n",
            "epoch: 4000  loss: 0.58702135\n",
            "epoch: 5000  loss: 0.57535917\n",
            "epoch: 1000  loss: 0.54784518\n",
            "epoch: 2000  loss: 0.53781199\n",
            "epoch: 3000  loss: 0.52988321\n",
            "epoch: 4000  loss: 0.52332366\n",
            "epoch: 5000  loss: 0.51750076\n",
            "epoch: 1000  loss: 0.50795782\n",
            "epoch: 2000  loss: 0.50142515\n",
            "epoch: 3000  loss: 0.49593487\n",
            "epoch: 4000  loss: 0.49127457\n",
            "epoch: 5000  loss: 0.48730525\n",
            "epoch: 1000  loss: 0.51766169\n",
            "epoch: 2000  loss: 0.51485276\n",
            "epoch: 3000  loss: 0.51228768\n",
            "epoch: 4000  loss: 0.50996435\n",
            "epoch: 5000  loss: 0.50802374\n",
            "correct prediction: 0\n",
            "correct prediction: 1\n",
            "correct prediction: 3\n",
            "correct prediction: 4\n",
            "correct prediction: 5\n",
            "correct prediction: 6\n",
            "correct prediction: 8\n",
            "correct prediction: 10\n",
            "correct prediction: 11\n",
            "correct prediction: 14\n",
            "correct prediction: 15\n",
            "correct prediction: 16\n",
            "correct prediction: 17\n",
            "correct prediction: 18\n",
            "correct prediction: 19\n",
            "correct prediction: 21\n",
            "correct prediction: 22\n",
            "correct prediction: 23\n",
            "correct prediction: 25\n",
            "correct prediction: 27\n",
            "correct prediction: 32\n",
            "correct prediction: 33\n",
            "correct prediction: 38\n",
            "correct prediction: 39\n",
            "correct prediction: 40\n",
            "correct prediction: 41\n",
            "correct prediction: 43\n",
            "correct prediction: 44\n",
            "correct prediction: 47\n",
            "correct prediction: 48\n",
            "correct prediction: 50\n",
            "\n",
            "loss on the test set = 0.549\n",
            "Accuracy on the test set: 60.7800%\n",
            "          0         1         2         3         4         5         6   \\\n",
            "0   0.235795 -0.596528  0.764939  0.155125 -0.709392 -1.921546 -0.110274   \n",
            "1  -2.949119 -2.031854  0.231524  0.141733  2.474848  0.246714  0.185667   \n",
            "2  -0.228339 -0.359309 -0.337852  0.821626 -0.872955  0.984931  0.392991   \n",
            "3  -1.213337  2.146571 -0.274030  0.495122 -1.323094  1.240324 -0.271201   \n",
            "4   1.122109  0.807842  0.980374 -0.072595 -0.186554  0.357749 -0.860289   \n",
            "5   0.633887 -0.313952  0.828973 -0.531478  0.995154  0.668412 -0.913099   \n",
            "6  -1.196963  0.714636  0.874417 -2.132932 -0.346888 -0.139611  0.275690   \n",
            "7  -0.208678  0.924384  0.068978 -0.392970 -0.002467  0.156285 -0.486114   \n",
            "8  -0.459825 -0.898318 -1.508754  1.435905  0.692827  3.039905 -0.190101   \n",
            "9  -0.822481 -0.223209 -0.019506  1.332998 -1.304332 -1.430331  0.158815   \n",
            "10  4.518938  1.767920 -0.655172 -0.114569 -1.247974  0.208459 -0.956752   \n",
            "11  0.807073  0.455789  1.731345 -0.566816  1.194391  0.115341  1.135136   \n",
            "12  0.127495  0.425414 -0.007127 -1.479204 -1.642907 -0.709907 -0.342184   \n",
            "13 -1.044644 -0.399502 -0.448795  0.109540  1.014075  0.344043 -0.407666   \n",
            "14  0.703290 -0.205306 -1.139115  1.155272 -1.439506  0.086987 -1.145898   \n",
            "15 -0.987320 -0.488019  0.505248  0.574651 -0.600331  0.469882  1.185208   \n",
            "16 -1.590583 -0.297732 -0.862299 -0.494346  0.679952 -1.054007  0.043731   \n",
            "17  0.961275  1.291009  0.965461 -0.210189 -0.250317 -1.335828 -0.919517   \n",
            "18 -0.675030 -0.008716 -1.001394  1.176765 -0.357519  1.582644  1.515647   \n",
            "19  0.670038  0.478898  0.133934  0.317207 -0.383914  2.476607 -2.575679   \n",
            "20  1.780669  1.033887 -0.043665  0.135341  0.036268 -0.694678 -1.321278   \n",
            "21 -2.905534  0.963953  1.057505 -0.600924  0.621815 -0.175828  1.888403   \n",
            "22 -0.484269 -0.717361 -1.216896 -1.341853  1.359862 -0.333006  0.239972   \n",
            "23  0.770300  0.286824 -0.522803  0.170265 -0.770391 -0.034662 -0.657823   \n",
            "24  0.171093  0.955744 -1.376500  0.614331  0.794085 -0.985971 -1.033522   \n",
            "25 -1.382818 -0.457078 -0.456693  2.177205  0.242991 -0.474857  0.064215   \n",
            "26 -0.986676  0.818727  0.758720 -0.899834 -0.734657 -1.053275 -0.060929   \n",
            "27  1.099488 -0.697891 -0.403934 -0.176542  0.164813  0.451029 -1.484485   \n",
            "28  2.423820  1.457278  0.317998 -2.058740 -0.420090 -0.691256  0.167741   \n",
            "29  0.503652 -0.642579 -0.903764  0.374938 -0.213290  0.795397 -0.387991   \n",
            "30  1.014337 -1.608052  0.375635 -2.259168 -0.696333 -1.079305 -0.400122   \n",
            "31  3.116689  1.723113  0.681957 -1.420649 -1.193706 -0.665680 -0.091697   \n",
            "32 -1.517675 -1.205269 -0.024106  0.664088  3.003847  0.674357 -0.142209   \n",
            "33 -1.125131  0.378030 -0.374538  0.361702  1.661269  1.541844  1.134609   \n",
            "34  0.007997  0.717900 -1.084321  0.565784  0.038188 -1.910262 -0.709885   \n",
            "35  1.317641  0.414077  0.614004 -0.419078 -1.573296  0.660523  1.796185   \n",
            "36  0.786040  0.074047 -0.166161 -0.262730  0.763262  0.706814  2.259574   \n",
            "37  0.011365 -0.370733  0.261825  0.013103  1.260039 -0.554880  0.191121   \n",
            "38 -0.560580  0.164782  0.557392 -1.035448  0.752872 -1.385119 -0.630531   \n",
            "39 -0.725218  0.331052 -0.523784  0.194932 -0.856695  0.352498 -0.053287   \n",
            "40  0.803086 -0.835295 -0.639992 -0.869191 -1.658046  0.863716  0.382231   \n",
            "41  0.272421  1.009930  1.523204 -1.375841 -1.603089 -0.350015  0.297634   \n",
            "42 -2.727341  0.145088  0.484466  1.042311  1.162478 -1.482644 -0.157254   \n",
            "43 -0.789957  0.549711  0.977592 -0.019222 -0.048327 -0.215502  0.429913   \n",
            "44 -3.751709 -0.048892  1.479589  1.088468  1.030387  0.744044  1.597948   \n",
            "45  0.261220  1.287996  0.349535 -0.324269 -1.129768 -1.046599  0.082300   \n",
            "46  1.947578  1.323064 -0.352006 -1.286524 -1.308874  0.670241 -0.144401   \n",
            "47 -1.549491 -1.522130 -0.313656  0.316227 -0.249302  1.104591 -0.581757   \n",
            "48  1.092981 -1.028857 -0.250087 -1.441544 -1.115122  1.402114 -1.131119   \n",
            "49  0.572089  0.007242 -0.636844  0.853076  1.122678  1.509380 -1.020430   \n",
            "50 -2.689340 -0.277448  1.042525  1.452017  0.123011  1.133931  0.748255   \n",
            "\n",
            "          7         8         9   ...        29        30        31        32  \\\n",
            "0   0.382224 -1.064994 -2.581160  ...  0.219561 -1.164105 -2.302173 -1.035291   \n",
            "1  -0.375862 -0.384313 -0.203708  ... -0.154425  0.859029  0.434372 -0.076124   \n",
            "2  -1.113034 -1.138260 -0.083353  ...  0.030184 -1.164105  0.434372  0.883043   \n",
            "3  -0.098764  0.337893  2.173916  ...  2.094682 -1.164105  0.434372  0.403459   \n",
            "4   1.165677  1.248191  0.310892  ...  0.263554 -1.164105  0.434372  0.883043   \n",
            "5   1.512495 -0.868699 -1.661515  ...  0.149895  0.859029 -2.302173  0.883043   \n",
            "6  -0.951835  1.527703 -0.403727  ... -0.395063  0.859029 -2.302173 -0.555708   \n",
            "7  -0.173060  0.068664  0.033924  ... -0.087071  0.859029  0.434372 -0.555708   \n",
            "8   0.584333 -1.482787  1.048752  ...  1.611295  0.859029  0.434372 -0.076124   \n",
            "9   0.488403  0.530823 -1.020090  ... -0.079679  0.859029  0.434372 -0.076124   \n",
            "10  0.252102  0.355264  1.974272  ...  0.574427  0.859029  0.434372 -0.076124   \n",
            "11  1.379504 -0.101004 -0.011852  ... -2.185978 -1.164105 -2.302173  2.321793   \n",
            "12 -1.226289  1.084064  0.252926  ... -0.787906 -1.164105  0.434372  0.403459   \n",
            "13  0.077320 -0.676263  0.871215  ...  1.041880  0.859029  0.434372 -0.555708   \n",
            "14  0.471782  0.232531  2.293824  ...  2.185123  0.859029  0.434372 -0.076124   \n",
            "15 -0.273246 -0.531004 -0.567577  ... -0.876400 -1.164105 -2.302173  2.321793   \n",
            "16 -0.891745 -0.625318 -1.002406  ...  1.048573 -1.164105 -2.302173 -1.035291   \n",
            "17  1.668175 -0.032122 -0.608809  ... -0.085208  0.859029  0.434372 -0.076124   \n",
            "18 -1.201217 -1.086889  0.143054  ... -0.610362  0.859029  0.434372 -0.555708   \n",
            "19  3.880914 -2.190124  0.326164  ...  2.908866  0.859029  0.434372 -0.076124   \n",
            "20  1.928448 -0.231571  1.163341  ...  0.765533 -1.164105  0.434372  0.883043   \n",
            "21  0.477749 -0.935990 -0.204657  ...  0.077658  0.859029  0.434372 -0.076124   \n",
            "22  0.308476  1.260645  0.242491  ... -0.227157  0.859029  0.434372  0.403459   \n",
            "23  0.957006 -0.012627 -0.059101  ... -0.139646  0.859029  0.434372 -0.076124   \n",
            "24 -0.577370 -1.338864 -0.805743  ...  1.489135  0.859029  0.434372  0.403459   \n",
            "25  0.012977 -0.374317  0.811695  ...  0.989533 -1.164105  0.434372  0.403459   \n",
            "26 -0.306770  0.288864  0.338384  ... -0.651107  0.859029  0.434372 -0.555708   \n",
            "27  0.342000 -0.040139 -0.198566  ...  1.079230 -1.164105  0.434372  0.403459   \n",
            "28  1.034245  0.167103 -0.535725  ... -0.677696  0.859029  0.434372 -0.555708   \n",
            "29  0.128518 -0.699788  0.538024  ...  0.641478 -1.164105  0.434372  0.403459   \n",
            "30 -0.226448  1.329023 -0.446898  ... -1.456581 -1.164105  0.434372 -0.555708   \n",
            "31  1.514548 -0.396321 -0.823777  ...  0.047431 -1.164105  0.434372 -1.994458   \n",
            "32 -0.628094 -1.419638  1.878376  ...  0.980789 -1.164105  0.434372  1.842209   \n",
            "33 -2.042031  0.616344  1.442150  ... -0.415767 -1.164105  0.434372  1.842209   \n",
            "34  0.442049  0.151550  0.522742  ...  0.479677 -1.164105  0.434372  0.403459   \n",
            "35  0.938744  1.178404  0.439759  ... -0.505916  0.859029  0.434372 -0.076124   \n",
            "36 -1.782716  1.869428  0.239494  ... -0.741127  0.859029  0.434372  0.403459   \n",
            "37  0.257665 -0.422206 -1.140060  ... -1.971977 -1.164105  0.434372  0.883043   \n",
            "38 -1.204079  1.796819  0.624790  ... -0.385871  0.859029  0.434372  0.403459   \n",
            "39  0.339062 -0.523117 -0.516064  ...  0.357690 -1.164105 -2.302173 -1.035291   \n",
            "40  0.349595 -1.054598 -1.122964  ... -1.632031  0.859029  0.434372  0.403459   \n",
            "41  0.143264  1.653829  0.386328  ... -1.314442  0.859029 -2.302173  0.883043   \n",
            "42 -0.797886 -0.517182 -0.206603  ...  0.727034 -1.164105  0.434372 -0.555708   \n",
            "43  0.702972  0.181321  0.734456  ...  0.258117  0.859029  0.434372  0.883043   \n",
            "44  0.149512 -0.831327  1.386731  ... -0.265317  0.859029  0.434372 -0.076124   \n",
            "45  0.076983  0.999414 -0.797919  ...  0.080311 -1.164105  0.434372  0.403459   \n",
            "46  0.690211  2.110784  0.259129  ... -0.905546 -1.164105  0.434372 -1.994458   \n",
            "47 -0.235874 -1.889797  1.388273  ...  0.720933 -1.164105  0.434372  1.842209   \n",
            "48 -0.576694  1.359136  0.334442  ... -1.248674  0.859029 -2.302173 -0.555708   \n",
            "49  1.017679 -1.414678  0.752618  ...  0.679185  0.859029  0.434372 -1.514875   \n",
            "50  0.395918 -0.451649 -0.826654  ... -0.224069  0.859029  0.434372 -0.076124   \n",
            "\n",
            "          33        34        35        36   37  38  \n",
            "0   2.585000  2.817139  2.571306  1.736718  1.0   1  \n",
            "1  -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "2   1.098832 -0.026861 -0.116593  1.126096  0.0   1  \n",
            "3   1.344660  0.292413  0.468332  1.126096  1.0   1  \n",
            "4   1.098832 -0.026861 -0.116593  1.126096  0.0   0  \n",
            "5  -0.919079 -0.644039 -0.593604  1.736718  1.0   1  \n",
            "6   1.049271  1.118267  1.003893  1.431407  1.0   1  \n",
            "7  -0.290092 -0.493438 -0.743830 -0.705769  1.0   0  \n",
            "8  -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "9  -0.102652  0.180798  0.405252 -0.705769  0.0   1  \n",
            "10 -0.303103 -0.272472 -0.300268  1.655301  0.0   0  \n",
            "11  0.820384  0.663055  0.713856  1.187158  1.0   1  \n",
            "12 -0.626621 -0.792831 -0.764792 -0.705769  1.0   0  \n",
            "13 -0.290092 -0.493438 -0.743830 -0.705769  1.0   0  \n",
            "14 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "15  0.820384  0.663055  0.713856  1.187158  1.0   1  \n",
            "16  0.789663  0.562746  0.645536  1.349991  1.0   1  \n",
            "17 -0.102652  0.180798  0.405252 -0.705769  0.0   0  \n",
            "18 -0.997746 -0.768170 -0.834729 -0.705769  0.0   0  \n",
            "19 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "20  1.098832 -0.026861 -0.116593  1.126096  0.0   1  \n",
            "21 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "22  0.251586 -0.296086 -0.246117 -0.705769  0.0   0  \n",
            "23 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "24  0.251586 -0.296086 -0.246117 -0.705769  0.0   1  \n",
            "25  1.344660  0.292413  0.468332  1.126096  1.0   1  \n",
            "26 -0.290092 -0.493438 -0.743830 -0.705769  1.0   0  \n",
            "27  1.344660  0.292413  0.468332  1.126096  1.0   1  \n",
            "28 -0.997746 -0.768170 -0.834729 -0.705769  0.0   1  \n",
            "29  1.344660  0.292413  0.468332  1.126096  1.0   0  \n",
            "30 -1.045375 -0.932500 -0.896321 -0.705769  1.0   0  \n",
            "31 -0.298986 -0.514065 -0.582735 -0.705769  0.0   1  \n",
            "32  0.395284  2.473013  2.730719 -0.705769  0.0   0  \n",
            "33  0.395284  2.473013  2.730719 -0.705769  0.0   0  \n",
            "34 -1.215280 -0.947767 -0.917477 -0.705769  1.0   0  \n",
            "35 -0.102652  0.180798  0.405252 -0.705769  0.0   1  \n",
            "36  0.251586 -0.296086 -0.246117 -0.705769  0.0   1  \n",
            "37  1.098832 -0.026861 -0.116593  1.126096  0.0   1  \n",
            "38  0.050038  0.009018 -0.100743 -0.705769  1.0   1  \n",
            "39  2.585000  2.817139  2.571306  1.736718  1.0   1  \n",
            "40  0.251586 -0.296086 -0.246117 -0.705769  0.0   0  \n",
            "41 -0.919079 -0.644039 -0.593604  1.736718  1.0   1  \n",
            "42 -1.045375 -0.932500 -0.896321 -0.705769  1.0   0  \n",
            "43  2.249470  1.713688  0.624122 -0.705769  1.0   1  \n",
            "44 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "45 -0.626621 -0.792831 -0.764792 -0.705769  1.0   0  \n",
            "46 -0.298986 -0.514065 -0.582735 -0.705769  0.0   1  \n",
            "47  0.395284  2.473013  2.730719 -0.705769  0.0   0  \n",
            "48  1.049271  1.118267  1.003893  1.431407  1.0   1  \n",
            "49 -0.058790 -0.544583 -0.759293 -0.705769  1.0   0  \n",
            "50 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "\n",
            "[51 rows x 39 columns]\n",
            "lr = 0.001, wd = 0.001, mm = 0.725, ld = 1\n",
            "epoch: 1000  loss: 0.66837072\n",
            "epoch: 2000  loss: 0.62289679\n",
            "epoch: 3000  loss: 0.56371748\n",
            "epoch: 4000  loss: 0.50495511\n",
            "epoch: 5000  loss: 0.45493120\n",
            "epoch: 1000  loss: 0.38241288\n",
            "epoch: 2000  loss: 0.31550926\n",
            "epoch: 3000  loss: 0.26365286\n",
            "epoch: 4000  loss: 0.23013239\n",
            "epoch: 5000  loss: 0.20349969\n",
            "epoch: 1000  loss: 0.18642007\n",
            "epoch: 2000  loss: 0.15276037\n",
            "epoch: 3000  loss: 0.12934162\n",
            "epoch: 4000  loss: 0.11669733\n",
            "epoch: 5000  loss: 0.10912503\n",
            "epoch: 1000  loss: 0.15350084\n",
            "epoch: 2000  loss: 0.12589143\n",
            "epoch: 3000  loss: 0.11530112\n",
            "epoch: 4000  loss: 0.10945619\n",
            "epoch: 5000  loss: 0.10558903\n",
            "epoch: 1000  loss: 0.10137403\n",
            "epoch: 2000  loss: 0.09697445\n",
            "epoch: 3000  loss: 0.09470947\n",
            "epoch: 4000  loss: 0.09316510\n",
            "epoch: 5000  loss: 0.09203990\n",
            "correct prediction: 0\n",
            "correct prediction: 1\n",
            "correct prediction: 3\n",
            "correct prediction: 4\n",
            "correct prediction: 5\n",
            "correct prediction: 6\n",
            "correct prediction: 8\n",
            "correct prediction: 9\n",
            "correct prediction: 10\n",
            "correct prediction: 11\n",
            "correct prediction: 12\n",
            "correct prediction: 13\n",
            "correct prediction: 14\n",
            "correct prediction: 15\n",
            "correct prediction: 16\n",
            "correct prediction: 18\n",
            "correct prediction: 21\n",
            "correct prediction: 22\n",
            "correct prediction: 25\n",
            "correct prediction: 27\n",
            "correct prediction: 29\n",
            "correct prediction: 33\n",
            "correct prediction: 34\n",
            "correct prediction: 36\n",
            "correct prediction: 37\n",
            "correct prediction: 38\n",
            "correct prediction: 39\n",
            "correct prediction: 41\n",
            "correct prediction: 43\n",
            "correct prediction: 44\n",
            "correct prediction: 45\n",
            "correct prediction: 46\n",
            "correct prediction: 48\n",
            "correct prediction: 50\n",
            "\n",
            "loss on the test set = 1.077\n",
            "Accuracy on the test set: 66.6700%\n",
            "          0         1         2         3         4         5         6   \\\n",
            "0   0.235795 -0.596528  0.764939  0.155125 -0.709392 -1.921546 -0.110274   \n",
            "1  -2.949119 -2.031854  0.231524  0.141733  2.474848  0.246714  0.185667   \n",
            "2  -0.228339 -0.359309 -0.337852  0.821626 -0.872955  0.984931  0.392991   \n",
            "3  -1.213337  2.146571 -0.274030  0.495122 -1.323094  1.240324 -0.271201   \n",
            "4   1.122109  0.807842  0.980374 -0.072595 -0.186554  0.357749 -0.860289   \n",
            "5   0.633887 -0.313952  0.828973 -0.531478  0.995154  0.668412 -0.913099   \n",
            "6  -1.196963  0.714636  0.874417 -2.132932 -0.346888 -0.139611  0.275690   \n",
            "7  -0.208678  0.924384  0.068978 -0.392970 -0.002467  0.156285 -0.486114   \n",
            "8  -0.459825 -0.898318 -1.508754  1.435905  0.692827  3.039905 -0.190101   \n",
            "9  -0.822481 -0.223209 -0.019506  1.332998 -1.304332 -1.430331  0.158815   \n",
            "10  4.518938  1.767920 -0.655172 -0.114569 -1.247974  0.208459 -0.956752   \n",
            "11  0.807073  0.455789  1.731345 -0.566816  1.194391  0.115341  1.135136   \n",
            "12  0.127495  0.425414 -0.007127 -1.479204 -1.642907 -0.709907 -0.342184   \n",
            "13 -1.044644 -0.399502 -0.448795  0.109540  1.014075  0.344043 -0.407666   \n",
            "14  0.703290 -0.205306 -1.139115  1.155272 -1.439506  0.086987 -1.145898   \n",
            "15 -0.987320 -0.488019  0.505248  0.574651 -0.600331  0.469882  1.185208   \n",
            "16 -1.590583 -0.297732 -0.862299 -0.494346  0.679952 -1.054007  0.043731   \n",
            "17  0.961275  1.291009  0.965461 -0.210189 -0.250317 -1.335828 -0.919517   \n",
            "18 -0.675030 -0.008716 -1.001394  1.176765 -0.357519  1.582644  1.515647   \n",
            "19  0.670038  0.478898  0.133934  0.317207 -0.383914  2.476607 -2.575679   \n",
            "20  1.780669  1.033887 -0.043665  0.135341  0.036268 -0.694678 -1.321278   \n",
            "21 -2.905534  0.963953  1.057505 -0.600924  0.621815 -0.175828  1.888403   \n",
            "22 -0.484269 -0.717361 -1.216896 -1.341853  1.359862 -0.333006  0.239972   \n",
            "23  0.770300  0.286824 -0.522803  0.170265 -0.770391 -0.034662 -0.657823   \n",
            "24  0.171093  0.955744 -1.376500  0.614331  0.794085 -0.985971 -1.033522   \n",
            "25 -1.382818 -0.457078 -0.456693  2.177205  0.242991 -0.474857  0.064215   \n",
            "26 -0.986676  0.818727  0.758720 -0.899834 -0.734657 -1.053275 -0.060929   \n",
            "27  1.099488 -0.697891 -0.403934 -0.176542  0.164813  0.451029 -1.484485   \n",
            "28  2.423820  1.457278  0.317998 -2.058740 -0.420090 -0.691256  0.167741   \n",
            "29  0.503652 -0.642579 -0.903764  0.374938 -0.213290  0.795397 -0.387991   \n",
            "30  1.014337 -1.608052  0.375635 -2.259168 -0.696333 -1.079305 -0.400122   \n",
            "31  3.116689  1.723113  0.681957 -1.420649 -1.193706 -0.665680 -0.091697   \n",
            "32 -1.517675 -1.205269 -0.024106  0.664088  3.003847  0.674357 -0.142209   \n",
            "33 -1.125131  0.378030 -0.374538  0.361702  1.661269  1.541844  1.134609   \n",
            "34  0.007997  0.717900 -1.084321  0.565784  0.038188 -1.910262 -0.709885   \n",
            "35  1.317641  0.414077  0.614004 -0.419078 -1.573296  0.660523  1.796185   \n",
            "36  0.786040  0.074047 -0.166161 -0.262730  0.763262  0.706814  2.259574   \n",
            "37  0.011365 -0.370733  0.261825  0.013103  1.260039 -0.554880  0.191121   \n",
            "38 -0.560580  0.164782  0.557392 -1.035448  0.752872 -1.385119 -0.630531   \n",
            "39 -0.725218  0.331052 -0.523784  0.194932 -0.856695  0.352498 -0.053287   \n",
            "40  0.803086 -0.835295 -0.639992 -0.869191 -1.658046  0.863716  0.382231   \n",
            "41  0.272421  1.009930  1.523204 -1.375841 -1.603089 -0.350015  0.297634   \n",
            "42 -2.727341  0.145088  0.484466  1.042311  1.162478 -1.482644 -0.157254   \n",
            "43 -0.789957  0.549711  0.977592 -0.019222 -0.048327 -0.215502  0.429913   \n",
            "44 -3.751709 -0.048892  1.479589  1.088468  1.030387  0.744044  1.597948   \n",
            "45  0.261220  1.287996  0.349535 -0.324269 -1.129768 -1.046599  0.082300   \n",
            "46  1.947578  1.323064 -0.352006 -1.286524 -1.308874  0.670241 -0.144401   \n",
            "47 -1.549491 -1.522130 -0.313656  0.316227 -0.249302  1.104591 -0.581757   \n",
            "48  1.092981 -1.028857 -0.250087 -1.441544 -1.115122  1.402114 -1.131119   \n",
            "49  0.572089  0.007242 -0.636844  0.853076  1.122678  1.509380 -1.020430   \n",
            "50 -2.689340 -0.277448  1.042525  1.452017  0.123011  1.133931  0.748255   \n",
            "\n",
            "          7         8         9   ...        29        30        31        32  \\\n",
            "0   0.382224 -1.064994 -2.581160  ...  0.219561 -1.164105 -2.302173 -1.035291   \n",
            "1  -0.375862 -0.384313 -0.203708  ... -0.154425  0.859029  0.434372 -0.076124   \n",
            "2  -1.113034 -1.138260 -0.083353  ...  0.030184 -1.164105  0.434372  0.883043   \n",
            "3  -0.098764  0.337893  2.173916  ...  2.094682 -1.164105  0.434372  0.403459   \n",
            "4   1.165677  1.248191  0.310892  ...  0.263554 -1.164105  0.434372  0.883043   \n",
            "5   1.512495 -0.868699 -1.661515  ...  0.149895  0.859029 -2.302173  0.883043   \n",
            "6  -0.951835  1.527703 -0.403727  ... -0.395063  0.859029 -2.302173 -0.555708   \n",
            "7  -0.173060  0.068664  0.033924  ... -0.087071  0.859029  0.434372 -0.555708   \n",
            "8   0.584333 -1.482787  1.048752  ...  1.611295  0.859029  0.434372 -0.076124   \n",
            "9   0.488403  0.530823 -1.020090  ... -0.079679  0.859029  0.434372 -0.076124   \n",
            "10  0.252102  0.355264  1.974272  ...  0.574427  0.859029  0.434372 -0.076124   \n",
            "11  1.379504 -0.101004 -0.011852  ... -2.185978 -1.164105 -2.302173  2.321793   \n",
            "12 -1.226289  1.084064  0.252926  ... -0.787906 -1.164105  0.434372  0.403459   \n",
            "13  0.077320 -0.676263  0.871215  ...  1.041880  0.859029  0.434372 -0.555708   \n",
            "14  0.471782  0.232531  2.293824  ...  2.185123  0.859029  0.434372 -0.076124   \n",
            "15 -0.273246 -0.531004 -0.567577  ... -0.876400 -1.164105 -2.302173  2.321793   \n",
            "16 -0.891745 -0.625318 -1.002406  ...  1.048573 -1.164105 -2.302173 -1.035291   \n",
            "17  1.668175 -0.032122 -0.608809  ... -0.085208  0.859029  0.434372 -0.076124   \n",
            "18 -1.201217 -1.086889  0.143054  ... -0.610362  0.859029  0.434372 -0.555708   \n",
            "19  3.880914 -2.190124  0.326164  ...  2.908866  0.859029  0.434372 -0.076124   \n",
            "20  1.928448 -0.231571  1.163341  ...  0.765533 -1.164105  0.434372  0.883043   \n",
            "21  0.477749 -0.935990 -0.204657  ...  0.077658  0.859029  0.434372 -0.076124   \n",
            "22  0.308476  1.260645  0.242491  ... -0.227157  0.859029  0.434372  0.403459   \n",
            "23  0.957006 -0.012627 -0.059101  ... -0.139646  0.859029  0.434372 -0.076124   \n",
            "24 -0.577370 -1.338864 -0.805743  ...  1.489135  0.859029  0.434372  0.403459   \n",
            "25  0.012977 -0.374317  0.811695  ...  0.989533 -1.164105  0.434372  0.403459   \n",
            "26 -0.306770  0.288864  0.338384  ... -0.651107  0.859029  0.434372 -0.555708   \n",
            "27  0.342000 -0.040139 -0.198566  ...  1.079230 -1.164105  0.434372  0.403459   \n",
            "28  1.034245  0.167103 -0.535725  ... -0.677696  0.859029  0.434372 -0.555708   \n",
            "29  0.128518 -0.699788  0.538024  ...  0.641478 -1.164105  0.434372  0.403459   \n",
            "30 -0.226448  1.329023 -0.446898  ... -1.456581 -1.164105  0.434372 -0.555708   \n",
            "31  1.514548 -0.396321 -0.823777  ...  0.047431 -1.164105  0.434372 -1.994458   \n",
            "32 -0.628094 -1.419638  1.878376  ...  0.980789 -1.164105  0.434372  1.842209   \n",
            "33 -2.042031  0.616344  1.442150  ... -0.415767 -1.164105  0.434372  1.842209   \n",
            "34  0.442049  0.151550  0.522742  ...  0.479677 -1.164105  0.434372  0.403459   \n",
            "35  0.938744  1.178404  0.439759  ... -0.505916  0.859029  0.434372 -0.076124   \n",
            "36 -1.782716  1.869428  0.239494  ... -0.741127  0.859029  0.434372  0.403459   \n",
            "37  0.257665 -0.422206 -1.140060  ... -1.971977 -1.164105  0.434372  0.883043   \n",
            "38 -1.204079  1.796819  0.624790  ... -0.385871  0.859029  0.434372  0.403459   \n",
            "39  0.339062 -0.523117 -0.516064  ...  0.357690 -1.164105 -2.302173 -1.035291   \n",
            "40  0.349595 -1.054598 -1.122964  ... -1.632031  0.859029  0.434372  0.403459   \n",
            "41  0.143264  1.653829  0.386328  ... -1.314442  0.859029 -2.302173  0.883043   \n",
            "42 -0.797886 -0.517182 -0.206603  ...  0.727034 -1.164105  0.434372 -0.555708   \n",
            "43  0.702972  0.181321  0.734456  ...  0.258117  0.859029  0.434372  0.883043   \n",
            "44  0.149512 -0.831327  1.386731  ... -0.265317  0.859029  0.434372 -0.076124   \n",
            "45  0.076983  0.999414 -0.797919  ...  0.080311 -1.164105  0.434372  0.403459   \n",
            "46  0.690211  2.110784  0.259129  ... -0.905546 -1.164105  0.434372 -1.994458   \n",
            "47 -0.235874 -1.889797  1.388273  ...  0.720933 -1.164105  0.434372  1.842209   \n",
            "48 -0.576694  1.359136  0.334442  ... -1.248674  0.859029 -2.302173 -0.555708   \n",
            "49  1.017679 -1.414678  0.752618  ...  0.679185  0.859029  0.434372 -1.514875   \n",
            "50  0.395918 -0.451649 -0.826654  ... -0.224069  0.859029  0.434372 -0.076124   \n",
            "\n",
            "          33        34        35        36   37  38  \n",
            "0   2.585000  2.817139  2.571306  1.736718  1.0   1  \n",
            "1  -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "2   1.098832 -0.026861 -0.116593  1.126096  0.0   1  \n",
            "3   1.344660  0.292413  0.468332  1.126096  1.0   1  \n",
            "4   1.098832 -0.026861 -0.116593  1.126096  0.0   0  \n",
            "5  -0.919079 -0.644039 -0.593604  1.736718  1.0   1  \n",
            "6   1.049271  1.118267  1.003893  1.431407  1.0   1  \n",
            "7  -0.290092 -0.493438 -0.743830 -0.705769  1.0   0  \n",
            "8  -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "9  -0.102652  0.180798  0.405252 -0.705769  0.0   0  \n",
            "10 -0.303103 -0.272472 -0.300268  1.655301  0.0   0  \n",
            "11  0.820384  0.663055  0.713856  1.187158  1.0   1  \n",
            "12 -0.626621 -0.792831 -0.764792 -0.705769  1.0   1  \n",
            "13 -0.290092 -0.493438 -0.743830 -0.705769  1.0   1  \n",
            "14 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "15  0.820384  0.663055  0.713856  1.187158  1.0   1  \n",
            "16  0.789663  0.562746  0.645536  1.349991  1.0   1  \n",
            "17 -0.102652  0.180798  0.405252 -0.705769  0.0   1  \n",
            "18 -0.997746 -0.768170 -0.834729 -0.705769  0.0   0  \n",
            "19 -0.797790 -0.718681 -0.718598 -0.705769  0.0   1  \n",
            "20  1.098832 -0.026861 -0.116593  1.126096  0.0   1  \n",
            "21 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "22  0.251586 -0.296086 -0.246117 -0.705769  0.0   0  \n",
            "23 -0.797790 -0.718681 -0.718598 -0.705769  0.0   1  \n",
            "24  0.251586 -0.296086 -0.246117 -0.705769  0.0   1  \n",
            "25  1.344660  0.292413  0.468332  1.126096  1.0   1  \n",
            "26 -0.290092 -0.493438 -0.743830 -0.705769  1.0   0  \n",
            "27  1.344660  0.292413  0.468332  1.126096  1.0   1  \n",
            "28 -0.997746 -0.768170 -0.834729 -0.705769  0.0   1  \n",
            "29  1.344660  0.292413  0.468332  1.126096  1.0   1  \n",
            "30 -1.045375 -0.932500 -0.896321 -0.705769  1.0   0  \n",
            "31 -0.298986 -0.514065 -0.582735 -0.705769  0.0   1  \n",
            "32  0.395284  2.473013  2.730719 -0.705769  0.0   1  \n",
            "33  0.395284  2.473013  2.730719 -0.705769  0.0   0  \n",
            "34 -1.215280 -0.947767 -0.917477 -0.705769  1.0   1  \n",
            "35 -0.102652  0.180798  0.405252 -0.705769  0.0   1  \n",
            "36  0.251586 -0.296086 -0.246117 -0.705769  0.0   0  \n",
            "37  1.098832 -0.026861 -0.116593  1.126096  0.0   0  \n",
            "38  0.050038  0.009018 -0.100743 -0.705769  1.0   1  \n",
            "39  2.585000  2.817139  2.571306  1.736718  1.0   1  \n",
            "40  0.251586 -0.296086 -0.246117 -0.705769  0.0   1  \n",
            "41 -0.919079 -0.644039 -0.593604  1.736718  1.0   1  \n",
            "42 -1.045375 -0.932500 -0.896321 -0.705769  1.0   0  \n",
            "43  2.249470  1.713688  0.624122 -0.705769  1.0   1  \n",
            "44 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "45 -0.626621 -0.792831 -0.764792 -0.705769  1.0   1  \n",
            "46 -0.298986 -0.514065 -0.582735 -0.705769  0.0   0  \n",
            "47  0.395284  2.473013  2.730719 -0.705769  0.0   1  \n",
            "48  1.049271  1.118267  1.003893  1.431407  1.0   1  \n",
            "49 -0.058790 -0.544583 -0.759293 -0.705769  1.0   0  \n",
            "50 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "\n",
            "[51 rows x 39 columns]\n",
            "lr = 0.001, wd = 0.001, mm = 0.725, ld = 0.9\n",
            "epoch: 1000  loss: 0.66617537\n",
            "epoch: 2000  loss: 0.61351192\n",
            "epoch: 3000  loss: 0.55290985\n",
            "epoch: 4000  loss: 0.51134008\n",
            "epoch: 5000  loss: 0.48028517\n",
            "epoch: 1000  loss: 0.50129461\n",
            "epoch: 2000  loss: 0.48593622\n",
            "epoch: 3000  loss: 0.47212049\n",
            "epoch: 4000  loss: 0.46001160\n",
            "epoch: 5000  loss: 0.44971144\n",
            "epoch: 1000  loss: 0.45717162\n",
            "epoch: 2000  loss: 0.44640028\n",
            "epoch: 3000  loss: 0.43786317\n",
            "epoch: 4000  loss: 0.43035904\n",
            "epoch: 5000  loss: 0.42318147\n",
            "epoch: 1000  loss: 0.41603923\n",
            "epoch: 2000  loss: 0.40831426\n",
            "epoch: 3000  loss: 0.40193963\n",
            "epoch: 4000  loss: 0.39659458\n",
            "epoch: 5000  loss: 0.39168727\n",
            "epoch: 1000  loss: 0.42357716\n",
            "epoch: 2000  loss: 0.41711122\n",
            "epoch: 3000  loss: 0.41168728\n",
            "epoch: 4000  loss: 0.40724373\n",
            "epoch: 5000  loss: 0.40340337\n",
            "correct prediction: 0\n",
            "correct prediction: 1\n",
            "correct prediction: 2\n",
            "correct prediction: 5\n",
            "correct prediction: 6\n",
            "correct prediction: 8\n",
            "correct prediction: 9\n",
            "correct prediction: 10\n",
            "correct prediction: 11\n",
            "correct prediction: 15\n",
            "correct prediction: 16\n",
            "correct prediction: 18\n",
            "correct prediction: 19\n",
            "correct prediction: 21\n",
            "correct prediction: 22\n",
            "correct prediction: 23\n",
            "correct prediction: 24\n",
            "correct prediction: 31\n",
            "correct prediction: 32\n",
            "correct prediction: 33\n",
            "correct prediction: 34\n",
            "correct prediction: 36\n",
            "correct prediction: 37\n",
            "correct prediction: 38\n",
            "correct prediction: 39\n",
            "correct prediction: 40\n",
            "correct prediction: 41\n",
            "correct prediction: 43\n",
            "correct prediction: 44\n",
            "correct prediction: 45\n",
            "correct prediction: 46\n",
            "correct prediction: 47\n",
            "correct prediction: 48\n",
            "correct prediction: 50\n",
            "\n",
            "loss on the test set = 0.620\n",
            "Accuracy on the test set: 66.6700%\n",
            "          0         1         2         3         4         5         6   \\\n",
            "0   0.235795 -0.596528  0.764939  0.155125 -0.709392 -1.921546 -0.110274   \n",
            "1  -2.949119 -2.031854  0.231524  0.141733  2.474848  0.246714  0.185667   \n",
            "2  -0.228339 -0.359309 -0.337852  0.821626 -0.872955  0.984931  0.392991   \n",
            "3  -1.213337  2.146571 -0.274030  0.495122 -1.323094  1.240324 -0.271201   \n",
            "4   1.122109  0.807842  0.980374 -0.072595 -0.186554  0.357749 -0.860289   \n",
            "5   0.633887 -0.313952  0.828973 -0.531478  0.995154  0.668412 -0.913099   \n",
            "6  -1.196963  0.714636  0.874417 -2.132932 -0.346888 -0.139611  0.275690   \n",
            "7  -0.208678  0.924384  0.068978 -0.392970 -0.002467  0.156285 -0.486114   \n",
            "8  -0.459825 -0.898318 -1.508754  1.435905  0.692827  3.039905 -0.190101   \n",
            "9  -0.822481 -0.223209 -0.019506  1.332998 -1.304332 -1.430331  0.158815   \n",
            "10  4.518938  1.767920 -0.655172 -0.114569 -1.247974  0.208459 -0.956752   \n",
            "11  0.807073  0.455789  1.731345 -0.566816  1.194391  0.115341  1.135136   \n",
            "12  0.127495  0.425414 -0.007127 -1.479204 -1.642907 -0.709907 -0.342184   \n",
            "13 -1.044644 -0.399502 -0.448795  0.109540  1.014075  0.344043 -0.407666   \n",
            "14  0.703290 -0.205306 -1.139115  1.155272 -1.439506  0.086987 -1.145898   \n",
            "15 -0.987320 -0.488019  0.505248  0.574651 -0.600331  0.469882  1.185208   \n",
            "16 -1.590583 -0.297732 -0.862299 -0.494346  0.679952 -1.054007  0.043731   \n",
            "17  0.961275  1.291009  0.965461 -0.210189 -0.250317 -1.335828 -0.919517   \n",
            "18 -0.675030 -0.008716 -1.001394  1.176765 -0.357519  1.582644  1.515647   \n",
            "19  0.670038  0.478898  0.133934  0.317207 -0.383914  2.476607 -2.575679   \n",
            "20  1.780669  1.033887 -0.043665  0.135341  0.036268 -0.694678 -1.321278   \n",
            "21 -2.905534  0.963953  1.057505 -0.600924  0.621815 -0.175828  1.888403   \n",
            "22 -0.484269 -0.717361 -1.216896 -1.341853  1.359862 -0.333006  0.239972   \n",
            "23  0.770300  0.286824 -0.522803  0.170265 -0.770391 -0.034662 -0.657823   \n",
            "24  0.171093  0.955744 -1.376500  0.614331  0.794085 -0.985971 -1.033522   \n",
            "25 -1.382818 -0.457078 -0.456693  2.177205  0.242991 -0.474857  0.064215   \n",
            "26 -0.986676  0.818727  0.758720 -0.899834 -0.734657 -1.053275 -0.060929   \n",
            "27  1.099488 -0.697891 -0.403934 -0.176542  0.164813  0.451029 -1.484485   \n",
            "28  2.423820  1.457278  0.317998 -2.058740 -0.420090 -0.691256  0.167741   \n",
            "29  0.503652 -0.642579 -0.903764  0.374938 -0.213290  0.795397 -0.387991   \n",
            "30  1.014337 -1.608052  0.375635 -2.259168 -0.696333 -1.079305 -0.400122   \n",
            "31  3.116689  1.723113  0.681957 -1.420649 -1.193706 -0.665680 -0.091697   \n",
            "32 -1.517675 -1.205269 -0.024106  0.664088  3.003847  0.674357 -0.142209   \n",
            "33 -1.125131  0.378030 -0.374538  0.361702  1.661269  1.541844  1.134609   \n",
            "34  0.007997  0.717900 -1.084321  0.565784  0.038188 -1.910262 -0.709885   \n",
            "35  1.317641  0.414077  0.614004 -0.419078 -1.573296  0.660523  1.796185   \n",
            "36  0.786040  0.074047 -0.166161 -0.262730  0.763262  0.706814  2.259574   \n",
            "37  0.011365 -0.370733  0.261825  0.013103  1.260039 -0.554880  0.191121   \n",
            "38 -0.560580  0.164782  0.557392 -1.035448  0.752872 -1.385119 -0.630531   \n",
            "39 -0.725218  0.331052 -0.523784  0.194932 -0.856695  0.352498 -0.053287   \n",
            "40  0.803086 -0.835295 -0.639992 -0.869191 -1.658046  0.863716  0.382231   \n",
            "41  0.272421  1.009930  1.523204 -1.375841 -1.603089 -0.350015  0.297634   \n",
            "42 -2.727341  0.145088  0.484466  1.042311  1.162478 -1.482644 -0.157254   \n",
            "43 -0.789957  0.549711  0.977592 -0.019222 -0.048327 -0.215502  0.429913   \n",
            "44 -3.751709 -0.048892  1.479589  1.088468  1.030387  0.744044  1.597948   \n",
            "45  0.261220  1.287996  0.349535 -0.324269 -1.129768 -1.046599  0.082300   \n",
            "46  1.947578  1.323064 -0.352006 -1.286524 -1.308874  0.670241 -0.144401   \n",
            "47 -1.549491 -1.522130 -0.313656  0.316227 -0.249302  1.104591 -0.581757   \n",
            "48  1.092981 -1.028857 -0.250087 -1.441544 -1.115122  1.402114 -1.131119   \n",
            "49  0.572089  0.007242 -0.636844  0.853076  1.122678  1.509380 -1.020430   \n",
            "50 -2.689340 -0.277448  1.042525  1.452017  0.123011  1.133931  0.748255   \n",
            "\n",
            "          7         8         9   ...        29        30        31        32  \\\n",
            "0   0.382224 -1.064994 -2.581160  ...  0.219561 -1.164105 -2.302173 -1.035291   \n",
            "1  -0.375862 -0.384313 -0.203708  ... -0.154425  0.859029  0.434372 -0.076124   \n",
            "2  -1.113034 -1.138260 -0.083353  ...  0.030184 -1.164105  0.434372  0.883043   \n",
            "3  -0.098764  0.337893  2.173916  ...  2.094682 -1.164105  0.434372  0.403459   \n",
            "4   1.165677  1.248191  0.310892  ...  0.263554 -1.164105  0.434372  0.883043   \n",
            "5   1.512495 -0.868699 -1.661515  ...  0.149895  0.859029 -2.302173  0.883043   \n",
            "6  -0.951835  1.527703 -0.403727  ... -0.395063  0.859029 -2.302173 -0.555708   \n",
            "7  -0.173060  0.068664  0.033924  ... -0.087071  0.859029  0.434372 -0.555708   \n",
            "8   0.584333 -1.482787  1.048752  ...  1.611295  0.859029  0.434372 -0.076124   \n",
            "9   0.488403  0.530823 -1.020090  ... -0.079679  0.859029  0.434372 -0.076124   \n",
            "10  0.252102  0.355264  1.974272  ...  0.574427  0.859029  0.434372 -0.076124   \n",
            "11  1.379504 -0.101004 -0.011852  ... -2.185978 -1.164105 -2.302173  2.321793   \n",
            "12 -1.226289  1.084064  0.252926  ... -0.787906 -1.164105  0.434372  0.403459   \n",
            "13  0.077320 -0.676263  0.871215  ...  1.041880  0.859029  0.434372 -0.555708   \n",
            "14  0.471782  0.232531  2.293824  ...  2.185123  0.859029  0.434372 -0.076124   \n",
            "15 -0.273246 -0.531004 -0.567577  ... -0.876400 -1.164105 -2.302173  2.321793   \n",
            "16 -0.891745 -0.625318 -1.002406  ...  1.048573 -1.164105 -2.302173 -1.035291   \n",
            "17  1.668175 -0.032122 -0.608809  ... -0.085208  0.859029  0.434372 -0.076124   \n",
            "18 -1.201217 -1.086889  0.143054  ... -0.610362  0.859029  0.434372 -0.555708   \n",
            "19  3.880914 -2.190124  0.326164  ...  2.908866  0.859029  0.434372 -0.076124   \n",
            "20  1.928448 -0.231571  1.163341  ...  0.765533 -1.164105  0.434372  0.883043   \n",
            "21  0.477749 -0.935990 -0.204657  ...  0.077658  0.859029  0.434372 -0.076124   \n",
            "22  0.308476  1.260645  0.242491  ... -0.227157  0.859029  0.434372  0.403459   \n",
            "23  0.957006 -0.012627 -0.059101  ... -0.139646  0.859029  0.434372 -0.076124   \n",
            "24 -0.577370 -1.338864 -0.805743  ...  1.489135  0.859029  0.434372  0.403459   \n",
            "25  0.012977 -0.374317  0.811695  ...  0.989533 -1.164105  0.434372  0.403459   \n",
            "26 -0.306770  0.288864  0.338384  ... -0.651107  0.859029  0.434372 -0.555708   \n",
            "27  0.342000 -0.040139 -0.198566  ...  1.079230 -1.164105  0.434372  0.403459   \n",
            "28  1.034245  0.167103 -0.535725  ... -0.677696  0.859029  0.434372 -0.555708   \n",
            "29  0.128518 -0.699788  0.538024  ...  0.641478 -1.164105  0.434372  0.403459   \n",
            "30 -0.226448  1.329023 -0.446898  ... -1.456581 -1.164105  0.434372 -0.555708   \n",
            "31  1.514548 -0.396321 -0.823777  ...  0.047431 -1.164105  0.434372 -1.994458   \n",
            "32 -0.628094 -1.419638  1.878376  ...  0.980789 -1.164105  0.434372  1.842209   \n",
            "33 -2.042031  0.616344  1.442150  ... -0.415767 -1.164105  0.434372  1.842209   \n",
            "34  0.442049  0.151550  0.522742  ...  0.479677 -1.164105  0.434372  0.403459   \n",
            "35  0.938744  1.178404  0.439759  ... -0.505916  0.859029  0.434372 -0.076124   \n",
            "36 -1.782716  1.869428  0.239494  ... -0.741127  0.859029  0.434372  0.403459   \n",
            "37  0.257665 -0.422206 -1.140060  ... -1.971977 -1.164105  0.434372  0.883043   \n",
            "38 -1.204079  1.796819  0.624790  ... -0.385871  0.859029  0.434372  0.403459   \n",
            "39  0.339062 -0.523117 -0.516064  ...  0.357690 -1.164105 -2.302173 -1.035291   \n",
            "40  0.349595 -1.054598 -1.122964  ... -1.632031  0.859029  0.434372  0.403459   \n",
            "41  0.143264  1.653829  0.386328  ... -1.314442  0.859029 -2.302173  0.883043   \n",
            "42 -0.797886 -0.517182 -0.206603  ...  0.727034 -1.164105  0.434372 -0.555708   \n",
            "43  0.702972  0.181321  0.734456  ...  0.258117  0.859029  0.434372  0.883043   \n",
            "44  0.149512 -0.831327  1.386731  ... -0.265317  0.859029  0.434372 -0.076124   \n",
            "45  0.076983  0.999414 -0.797919  ...  0.080311 -1.164105  0.434372  0.403459   \n",
            "46  0.690211  2.110784  0.259129  ... -0.905546 -1.164105  0.434372 -1.994458   \n",
            "47 -0.235874 -1.889797  1.388273  ...  0.720933 -1.164105  0.434372  1.842209   \n",
            "48 -0.576694  1.359136  0.334442  ... -1.248674  0.859029 -2.302173 -0.555708   \n",
            "49  1.017679 -1.414678  0.752618  ...  0.679185  0.859029  0.434372 -1.514875   \n",
            "50  0.395918 -0.451649 -0.826654  ... -0.224069  0.859029  0.434372 -0.076124   \n",
            "\n",
            "          33        34        35        36   37  38  \n",
            "0   2.585000  2.817139  2.571306  1.736718  1.0   1  \n",
            "1  -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "2   1.098832 -0.026861 -0.116593  1.126096  0.0   0  \n",
            "3   1.344660  0.292413  0.468332  1.126096  1.0   0  \n",
            "4   1.098832 -0.026861 -0.116593  1.126096  0.0   1  \n",
            "5  -0.919079 -0.644039 -0.593604  1.736718  1.0   1  \n",
            "6   1.049271  1.118267  1.003893  1.431407  1.0   1  \n",
            "7  -0.290092 -0.493438 -0.743830 -0.705769  1.0   0  \n",
            "8  -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "9  -0.102652  0.180798  0.405252 -0.705769  0.0   0  \n",
            "10 -0.303103 -0.272472 -0.300268  1.655301  0.0   0  \n",
            "11  0.820384  0.663055  0.713856  1.187158  1.0   1  \n",
            "12 -0.626621 -0.792831 -0.764792 -0.705769  1.0   0  \n",
            "13 -0.290092 -0.493438 -0.743830 -0.705769  1.0   0  \n",
            "14 -0.797790 -0.718681 -0.718598 -0.705769  0.0   1  \n",
            "15  0.820384  0.663055  0.713856  1.187158  1.0   1  \n",
            "16  0.789663  0.562746  0.645536  1.349991  1.0   1  \n",
            "17 -0.102652  0.180798  0.405252 -0.705769  0.0   1  \n",
            "18 -0.997746 -0.768170 -0.834729 -0.705769  0.0   0  \n",
            "19 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "20  1.098832 -0.026861 -0.116593  1.126096  0.0   1  \n",
            "21 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "22  0.251586 -0.296086 -0.246117 -0.705769  0.0   0  \n",
            "23 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "24  0.251586 -0.296086 -0.246117 -0.705769  0.0   0  \n",
            "25  1.344660  0.292413  0.468332  1.126096  1.0   0  \n",
            "26 -0.290092 -0.493438 -0.743830 -0.705769  1.0   0  \n",
            "27  1.344660  0.292413  0.468332  1.126096  1.0   0  \n",
            "28 -0.997746 -0.768170 -0.834729 -0.705769  0.0   1  \n",
            "29  1.344660  0.292413  0.468332  1.126096  1.0   0  \n",
            "30 -1.045375 -0.932500 -0.896321 -0.705769  1.0   0  \n",
            "31 -0.298986 -0.514065 -0.582735 -0.705769  0.0   0  \n",
            "32  0.395284  2.473013  2.730719 -0.705769  0.0   0  \n",
            "33  0.395284  2.473013  2.730719 -0.705769  0.0   0  \n",
            "34 -1.215280 -0.947767 -0.917477 -0.705769  1.0   1  \n",
            "35 -0.102652  0.180798  0.405252 -0.705769  0.0   1  \n",
            "36  0.251586 -0.296086 -0.246117 -0.705769  0.0   0  \n",
            "37  1.098832 -0.026861 -0.116593  1.126096  0.0   0  \n",
            "38  0.050038  0.009018 -0.100743 -0.705769  1.0   1  \n",
            "39  2.585000  2.817139  2.571306  1.736718  1.0   1  \n",
            "40  0.251586 -0.296086 -0.246117 -0.705769  0.0   0  \n",
            "41 -0.919079 -0.644039 -0.593604  1.736718  1.0   1  \n",
            "42 -1.045375 -0.932500 -0.896321 -0.705769  1.0   0  \n",
            "43  2.249470  1.713688  0.624122 -0.705769  1.0   1  \n",
            "44 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "45 -0.626621 -0.792831 -0.764792 -0.705769  1.0   1  \n",
            "46 -0.298986 -0.514065 -0.582735 -0.705769  0.0   0  \n",
            "47  0.395284  2.473013  2.730719 -0.705769  0.0   0  \n",
            "48  1.049271  1.118267  1.003893  1.431407  1.0   1  \n",
            "49 -0.058790 -0.544583 -0.759293 -0.705769  1.0   0  \n",
            "50 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "\n",
            "[51 rows x 39 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create the loss function and the optimizer. We will use MSE loss function\n",
        "criterion = nn.BCELoss() # Binary Cross Entropy loss for binary classification\n",
        "#optimizer = torch.optim.Adam(net.parameters(), lr=0.1, weight_decay=0)\n",
        "# another optimizer that we can use is SGD\n",
        "\n",
        "# run model_1.py for different hyperparameters\n",
        "# and save the results in a csv file\n",
        "# lr = learning rate\n",
        "# wd = weight decay\n",
        "# mm = momentum\n",
        "# ld = learning rate decay\n",
        "\n",
        "for lr in [0.01]:\n",
        "    for wd in [ 0.001]:\n",
        "        for mm in [0.5]:\n",
        "            for ld in [1]:\n",
        "                hyperparameters = {'lr': lr, 'weight_decay': wd, 'momentum': mm}\n",
        "                lr_decay = ld\n",
        "                print(f'lr = {lr}, wd = {wd}, mm = {mm}, ld = {ld}')\n",
        "                net = Net()\n",
        "\n",
        "\n",
        "                hyperparameters = {'lr': lr, 'weight_decay': wd, 'momentum': mm}\n",
        "                lr_decay = ld\n",
        "\n",
        "                optimizer = torch.optim.SGD(net.parameters(), **hyperparameters)\n",
        "\n",
        "\n",
        "                # train the model\n",
        "                epochs = 10000\n",
        "                loss_values = []\n",
        "\n",
        "                from sklearn.model_selection import KFold\n",
        "\n",
        "                # Define the number of folds for cross-validation\n",
        "                num_folds = 5\n",
        "\n",
        "                # Create a KFold object\n",
        "                kf = KFold(n_splits=num_folds, shuffle=True)\n",
        "                for fold, (train_ids, val_ids) in enumerate(kf.split(X_train_)):\n",
        "                    X_train_fold = X_train_[train_ids]\n",
        "                    y_train_fold = y_train_[train_ids]\n",
        "                    X_val_fold = X_train_[val_ids]\n",
        "                    y_val_fold = y_train_[val_ids]\n",
        "\n",
        "\n",
        "                    for i in range(epochs):\n",
        "                        i += 1\n",
        "                        y_pred = net.forward(X_train_fold)\n",
        "\n",
        "                        loss = criterion(y_pred, y_train_fold.unsqueeze(1).float())\n",
        "                        optimizer.zero_grad()\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        # print every 10 epochs\n",
        "                        if i%1000 == 0:\n",
        "                            print(f'epoch: {i:2}  loss: {loss.item():10.8f}')\n",
        "                            loss_values.append([i, loss.item()])\n",
        "                        # decrease the learning rate every 300 epochs\n",
        "                        if i % 1000 == 0:\n",
        "                            for g in optimizer.param_groups:\n",
        "                                g['lr'] = g['lr'] * lr_decay\n",
        "\n",
        "\n",
        "\n",
        "                # test the model\n",
        "                with torch.no_grad():\n",
        "                    y_val = net.forward(X_test_)\n",
        "                    loss = criterion(y_val, y_test_.unsqueeze(1).float())\n",
        "\n",
        "                # calculate the accuracy\n",
        "                correct = 0\n",
        "                total = 0\n",
        "                correct_predictions = []\n",
        "                prediction = []\n",
        "                with torch.no_grad():\n",
        "                    predictions = net.forward(X_test_)\n",
        "                    for i in range(len(y_test_)):\n",
        "                        if predictions[i] >= 0.5:\n",
        "                            y_pred = 1\n",
        "                            prediction.append(1)\n",
        "                        else:\n",
        "                            y_pred = 0\n",
        "                            prediction.append(0)\n",
        "                        if y_pred == y_test_[i]:\n",
        "                            correct += 1\n",
        "                            print(f'correct prediction: {i}')\n",
        "                            correct_predictions.append(i)\n",
        "                        total += 1\n",
        "                print()\n",
        "                accuracy = correct/total\n",
        "                print(f'loss on the test set = {loss:.3f}')\n",
        "                print(f'Accuracy on the test set: {round(accuracy, 4)*100:3.4f}%')\n",
        "\n",
        "\n",
        "\n",
        "                # csv with colums: epochs, lr, weight_decay, momentum, accuracy\n",
        "                # in order to find the best hyperparameters\n",
        "                df = pd.read_csv('NN_hyperparameters.csv')\n",
        "\n",
        "\n",
        "\n",
        "                df = pd.concat([df, pd.DataFrame([[epochs, hyperparameters['lr'], hyperparameters['weight_decay'], \\\n",
        "                                                    hyperparameters['momentum'], lr_decay, accuracy]], columns=['epochs', 'lr', \\\n",
        "                                                    'weight_decay', 'momentum', 'lr_decay',  'accuracy'])], axis=0, ignore_index=True)\n",
        "\n",
        "                # change the order of the columns\n",
        "                df = df[['epochs', 'lr', 'weight_decay', 'momentum', 'lr_decay',  'accuracy']]\n",
        "\n",
        "                df.to_csv('NN_hyperparameters.csv', index=False, header=True)\n",
        "\n",
        "\n",
        "                # save the model's weights in order to plot the features with their weights\n",
        "                torch.save(net.state_dict(), 'NN_weights.pt')\n",
        "\n",
        "\n",
        "                # merge X_test with y_test and save it to csv\n",
        "                X_test_df = pd.DataFrame(X_test_)\n",
        "                y_test_df = pd.DataFrame(y_test_)\n",
        "                predictions_df = pd.DataFrame(prediction)\n",
        "                # merge X_test_df with y_test_df\n",
        "                test_df = pd.concat([X_test_df, y_test_df, predictions_df], axis=1, ignore_index=True)\n",
        "\n",
        "                # header\n",
        "                header = ['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', \\\n",
        "                        'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', \\\n",
        "                            'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', \\\n",
        "                                'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'gender', \\\n",
        "                                 \t'inpatient', \t'age',\t'mean' ,\t'std', \t'max', \t'min', 'target', 'prediction']\n",
        "\n",
        "\n",
        "                print(test_df)\n",
        "                # save the test_df to csv\n",
        "                test_df.to_csv('test_df.csv', index=False, header=header)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgwj4Zg3vc8P",
        "outputId": "b99a9708-965e-4bd3-8386-321b51a4fb89"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr = 0.01, wd = 0.001, mm = 0.5, ld = 1\n",
            "epoch: 1000  loss: 0.43767014\n",
            "epoch: 2000  loss: 0.15775484\n",
            "epoch: 3000  loss: 0.09176248\n",
            "epoch: 4000  loss: 0.07946526\n",
            "epoch: 5000  loss: 0.07593711\n",
            "epoch: 6000  loss: 0.07191874\n",
            "epoch: 7000  loss: 0.07066028\n",
            "epoch: 8000  loss: 0.07000837\n",
            "epoch: 9000  loss: 0.06961896\n",
            "epoch: 10000  loss: 0.06936412\n",
            "epoch: 1000  loss: 0.08714816\n",
            "epoch: 2000  loss: 0.08552241\n",
            "epoch: 3000  loss: 0.08347196\n",
            "epoch: 4000  loss: 0.08294921\n",
            "epoch: 5000  loss: 0.08271031\n",
            "epoch: 6000  loss: 0.08257045\n",
            "epoch: 7000  loss: 0.08248807\n",
            "epoch: 8000  loss: 0.08242690\n",
            "epoch: 9000  loss: 0.08239860\n",
            "epoch: 10000  loss: 0.08237817\n",
            "epoch: 1000  loss: 0.08628485\n",
            "epoch: 2000  loss: 0.08525656\n",
            "epoch: 3000  loss: 0.08482327\n",
            "epoch: 4000  loss: 0.08457090\n",
            "epoch: 5000  loss: 0.08440936\n",
            "epoch: 6000  loss: 0.06684068\n",
            "epoch: 7000  loss: 0.06588088\n",
            "epoch: 8000  loss: 0.06575238\n",
            "epoch: 9000  loss: 0.04299818\n",
            "epoch: 10000  loss: 0.04236787\n",
            "epoch: 1000  loss: 0.03481787\n",
            "epoch: 2000  loss: 0.03406127\n",
            "epoch: 3000  loss: 0.03377239\n",
            "epoch: 4000  loss: 0.03361870\n",
            "epoch: 5000  loss: 0.03353278\n",
            "epoch: 6000  loss: 0.03348093\n",
            "epoch: 7000  loss: 0.03344898\n",
            "epoch: 8000  loss: 0.03343311\n",
            "epoch: 9000  loss: 0.03252591\n",
            "epoch: 10000  loss: 0.03211844\n",
            "epoch: 1000  loss: 0.02467355\n",
            "epoch: 2000  loss: 0.02362493\n",
            "epoch: 3000  loss: 0.02317953\n",
            "epoch: 4000  loss: 0.02292070\n",
            "epoch: 5000  loss: 0.02277293\n",
            "epoch: 6000  loss: 0.02269005\n",
            "epoch: 7000  loss: 0.02264181\n",
            "epoch: 8000  loss: 0.02262157\n",
            "epoch: 9000  loss: 0.02261220\n",
            "epoch: 10000  loss: 0.02260886\n",
            "correct prediction: 0\n",
            "correct prediction: 4\n",
            "correct prediction: 5\n",
            "correct prediction: 6\n",
            "correct prediction: 8\n",
            "correct prediction: 9\n",
            "correct prediction: 10\n",
            "correct prediction: 11\n",
            "correct prediction: 12\n",
            "correct prediction: 14\n",
            "correct prediction: 15\n",
            "correct prediction: 16\n",
            "correct prediction: 18\n",
            "correct prediction: 19\n",
            "correct prediction: 20\n",
            "correct prediction: 21\n",
            "correct prediction: 22\n",
            "correct prediction: 23\n",
            "correct prediction: 24\n",
            "correct prediction: 26\n",
            "correct prediction: 27\n",
            "correct prediction: 29\n",
            "correct prediction: 30\n",
            "correct prediction: 32\n",
            "correct prediction: 34\n",
            "correct prediction: 35\n",
            "correct prediction: 36\n",
            "correct prediction: 38\n",
            "correct prediction: 39\n",
            "correct prediction: 40\n",
            "correct prediction: 42\n",
            "correct prediction: 43\n",
            "correct prediction: 44\n",
            "correct prediction: 45\n",
            "correct prediction: 46\n",
            "correct prediction: 47\n",
            "correct prediction: 48\n",
            "correct prediction: 49\n",
            "correct prediction: 50\n",
            "\n",
            "loss on the test set = 1.054\n",
            "Accuracy on the test set: 76.4700%\n",
            "          0         1         2         3         4         5         6   \\\n",
            "0   0.235795 -0.596528  0.764939  0.155125 -0.709392 -1.921546 -0.110274   \n",
            "1  -2.949119 -2.031854  0.231524  0.141733  2.474848  0.246714  0.185667   \n",
            "2  -0.228339 -0.359309 -0.337852  0.821626 -0.872955  0.984931  0.392991   \n",
            "3  -1.213337  2.146571 -0.274030  0.495122 -1.323094  1.240324 -0.271201   \n",
            "4   1.122109  0.807842  0.980374 -0.072595 -0.186554  0.357749 -0.860289   \n",
            "5   0.633887 -0.313952  0.828973 -0.531478  0.995154  0.668412 -0.913099   \n",
            "6  -1.196963  0.714636  0.874417 -2.132932 -0.346888 -0.139611  0.275690   \n",
            "7  -0.208678  0.924384  0.068978 -0.392970 -0.002467  0.156285 -0.486114   \n",
            "8  -0.459825 -0.898318 -1.508754  1.435905  0.692827  3.039905 -0.190101   \n",
            "9  -0.822481 -0.223209 -0.019506  1.332998 -1.304332 -1.430331  0.158815   \n",
            "10  4.518938  1.767920 -0.655172 -0.114569 -1.247974  0.208459 -0.956752   \n",
            "11  0.807073  0.455789  1.731345 -0.566816  1.194391  0.115341  1.135136   \n",
            "12  0.127495  0.425414 -0.007127 -1.479204 -1.642907 -0.709907 -0.342184   \n",
            "13 -1.044644 -0.399502 -0.448795  0.109540  1.014075  0.344043 -0.407666   \n",
            "14  0.703290 -0.205306 -1.139115  1.155272 -1.439506  0.086987 -1.145898   \n",
            "15 -0.987320 -0.488019  0.505248  0.574651 -0.600331  0.469882  1.185208   \n",
            "16 -1.590583 -0.297732 -0.862299 -0.494346  0.679952 -1.054007  0.043731   \n",
            "17  0.961275  1.291009  0.965461 -0.210189 -0.250317 -1.335828 -0.919517   \n",
            "18 -0.675030 -0.008716 -1.001394  1.176765 -0.357519  1.582644  1.515647   \n",
            "19  0.670038  0.478898  0.133934  0.317207 -0.383914  2.476607 -2.575679   \n",
            "20  1.780669  1.033887 -0.043665  0.135341  0.036268 -0.694678 -1.321278   \n",
            "21 -2.905534  0.963953  1.057505 -0.600924  0.621815 -0.175828  1.888403   \n",
            "22 -0.484269 -0.717361 -1.216896 -1.341853  1.359862 -0.333006  0.239972   \n",
            "23  0.770300  0.286824 -0.522803  0.170265 -0.770391 -0.034662 -0.657823   \n",
            "24  0.171093  0.955744 -1.376500  0.614331  0.794085 -0.985971 -1.033522   \n",
            "25 -1.382818 -0.457078 -0.456693  2.177205  0.242991 -0.474857  0.064215   \n",
            "26 -0.986676  0.818727  0.758720 -0.899834 -0.734657 -1.053275 -0.060929   \n",
            "27  1.099488 -0.697891 -0.403934 -0.176542  0.164813  0.451029 -1.484485   \n",
            "28  2.423820  1.457278  0.317998 -2.058740 -0.420090 -0.691256  0.167741   \n",
            "29  0.503652 -0.642579 -0.903764  0.374938 -0.213290  0.795397 -0.387991   \n",
            "30  1.014337 -1.608052  0.375635 -2.259168 -0.696333 -1.079305 -0.400122   \n",
            "31  3.116689  1.723113  0.681957 -1.420649 -1.193706 -0.665680 -0.091697   \n",
            "32 -1.517675 -1.205269 -0.024106  0.664088  3.003847  0.674357 -0.142209   \n",
            "33 -1.125131  0.378030 -0.374538  0.361702  1.661269  1.541844  1.134609   \n",
            "34  0.007997  0.717900 -1.084321  0.565784  0.038188 -1.910262 -0.709885   \n",
            "35  1.317641  0.414077  0.614004 -0.419078 -1.573296  0.660523  1.796185   \n",
            "36  0.786040  0.074047 -0.166161 -0.262730  0.763262  0.706814  2.259574   \n",
            "37  0.011365 -0.370733  0.261825  0.013103  1.260039 -0.554880  0.191121   \n",
            "38 -0.560580  0.164782  0.557392 -1.035448  0.752872 -1.385119 -0.630531   \n",
            "39 -0.725218  0.331052 -0.523784  0.194932 -0.856695  0.352498 -0.053287   \n",
            "40  0.803086 -0.835295 -0.639992 -0.869191 -1.658046  0.863716  0.382231   \n",
            "41  0.272421  1.009930  1.523204 -1.375841 -1.603089 -0.350015  0.297634   \n",
            "42 -2.727341  0.145088  0.484466  1.042311  1.162478 -1.482644 -0.157254   \n",
            "43 -0.789957  0.549711  0.977592 -0.019222 -0.048327 -0.215502  0.429913   \n",
            "44 -3.751709 -0.048892  1.479589  1.088468  1.030387  0.744044  1.597948   \n",
            "45  0.261220  1.287996  0.349535 -0.324269 -1.129768 -1.046599  0.082300   \n",
            "46  1.947578  1.323064 -0.352006 -1.286524 -1.308874  0.670241 -0.144401   \n",
            "47 -1.549491 -1.522130 -0.313656  0.316227 -0.249302  1.104591 -0.581757   \n",
            "48  1.092981 -1.028857 -0.250087 -1.441544 -1.115122  1.402114 -1.131119   \n",
            "49  0.572089  0.007242 -0.636844  0.853076  1.122678  1.509380 -1.020430   \n",
            "50 -2.689340 -0.277448  1.042525  1.452017  0.123011  1.133931  0.748255   \n",
            "\n",
            "          7         8         9   ...        29        30        31        32  \\\n",
            "0   0.382224 -1.064994 -2.581160  ...  0.219561 -1.164105 -2.302173 -1.035291   \n",
            "1  -0.375862 -0.384313 -0.203708  ... -0.154425  0.859029  0.434372 -0.076124   \n",
            "2  -1.113034 -1.138260 -0.083353  ...  0.030184 -1.164105  0.434372  0.883043   \n",
            "3  -0.098764  0.337893  2.173916  ...  2.094682 -1.164105  0.434372  0.403459   \n",
            "4   1.165677  1.248191  0.310892  ...  0.263554 -1.164105  0.434372  0.883043   \n",
            "5   1.512495 -0.868699 -1.661515  ...  0.149895  0.859029 -2.302173  0.883043   \n",
            "6  -0.951835  1.527703 -0.403727  ... -0.395063  0.859029 -2.302173 -0.555708   \n",
            "7  -0.173060  0.068664  0.033924  ... -0.087071  0.859029  0.434372 -0.555708   \n",
            "8   0.584333 -1.482787  1.048752  ...  1.611295  0.859029  0.434372 -0.076124   \n",
            "9   0.488403  0.530823 -1.020090  ... -0.079679  0.859029  0.434372 -0.076124   \n",
            "10  0.252102  0.355264  1.974272  ...  0.574427  0.859029  0.434372 -0.076124   \n",
            "11  1.379504 -0.101004 -0.011852  ... -2.185978 -1.164105 -2.302173  2.321793   \n",
            "12 -1.226289  1.084064  0.252926  ... -0.787906 -1.164105  0.434372  0.403459   \n",
            "13  0.077320 -0.676263  0.871215  ...  1.041880  0.859029  0.434372 -0.555708   \n",
            "14  0.471782  0.232531  2.293824  ...  2.185123  0.859029  0.434372 -0.076124   \n",
            "15 -0.273246 -0.531004 -0.567577  ... -0.876400 -1.164105 -2.302173  2.321793   \n",
            "16 -0.891745 -0.625318 -1.002406  ...  1.048573 -1.164105 -2.302173 -1.035291   \n",
            "17  1.668175 -0.032122 -0.608809  ... -0.085208  0.859029  0.434372 -0.076124   \n",
            "18 -1.201217 -1.086889  0.143054  ... -0.610362  0.859029  0.434372 -0.555708   \n",
            "19  3.880914 -2.190124  0.326164  ...  2.908866  0.859029  0.434372 -0.076124   \n",
            "20  1.928448 -0.231571  1.163341  ...  0.765533 -1.164105  0.434372  0.883043   \n",
            "21  0.477749 -0.935990 -0.204657  ...  0.077658  0.859029  0.434372 -0.076124   \n",
            "22  0.308476  1.260645  0.242491  ... -0.227157  0.859029  0.434372  0.403459   \n",
            "23  0.957006 -0.012627 -0.059101  ... -0.139646  0.859029  0.434372 -0.076124   \n",
            "24 -0.577370 -1.338864 -0.805743  ...  1.489135  0.859029  0.434372  0.403459   \n",
            "25  0.012977 -0.374317  0.811695  ...  0.989533 -1.164105  0.434372  0.403459   \n",
            "26 -0.306770  0.288864  0.338384  ... -0.651107  0.859029  0.434372 -0.555708   \n",
            "27  0.342000 -0.040139 -0.198566  ...  1.079230 -1.164105  0.434372  0.403459   \n",
            "28  1.034245  0.167103 -0.535725  ... -0.677696  0.859029  0.434372 -0.555708   \n",
            "29  0.128518 -0.699788  0.538024  ...  0.641478 -1.164105  0.434372  0.403459   \n",
            "30 -0.226448  1.329023 -0.446898  ... -1.456581 -1.164105  0.434372 -0.555708   \n",
            "31  1.514548 -0.396321 -0.823777  ...  0.047431 -1.164105  0.434372 -1.994458   \n",
            "32 -0.628094 -1.419638  1.878376  ...  0.980789 -1.164105  0.434372  1.842209   \n",
            "33 -2.042031  0.616344  1.442150  ... -0.415767 -1.164105  0.434372  1.842209   \n",
            "34  0.442049  0.151550  0.522742  ...  0.479677 -1.164105  0.434372  0.403459   \n",
            "35  0.938744  1.178404  0.439759  ... -0.505916  0.859029  0.434372 -0.076124   \n",
            "36 -1.782716  1.869428  0.239494  ... -0.741127  0.859029  0.434372  0.403459   \n",
            "37  0.257665 -0.422206 -1.140060  ... -1.971977 -1.164105  0.434372  0.883043   \n",
            "38 -1.204079  1.796819  0.624790  ... -0.385871  0.859029  0.434372  0.403459   \n",
            "39  0.339062 -0.523117 -0.516064  ...  0.357690 -1.164105 -2.302173 -1.035291   \n",
            "40  0.349595 -1.054598 -1.122964  ... -1.632031  0.859029  0.434372  0.403459   \n",
            "41  0.143264  1.653829  0.386328  ... -1.314442  0.859029 -2.302173  0.883043   \n",
            "42 -0.797886 -0.517182 -0.206603  ...  0.727034 -1.164105  0.434372 -0.555708   \n",
            "43  0.702972  0.181321  0.734456  ...  0.258117  0.859029  0.434372  0.883043   \n",
            "44  0.149512 -0.831327  1.386731  ... -0.265317  0.859029  0.434372 -0.076124   \n",
            "45  0.076983  0.999414 -0.797919  ...  0.080311 -1.164105  0.434372  0.403459   \n",
            "46  0.690211  2.110784  0.259129  ... -0.905546 -1.164105  0.434372 -1.994458   \n",
            "47 -0.235874 -1.889797  1.388273  ...  0.720933 -1.164105  0.434372  1.842209   \n",
            "48 -0.576694  1.359136  0.334442  ... -1.248674  0.859029 -2.302173 -0.555708   \n",
            "49  1.017679 -1.414678  0.752618  ...  0.679185  0.859029  0.434372 -1.514875   \n",
            "50  0.395918 -0.451649 -0.826654  ... -0.224069  0.859029  0.434372 -0.076124   \n",
            "\n",
            "          33        34        35        36   37  38  \n",
            "0   2.585000  2.817139  2.571306  1.736718  1.0   1  \n",
            "1  -0.797790 -0.718681 -0.718598 -0.705769  0.0   1  \n",
            "2   1.098832 -0.026861 -0.116593  1.126096  0.0   1  \n",
            "3   1.344660  0.292413  0.468332  1.126096  1.0   0  \n",
            "4   1.098832 -0.026861 -0.116593  1.126096  0.0   0  \n",
            "5  -0.919079 -0.644039 -0.593604  1.736718  1.0   1  \n",
            "6   1.049271  1.118267  1.003893  1.431407  1.0   1  \n",
            "7  -0.290092 -0.493438 -0.743830 -0.705769  1.0   0  \n",
            "8  -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "9  -0.102652  0.180798  0.405252 -0.705769  0.0   0  \n",
            "10 -0.303103 -0.272472 -0.300268  1.655301  0.0   0  \n",
            "11  0.820384  0.663055  0.713856  1.187158  1.0   1  \n",
            "12 -0.626621 -0.792831 -0.764792 -0.705769  1.0   1  \n",
            "13 -0.290092 -0.493438 -0.743830 -0.705769  1.0   0  \n",
            "14 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "15  0.820384  0.663055  0.713856  1.187158  1.0   1  \n",
            "16  0.789663  0.562746  0.645536  1.349991  1.0   1  \n",
            "17 -0.102652  0.180798  0.405252 -0.705769  0.0   1  \n",
            "18 -0.997746 -0.768170 -0.834729 -0.705769  0.0   0  \n",
            "19 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "20  1.098832 -0.026861 -0.116593  1.126096  0.0   0  \n",
            "21 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "22  0.251586 -0.296086 -0.246117 -0.705769  0.0   0  \n",
            "23 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "24  0.251586 -0.296086 -0.246117 -0.705769  0.0   0  \n",
            "25  1.344660  0.292413  0.468332  1.126096  1.0   0  \n",
            "26 -0.290092 -0.493438 -0.743830 -0.705769  1.0   1  \n",
            "27  1.344660  0.292413  0.468332  1.126096  1.0   1  \n",
            "28 -0.997746 -0.768170 -0.834729 -0.705769  0.0   1  \n",
            "29  1.344660  0.292413  0.468332  1.126096  1.0   1  \n",
            "30 -1.045375 -0.932500 -0.896321 -0.705769  1.0   1  \n",
            "31 -0.298986 -0.514065 -0.582735 -0.705769  0.0   1  \n",
            "32  0.395284  2.473013  2.730719 -0.705769  0.0   0  \n",
            "33  0.395284  2.473013  2.730719 -0.705769  0.0   1  \n",
            "34 -1.215280 -0.947767 -0.917477 -0.705769  1.0   1  \n",
            "35 -0.102652  0.180798  0.405252 -0.705769  0.0   0  \n",
            "36  0.251586 -0.296086 -0.246117 -0.705769  0.0   0  \n",
            "37  1.098832 -0.026861 -0.116593  1.126096  0.0   1  \n",
            "38  0.050038  0.009018 -0.100743 -0.705769  1.0   1  \n",
            "39  2.585000  2.817139  2.571306  1.736718  1.0   1  \n",
            "40  0.251586 -0.296086 -0.246117 -0.705769  0.0   0  \n",
            "41 -0.919079 -0.644039 -0.593604  1.736718  1.0   0  \n",
            "42 -1.045375 -0.932500 -0.896321 -0.705769  1.0   1  \n",
            "43  2.249470  1.713688  0.624122 -0.705769  1.0   1  \n",
            "44 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "45 -0.626621 -0.792831 -0.764792 -0.705769  1.0   1  \n",
            "46 -0.298986 -0.514065 -0.582735 -0.705769  0.0   0  \n",
            "47  0.395284  2.473013  2.730719 -0.705769  0.0   0  \n",
            "48  1.049271  1.118267  1.003893  1.431407  1.0   1  \n",
            "49 -0.058790 -0.544583 -0.759293 -0.705769  1.0   1  \n",
            "50 -0.797790 -0.718681 -0.718598 -0.705769  0.0   0  \n",
            "\n",
            "[51 rows x 39 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dice_ml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqlPqC3u0Yzh",
        "outputId": "4c61a454-8372-4e01-b2fc-af69e76882d0"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dice_ml\n",
            "  Downloading dice_ml-0.10-py3-none-any.whl (2.6 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m2.1/2.6 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from dice_ml) (4.3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from dice_ml) (1.22.4)\n",
            "Requirement already satisfied: pandas<2.0.0 in /usr/local/lib/python3.10/dist-packages (from dice_ml) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from dice_ml) (1.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dice_ml) (4.65.0)\n",
            "Collecting raiutils>=0.4.0 (from dice_ml)\n",
            "  Downloading raiutils-0.4.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0.0->dice_ml) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0.0->dice_ml) (2022.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from raiutils>=0.4.0->dice_ml) (2.27.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from raiutils>=0.4.0->dice_ml) (1.10.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->dice_ml) (23.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->dice_ml) (0.19.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->dice_ml) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->dice_ml) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas<2.0.0->dice_ml) (1.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->raiutils>=0.4.0->dice_ml) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->raiutils>=0.4.0->dice_ml) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->raiutils>=0.4.0->dice_ml) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->raiutils>=0.4.0->dice_ml) (3.4)\n",
            "Installing collected packages: raiutils, dice_ml\n",
            "Successfully installed dice_ml-0.10 raiutils-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.values\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yi3I--eu1JOF",
        "outputId": "fb2e2c14-646a-4722-ac5c-4cd58f5ceaf9"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
              "       1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
              "       0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
              "       0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
              "       0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
              "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
              "       1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,\n",
              "       0, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_proba(x):\n",
        "    predictions = net.forward(x)\n",
        "    if predictions >= 0.5:\n",
        "        y_pred = 1\n",
        "        prediction = 1\n",
        "    else:\n",
        "        y_pred = 0\n",
        "        prediction =0\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "7l0UhlOz3DJz"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "PR5g0G_b4h6d",
        "outputId": "495bdcdb-0e25-490e-bc86-56cc6dcf212f"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-f2f1de54d9f2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1614\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1615\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Net' object has no attribute 'weights'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dice_ml\n",
        "from dice_ml.utils import helpers # helper functions\n",
        "from sklearn.model_selection import train_test_split\n",
        "import copy\n",
        "\n",
        "total_train = copy.deepcopy(X_train)\n",
        "total_train[\"target\"] = y_train.values\n",
        "\n",
        "d = dice_ml.Data(dataframe=total_train,\n",
        "                 continuous_features=['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', \\\n",
        "                        'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', \\\n",
        "                            'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', \\\n",
        "                                'f25', 'f26', 'f27', 'f28', 'f29', 'f30',\\\n",
        "                                      'mean' ,\t'std', \t'max', \t'min'],\n",
        "                 outcome_name='target')\n",
        "\n",
        "m = dice_ml.Model(model=net, backend=\"PYT\")\n",
        "\n",
        "exp = dice_ml.Dice(d, m, method=\"gradient\")"
      ],
      "metadata": {
        "id": "SMUte7YGzHun"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test[0:1].to_numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQkcoHEr5eb5",
        "outputId": "6f08c263-f3ec-4221-cf7f-932209a67f17"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.23579499, -0.59652794,  0.76493944,  0.1551249 , -0.70939181,\n",
              "        -1.9215456 , -0.11027424,  0.38222368, -1.06499406, -2.58115972,\n",
              "        -0.38419395,  1.0881695 ,  0.96168222, -1.6384771 , -1.10662867,\n",
              "        -1.17581983,  0.28103921,  1.63970257, -0.72448319,  0.47746048,\n",
              "        -0.34333624,  1.28896   ,  2.17539774,  0.57472378,  2.34580742,\n",
              "         0.70065716, -0.26703981, -0.92457977, -0.26234671,  0.21956124,\n",
              "        -1.16410489, -2.30217289, -1.03529127,  2.58499983,  2.817139  ,\n",
              "         2.57130546,  1.73671759]])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate counterfactual examples\n",
        "query_instance = X_test[0:1]\n",
        "exp = exp.generate_counterfactuals(query_instance, total_CFs=4, desired_class=\"opposite\")\n",
        "# Visualize counterfactual explanation\n",
        "exp.visualize_as_dataframe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "hH_jzghQ5Ne1",
        "outputId": "673ed726-3eda-488c-8937-782aca1ca5cc"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]WARNING:root: MAD for feature min is 0, so replacing it with 1.0 to avoid error.\n",
            "100%|██████████| 1/1 [11:09<00:00, 669.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diverse Counterfactuals found! total time taken: 00 min 07 sec\n",
            "Query instance (original outcome : 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         f1        f2        f3        f4        f5        f6        f7  \\\n",
              "0  0.235795 -0.596528  0.764939  0.155125 -0.709392 -1.921546 -0.110274   \n",
              "\n",
              "         f8        f9      f10  ...       f29       f30    gender  inpatient  \\\n",
              "0  0.382224 -1.064994 -2.58116  ... -0.262347  0.219561 -1.164105  -2.302173   \n",
              "\n",
              "        age   mean       std       max       min  target  \n",
              "0 -1.035291  2.585  2.817139  2.571306  1.736718     1.0  \n",
              "\n",
              "[1 rows x 38 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-606159bb-6c6b-48ca-af5b-dd759f661743\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>f8</th>\n",
              "      <th>f9</th>\n",
              "      <th>f10</th>\n",
              "      <th>...</th>\n",
              "      <th>f29</th>\n",
              "      <th>f30</th>\n",
              "      <th>gender</th>\n",
              "      <th>inpatient</th>\n",
              "      <th>age</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>max</th>\n",
              "      <th>min</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.235795</td>\n",
              "      <td>-0.596528</td>\n",
              "      <td>0.764939</td>\n",
              "      <td>0.155125</td>\n",
              "      <td>-0.709392</td>\n",
              "      <td>-1.921546</td>\n",
              "      <td>-0.110274</td>\n",
              "      <td>0.382224</td>\n",
              "      <td>-1.064994</td>\n",
              "      <td>-2.58116</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.262347</td>\n",
              "      <td>0.219561</td>\n",
              "      <td>-1.164105</td>\n",
              "      <td>-2.302173</td>\n",
              "      <td>-1.035291</td>\n",
              "      <td>2.585</td>\n",
              "      <td>2.817139</td>\n",
              "      <td>2.571306</td>\n",
              "      <td>1.736718</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 38 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-606159bb-6c6b-48ca-af5b-dd759f661743')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-606159bb-6c6b-48ca-af5b-dd759f661743 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-606159bb-6c6b-48ca-af5b-dd759f661743');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Diverse Counterfactual set (new outcome: 0.0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         f1            f2        f3        f4            f5            f6  \\\n",
              "0  0.230737 -3.535150e-11  0.617340  0.135508  3.017268e-11 -1.811280e-10   \n",
              "1  0.164868 -3.535150e-11  0.762884  0.306157  3.017268e-11 -1.811280e-10   \n",
              "2  0.386192 -3.535150e-11  0.892289  0.155406  3.017268e-11 -1.811280e-10   \n",
              "3  0.255818 -3.535150e-11  0.764296  0.164220  3.017268e-11 -1.811280e-10   \n",
              "\n",
              "      f7        f8            f9           f10  ...           f29       f30  \\\n",
              "0 -0.001  0.378796  9.106870e-09 -6.759055e-09  ... -5.896983e-10  0.207510   \n",
              "1 -0.001  0.483216  7.037346e-02 -6.759055e-09  ... -5.896983e-10  0.216372   \n",
              "2 -0.001  0.348003  9.106870e-09 -6.759055e-09  ... -5.896983e-10  0.218201   \n",
              "3 -0.001  0.378449  9.106870e-09 -6.759055e-09  ... -5.896983e-10  0.428304   \n",
              "\n",
              "   gender  inpatient  age      mean  std  max  min  target  \n",
              "0     1.0        1.0  1.0  0.435374  1.0  1.0  1.0       0  \n",
              "1     1.0        1.0  1.0  0.994747  1.0  1.0  1.0       0  \n",
              "2     1.0        1.0  1.0  1.000000  1.0  1.0  1.0       0  \n",
              "3     1.0        1.0  1.0  1.000000  1.0  1.0  1.0       0  \n",
              "\n",
              "[4 rows x 38 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-50dfa9b7-b04f-455b-bc71-8e720f81a594\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>f8</th>\n",
              "      <th>f9</th>\n",
              "      <th>f10</th>\n",
              "      <th>...</th>\n",
              "      <th>f29</th>\n",
              "      <th>f30</th>\n",
              "      <th>gender</th>\n",
              "      <th>inpatient</th>\n",
              "      <th>age</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>max</th>\n",
              "      <th>min</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.230737</td>\n",
              "      <td>-3.535150e-11</td>\n",
              "      <td>0.617340</td>\n",
              "      <td>0.135508</td>\n",
              "      <td>3.017268e-11</td>\n",
              "      <td>-1.811280e-10</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>0.378796</td>\n",
              "      <td>9.106870e-09</td>\n",
              "      <td>-6.759055e-09</td>\n",
              "      <td>...</td>\n",
              "      <td>-5.896983e-10</td>\n",
              "      <td>0.207510</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.435374</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.164868</td>\n",
              "      <td>-3.535150e-11</td>\n",
              "      <td>0.762884</td>\n",
              "      <td>0.306157</td>\n",
              "      <td>3.017268e-11</td>\n",
              "      <td>-1.811280e-10</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>0.483216</td>\n",
              "      <td>7.037346e-02</td>\n",
              "      <td>-6.759055e-09</td>\n",
              "      <td>...</td>\n",
              "      <td>-5.896983e-10</td>\n",
              "      <td>0.216372</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.994747</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.386192</td>\n",
              "      <td>-3.535150e-11</td>\n",
              "      <td>0.892289</td>\n",
              "      <td>0.155406</td>\n",
              "      <td>3.017268e-11</td>\n",
              "      <td>-1.811280e-10</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>0.348003</td>\n",
              "      <td>9.106870e-09</td>\n",
              "      <td>-6.759055e-09</td>\n",
              "      <td>...</td>\n",
              "      <td>-5.896983e-10</td>\n",
              "      <td>0.218201</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.255818</td>\n",
              "      <td>-3.535150e-11</td>\n",
              "      <td>0.764296</td>\n",
              "      <td>0.164220</td>\n",
              "      <td>3.017268e-11</td>\n",
              "      <td>-1.811280e-10</td>\n",
              "      <td>-0.001</td>\n",
              "      <td>0.378449</td>\n",
              "      <td>9.106870e-09</td>\n",
              "      <td>-6.759055e-09</td>\n",
              "      <td>...</td>\n",
              "      <td>-5.896983e-10</td>\n",
              "      <td>0.428304</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4 rows × 38 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-50dfa9b7-b04f-455b-bc71-8e720f81a594')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-50dfa9b7-b04f-455b-bc71-8e720f81a594 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-50dfa9b7-b04f-455b-bc71-8e720f81a594');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Total number of columns (38) exceeds max_columns (20) limiting to first (20) columns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HZPTgRsCJaf",
        "outputId": "035b9c18-c36f-4727-ff9f-b5fcb44b4217"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    }
  ]
}